\documentclass[11pt,openany]{book}

% Encoding and fonts
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% Math packages
\usepackage{amsmath, amssymb, amsthm, mathtools, bm}

% Layout and spacing
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{setspace}
\onehalfspacing

% Theorem environments with cleveref support
\usepackage{aliascnt}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\providecommand*{\lemmaautorefname}{Lemma}

\newaliascnt{proposition}{theorem}
\newtheorem{proposition}[proposition]{Proposition}
\aliascntresetthe{proposition}
\providecommand*{\propositionautorefname}{Proposition}

\newaliascnt{claim}{theorem}
\newtheorem{claim}[claim]{Proposition}
\aliascntresetthe{claim}
\providecommand*{\claimautorefname}{Claim}


\newaliascnt{corollary}{theorem}
\newtheorem{corollary}[corollary]{Corollary}
\aliascntresetthe{corollary}
\providecommand*{\corollaryautorefname}{Corollary}

\theoremstyle{definition}
\newaliascnt{definition}{theorem}
\newtheorem{definition}[definition]{Definition}
\aliascntresetthe{definition}
\providecommand*{\definitionautorefname}{Definition}

\newaliascnt{example}{theorem}
\newtheorem{example}[example]{Example}
\aliascntresetthe{example}
\providecommand*{\exampleautorefname}{Example}

\theoremstyle{remark}
\newaliascnt{remark}{theorem}
\newtheorem{remark}[remark]{Remark}
\aliascntresetthe{remark}
\providecommand*{\remarkautorefname}{Remark}

% Enumeration
\usepackage{enumitem}
\setlist{topsep=4pt,itemsep=2pt,parsep=2pt}

% Graphics and diagrams
\usepackage{graphicx}
\usepackage{tikz-cd}

% Header and footer
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\fancyfoot[C]{}

% Colors and hyperlinks
\usepackage{xcolor}
\definecolor{darkblue}{rgb}{0,0,0.5}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=darkblue,
  citecolor=darkblue,
  urlcolor=darkblue,
  pdfborder={0 0 0}
}

% Title
\title{MAT 3042 - Honours Linear Algebra II}
\date{}
\begin{document}
\maketitle
\tableofcontents


\chapter{Some Algebraic Background}
In MAT2042, one studies vector spaces over $\mathbb{R}$ or $\mathbb{C}$. One main goal of this course is to generalize the theories to other algebraic structures other than $\mathbb{R}$ or $\mathbb{C}$. We will roughly go through the very basics of abstract algebras, and give a definition of a field. More details will be covered in MAT3004 (Abstract Algebra I). 

\section{Groups}
When one talks about algebraic structure, we would think of addition $a + b$ and multiplication $a \cdot b$. In general, we make the following definition:
\begin{definition}
    Let $S$ be a set. A {\bf binary operation} $S$ is a map
    $$\ast: S \times S \to S.$$
    A subset $T \subseteq S$ is {\bf closed under $\ast$} if for all $a, b \in T$, $a \ast b \in T$.
\end{definition}

\begin{definition} \label{def-group}
    A group $G$ is a set along with a binary operation $\ast: G \times G \to G$ satisfying:
    \begin{itemize}
        \item $(a \ast b) \ast c = a \ast (b \ast c)$;
        \item There exists $e\in G$ such that $e \ast g = g \ast e = g$ for all $g \in G$.
        \item For all $g \in G$, there exists $g^{-1} \in G$ such that $g \ast g^{-1} = g^{-1} \ast g = e$.
    \end{itemize}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item $(\mathbb{Z},+)$, $(\mathbb{Q},+)$, $(\mathbb{R},+)$, $(\mathbb{C},+)$ are groups.
        \item $(\mathbb{R}[x],+)$ is a group.
        \item (Modular arithmetic) $(\mathbb{Z}_n,+)$ is a group.
        \item As for multiplication, $(R,\cdot)$ is not a group for $R = \mathbb{Z}, \mathbb{Q}, \mathbb{R}, \mathbb{C}, \mathbb{R}[x]$ or $\mathbb{Z}_n$, since $0^{-1}$ does not exist in all cases.
        \item $\mathbb{Z} \backslash \{0\}$ is still not a group, since $3^{-1}$ does not exist. But $(\mathbb{Q}\backslash \{0\}, \cdot)$ is a group.
        \item $(GL_n(\mathbb{R}), \cdot)$ is a group under matrix multiplication.
    \end{enumerate}
\end{example}    

\begin{definition}
    A group $(G, \ast)$ is called {\bf abelian/commutative} if
    $$a \ast b = b \ast a$$
    for all $a, b \in G$.
\end{definition}

\begin{example}
$(\mathbb{Z},+)$, $(\mathbb{Q},+)$, $(\mathbb{R},+)$, $(\mathbb{C},+)$, $(\mathbb{R}[x],+)$, $(\mathbb{Z}_n,+)$, $(\mathbb{Q}\backslash \{0\}, \cdot)$ are commutative, but $(GL_n(\mathbb{R}), \cdot)$ is not commutative.
\end{example}    

\section{Rings}
Now we study algebraic structure with both addition and multiplication structures:
\begin{definition}
    A {\bf ring} $(R,+,\cdot)$ is a set with two binary operations $+, \cdot: R \times R \to R$ such that:
    \begin{itemize}
        \item $(R,+)$ is an abelian group;
        \item $(a\cdot b) \cdot c = a \cdot (b \cdot c)$ for all $a, b, c \in R$;
        \item $(a+b)\cdot c = a\cdot c + b \cdot c$ and $c \cdot (a+b) = c\cdot a + c\cdot b$ for all $a, b, c \in R$;
    \end{itemize}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item $(R,+,\cdot)$ with $R = \mathbb{Z}, \mathbb{Q}, \mathbb{R}, \mathbb{C}, \mathbb{Z}_n, \mathbb{R}[x]$ are rings.
        \item $(2\mathbb{Z},+,\cdot)$ is a ring.
        \item For all rings $R$, $(M_{n\times n}(R),+,\cdot)$ (here $\cdot$ is matrix multiplication) is a ring.
    \end{enumerate}
\end{example}

Note that $(R, \cdot)$ is not necessarily a group. For instance, $2\mathbb{Z}$ does not have a multiplicative identity $1 \notin 2\mathbb{Z}$. 

Unless stated otherwise, we will assume the following properties hold for a ring $(R,+,\cdot)$:
\begin{definition}
Let $(R,+,\cdot)$ is a ring. We say $R$ is 
\begin{itemize}
    \item {\bf unital} if there exists $1_R \in R$ such that $1_R\cdot r=r\cdot 1_R=r$ for all $r\in R$;
    \item {\bf commutative} if $a \cdot b = b \cdot a$ for all $a, b \in R$;
\end{itemize}
\end{definition}

\begin{example}\
\begin{enumerate}
    \item $(R,+,\cdot)$ with $R = \mathbb{Z}, \mathbb{Q}, \mathbb{R}, \mathbb{C}, \mathbb{Z}_n, \mathbb{R}[x]$ are unital and commutative.
    \item $2\mathbb{Z}$ is not unital but commutative.
    \item $M_{n\times n}(R)$ is unital if $R$ is unital. However, it is always not commutative for $n > 1$.
\end{enumerate}
\end{example}

Note that if $R \neq \{0\}$, then $(R,\cdot)$ is never a group: To begin with, since $(R,+)$ is a(n abelian) group, it must have the additive identity element $0 \in R$. On the other hand, suppose on contrary that $(R,\cdot)$ is a group, then it must be unital and contain the multiplicative identity $1 \in R$. However, there exists no $a \in R$ such that
$$0 = 0 \cdot a = a \cdot 0 = 1$$
otherwise one has $0 = r \cdot 0 = r \cdot 1 = r$ for all $r \in R$.

\section{Fields}
Now we can make precise which algebraic objects we can generalize from $\mathbb{R}$ and $\mathbb{C}$ for vector spaces:
\begin{definition}
    Let $(\mathbb{F},+,\cdot)$ be a unital commutative ring. We say $\mathbb{F}$ is a {\bf field} if for all $a \in \mathbb{F} \backslash \{0\}$, there exists $a^{-1} \in \mathbb{F} \backslash \{0\}$ such that
    $$a \cdot a^{-1} = 1.$$
    The {\bf characteristic} of $\mathbb{F}$ is the smallest positive integer $\mathrm{char}(\mathbb{F}) = p$ such that 
    $$\overbrace{1+1+ \dots + 1}^{p\ terms} = 0$$ 
    (if no such integer exists, then we let $\mathrm{char}(\mathbb{F}) = 0$).
\end{definition}

\begin{example}
    \begin{enumerate}
        \item $\mathbb{Z}$ is not a field since $\frac{1}{5} = 5^{-1}$ does not exist in $\mathbb{Z}$. 
        \item $\mathbb{Z}_6$ is not a field for a slightly different reason - $[5] \cdot [5] = [25] = [1]$ in $\mathbb{Z}_6$, so $[5]^{-1} = [5]$ has an inverse. However, $[2]^{-1}$ does not exist.
        \item  $\mathbb{Q}$, $\mathbb{R}$, $\mathbb{C}$ are fields.
        \item In general, $\mathbb{Z}_p$ is a field if and only if $p$ is a prime number. Indeed, for any $[a] \neq [0]$, one has $\mathrm{gcd}(a,p) = 1$ and by Bezout's theorem (see \autoref{thm:bezout} below for a version of the theorem in terms of polynomials), there exists integers $r, s \in \mathbb{Z}$ such that
        $$1 = ar + ps$$
        and hence 
        $$[1] = [a]\cdot [r] + [p]\cdot [s] = [a]\cdot [r] + [0]\cdot [s] = [a]\cdot [r],$$
        that is, $[a]^{-1} = [r]$.
        In such a case, $\mathrm{char}(\mathbb{Z}_p) = p$. 
    \end{enumerate}
\end{example}
In the Homework set, you will construct a field $\mathbb{F}$ with $9$ elements. In general, all $\mathbb{F}$ with finitely many elements must have
$$|\mathbb{F}| = p^r$$
for some prime number $p$, with $\mathrm{char}(\mathbb{F}) = p$.

\section{Polynomials}

\begin{definition}  
Let $\mathbb{F}$ be a field. A polynomial over $\mathbb{F}$ has the form
    \[
        p(x) = a_n x^n + \dots + a_1 x + a_0, \quad (a_0 \neq 0)
    \]
    Here
    \begin{enumerate}
    \item $a_n \neq 0$ is called the \textit{leading term} of $p(x)$; 
    \item $n$ is called the \textit{degree}; $a_n$ is called the \textit{leading coefficient}; 
    \item $a_0, \dots, a_n$ are called the \textit{coefficients}.
\end{enumerate}
    A polynomial $p(x)$ over $\mathbb{F}$ is 
\begin{enumerate}
    \item \textit{monic} if its leading coefficient is 1.

    \item \textit{irreducible} if for any $q(x), r(x) \in \mathbb{F}[x]$,
    \[
        p(x) = q(x)r(x) \quad \Rightarrow \quad q(x) \text{ or } r(x) \text{ is constant}.
    \]
    Otherwise, $p(x)$ is \textit{reducible}.
\end{enumerate}
\end{definition}

\begin{example}  
The polynomial $p(x) = x^2 + 1$ is irreducible over $\mathbb{R}$ but reducible over $\mathbb{C}$ since
\[
    x^2 + 1 = (x+i)(x-i).
\]
\end{example}

\begin{definition}  
If $p(x) = q(x)r(x)$ with $p,q,r \in \mathbb{F}[x]$, then:
\begin{itemize}
    \item $p(x)$ is divisible by $q(x)$,
    \item $q(x)$ is a factor of $p(x)$,
    \item $q(x) \mid p(x)$,
    \item $p(x)$ is a multiple of $q(x)$.
\end{itemize}
\end{definition}

For polynomials over a field, one can always factorize any polynomial into irreducible factors:
\begin{theorem} [Unique Factorization] 
Let $\mathbb{F}$ be a field. Then every $f(x) \in \mathbb{F}[x]$ can be factorized as
\[
    f(x) = a_n p_1(x)^{r_1} \cdots p_m(x)^{r_m}
\]
where $p_j \in \mathbb{F}[x]$ are monic, irreducible, and distinct. This expression is unique up to the permutation of factors.
\end{theorem}

\begin{definition}  
\begin{enumerate}
    \item $d(x)$ is a \textit{common factor} of $f_1, \dots, f_k$ if $d \mid f_i$ for all $i$.
    \item $d(x)$ is the \textit{greatest common divisor} (gcd) if:
    \begin{itemize}
        \item $d$ is monic,
        \item $d$ divides all $f_i$,
        \item $d$ has the largest degree among common divisors.
    \end{itemize}
\end{enumerate}
\end{definition}
Note: Relatively prime polynomials $f_1, \dots, f_k$ satisfy $\gcd(f_1,\dots,f_k) = 1$, but pairwise relative primeness does not imply overall greatest common divisor is 1.


\begin{example}  
If $f(x) = (x^2 + x + 1)(x^2 - 3x^2 - 4)$ and $g(x) = (x^2 + 1)(x^2 + 3x^2 + 2)$ in $\mathbb{R}[x]$, then
\[
    \gcd(f,g) = x +1.
\]
\end{example}

To find the greatest common divisor, one has to divide polynomials:
\begin{theorem} [Division Algorithm] 
For all $p,q \in \mathbb{F}[x]$ such that $p \neq 0$, there exist unique $s,r \in \mathbb{F}[x]$ satisfying $\deg(r) < \deg(p)$, such that
\[
    q(x) = s(x) \cdot p(x) + r(x).
\]
Here $r(x)$ is called the \textit{remainder}.
\end{theorem}

\begin{example}  
Given $p(x) = x^4 + 1$ and $q(x) = x^2 + 1$, division gives
\[
    x^4 + 1 = (x^2 - 1)(x^2 + 1) + 2.
\]
\end{example}

With division algorithm, one can apply {\bf Euclidean algorithm} to find the greatest common divisor of two polynomials $\gcd(f_1, f_2)$:

\begin{example} To find the greatest common divisor of $x^3 + 6x + 7$ and $x^2 + 3x + 2$ in $\mathbb{R}[x]$, one divide the polynomial of larger degree by that of the lower degree:
\[
    x^3 + 6x + 7 = (x)(x^2 + 3x + 2) + 13x + 13
\]
Repeat the division algorithm on the remainder:
\[
    x^2 + 3x + 2 = \frac{x+2}{13}(13x+13) + 0
\]
Now we hit a zero remainder. Thus the greatest common divisor is just the remainder in the second last equation, i.e.
\[
    \gcd(x^3 + 6x + 7, x^2 + 3x + 2) = x + 1.
\]
(since we decree that the greatest common divisor is a monic polynomial).
\end{example}
In general, the greatest common divisor of $f_1, f_2, \dots, f_k$ can be computed inductively by:
$$\gcd(f_1, f_2, f_3, \dots f_k) = \gcd(\cdots(\gcd(\gcd(f_1,f_2),f_3)\cdots),f_k) $$

One important aspect of greatest common divisor is the following theorem:
\begin{theorem} [Bezout] \label{thm:bezout}
Let $g = \gcd(f,h)$. Then there exist $r,s \in \mathbb{F}[x]$ such that
\[
    g(x) = r(x)f(x) + s(x)h(x).
\]
More generally, $\gcd(f_1, \dots, f_k) = g$ implies the existence of $r_i \in \mathbb{F}[x]$ such that
\[
    g = r_1 f_1 + \dots + r_k f_k.
\]
\end{theorem}

Now we study the case when our polynomial has a linear factor $x - \lambda$:
\begin{theorem}  
For $p(x) \in \mathbb{F}[x]$ and $\lambda \in F$, $x - \lambda$ divides $p$ if and only if $p(\lambda) = 0$.
\end{theorem}
\begin{proof}  
If $(x - \lambda)$ divides $p$, then $p = (x - \lambda)q$ for some $q \in \mathbb{F}[x]$, so clearly $p(\lambda) = 0$.  
Conversely, if $p(\lambda) = 0$, by Division Theorem there exists $s,r \in \mathbb{F}[x]$ with
\[
    p = (x - \lambda)s + r, \quad \deg(r) < 1 \ \Rightarrow\ r \ \text{constant}.
\]
Evaluating at $x=\lambda$ gives $0 = 0 \cdot s(\lambda) + r$, hence $r=0$ and therefore $p = (x - \lambda)s$.
\end{proof}

\begin{corollary}  
A polynomial with degree $n$ has at most $n$ roots counting multiplicity.
\end{corollary}

\begin{definition} [Algebraically Closed] 
A field $\mathbb{F}$ is \textit{algebraically closed} if every non-constant polynomial $p(x) \in \mathbb{F}[x]$ has a root $\lambda \in F$.
\end{definition}

\begin{theorem} [Fundamental Theorem of Algebra]  
The set of complex numbers $\mathbb{C}$ is algebraically closed.
\end{theorem}

By induction, in an algebraically closed field $\mathbb{F}$, any non-constant polynomial can be factorized as
\[
    p(x) = a(x - \lambda_1)^{r_1} \cdots (x - \lambda_m)^{r_m}, \quad \lambda_i \in F.
\]





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Vector Spaces}
\section{Definition of Vector Space}
As in MAT2042, here is our definition of vector space:
\begin{definition}[Vector Space]
Let $\mathbb{F}$ be a field. A vector space $V$ over $\mathbb{F}$ is a set equipped with two operations
$$+: V \times V \to V\ (addition) \quad and \quad  \cdot: \mathbb{F} \times V \to V\ (scalar\ multiplication)$$
such that the following holds:
\begin{itemize}
    \item Additive axioms - For every $x,y,z \in V$, we have
        \begin{enumerate}
            \item $x+y = y+x$.
            \item $(x+y)+z = x+(y+z)$.
            \item There exists ${\bf 0} \in V$ such that ${\bf 0}+x = x+{\bf 0} = x$.
            \item There exists $-x \in V$ such that $(-x) + x = x + (-x) = {\bf 0}$.
        \end{enumerate}
    \item Multiplicative axioms - For every $x \in V$ and $\alpha, \beta \in \mathbb{F}$, we have
        \begin{enumerate}
            \item $0 \cdot x = {\bf 0}$
            \item $1\cdot x = x$
            \item $(\alpha\beta)\cdot x = \alpha \cdot (\beta \cdot x)$
        \end{enumerate}
        \item Distributive axioms - For every $x,y \in V$ and $\alpha, \beta \in \mathbb{F}$, we have
        \begin{enumerate}
            \item $\alpha\cdot (x+y) = \alpha\cdot x + \alpha\cdot y$.
            \item $(\alpha+\beta)\cdot x = \alpha\cdot x + \beta\cdot x$.    
        \end{enumerate}
\end{itemize}  

\end{definition}
\begin{example} \label{eg-v123}
    
    \begin{enumerate}
        \item $\mathbb{F}^n$ (all $n$-vectors) is a vector space over $\mathbb{F}$; 
        \item $M_{m \times n}(\mathbb{F})$ (all $m \times n$-matrices) is a vector space over $\mathbb{F}$;
        \item $\mathbb{R}[x]$, $C^{\infty}(\mathbb{R})$ are vector spaces over $\mathbb{R}$;
        \item $V_1 := \{(\alpha_1,\alpha_2,\dots)\ |\ \alpha_i \in \mathbb{R}\}$ is a vector space over $\mathbb{R}$.
        \item $V_2 := \{(\alpha_1,\alpha_2,\dots) \in V_1 \ |\ \lim_{i \to \infty} \alpha_i  = 0\}$ is a vector space over $\mathbb{R}$.
        \item $V_3 := \{(\alpha_1,\alpha_2,\dots) \in V_2 \ |\ \text{only finitely many}\ \alpha_i\ \text{are nonzero}\}$ is a vector space over $\mathbb{R}$.
    \end{enumerate}
\end{example}

\begin{definition}[Vector Subspace]
    Let $V$ be a vector space over $\mathbb{F}$ with addition $+$ and scalar multiplication $\cdot$. A subset $W \leq V$ is a vector subspace if the operations satisfy:
    $$+|_{W \times W}: W \times W \to W, \quad \quad \quad \cdot|_{\mathbb{F} \times W}: \mathbb{F} \times W \to W,$$
    i.e. for all $w_1, w_2 \in W$ and $\alpha \in \mathbb{F}$, $w_1 + w_2 \in W$ and $\alpha \cdot w_1 \in W$.
\end{definition}

The following proposition, proved in MAT2042, gives a necessary and sufficient condition for any subset of a vector space is a vector subspace:
\begin{proposition}
    Let $V$ be a vector space over $\mathbb{F}$, a subset $W$ of $V$ is a vector subspace if and only if
    \begin{center}
        for all $\alpha, \beta \in \mathbb{F}$ and all $w_1, w_2 \in W$, $\alpha w_1 + \beta w_2 \in W$.
    \end{center}
\end{proposition}

\begin{example} \
    \begin{enumerate}
        \item (MAT2042) In $V = \mathbb{F}^n$, all vector subspaces are of the form $W = \mathrm{Span}_{\mathbb{F}}\{v_1, \dots, v_k\}$ for some vectors $v_i \in \mathbb{F}^n$ (By convention, let $\mathrm{Span}_{\mathbb{F}}(\emptyset)=\{0\}$).
        \item In $V = \mathbb{F}[x]$, the subset 
        $$W = \{p(x) \in \mathbb{F}[x]\ |\ p(x) = p(-x)\}$$ 
        is a vector subspace.
        \item In $V = C^{\infty}(\mathbb{R})$, the subset 
        $$W = \{f(x) \in C^{\infty}(\mathbb{R}) \ |\ f'(1) = f''(2) = 0\}$$ 
        is a vector subspace.
        \item $V_3 \leq V_2 \leq V_1$ in the Example \ref{eg-v123} above.
        \item If $\{W_i\ |\ i \in I\}$ is a collection of vector subspaces of $V$, then $$\bigcap_{i \in I} W_i \leq V$$ is also a vector subspace.
    \end{enumerate}
\end{example}

\section{Basis and Dimension}
We will briefly go through some well-known notions in MAT2042.

\begin{definition}\label{vspace-notions}
    Let $V$ be a vector space over $\mathbb{F}$, and $S \subseteq V$ is a (not necessarily finite) subset. We say
    \begin{itemize}
        \item $v \in V$ is a {\bf linear combination of} $S$ if $v$ can be expressed as
        $$v = \alpha_1 s_1 + \dots \alpha_k s_k$$
        for some $\alpha_1, \dots, \alpha_k \in \mathbb{F}$, $s_1, \dots, s_k \in S$ and $k \in \mathbb{N}$. In particular, the above sum must be finite even when $S$ is infinite.
        \item The {\bf span} of $S$ is the collection of linear combinations of $S$:
        $$\mathrm{Span}_{\mathbb{F}}(S) = \{\alpha_1 s_1 + \dots \alpha_k s_k\ |\ \alpha_i \in \mathbb{F},\ s_i \in S,\ k \in \mathbb{N}\}$$
        \item $S$ {\bf spans} $V$ (or $S$ {\bf is a spanning set of} $V$) if $V = \mathrm{Span}(S)$.
        \item $S$ is {\bf linear independent} if for {\it any} finite subset 
        $\{s_1, \dots, s_n\} \subseteq S$, 
        $$\alpha_1s_1 + \dots + \alpha_ns_n = {\bf 0} \quad \Leftrightarrow \quad \alpha_1 = \dots = \alpha_n = 0.$$
        \item $S$ is a {\bf basis} of $V$ if $S$ is a spanning set of $V$ and $S$ is linearly independent.
    \end{itemize}
\end{definition}

Note that in the definition of linear combination of $S$, the sum must be finite. 

\begin{example} \label{eg-finitesum}
\begin{enumerate}
    \item $V = \mathbb{F}[x]$, and $S = \{x^i \ |\ i \in \mathbb{N} \cup \{0\}\}$, then
$$e+ \frac{1}{\pi} x^3 + \pi^{e}x^5 \in \mathrm{Span}_{\mathbb{F}}(S), \quad \quad 1+x+x^2+ \dots \notin \mathrm{Span}_{\mathbb{F}}(S).$$
    \item $V = V_1$ be given in Example \ref{eg-v123}, and $\mathcal{B} = \{e_i\ |\ i \in \mathbb{N}\}$ where 
    $$e_i := (0,\dots,0,\overbrace{1}^{i\text{-th entry}},0,\dots).$$ 
    Then $S$ is linearly independent in $V_1$, and $\mathrm{Span}(S) = V_3 < V_1$.
\end{enumerate}    
\end{example}

Here are some examples of bases of some vector spaces $V$:
\begin{example}\
    \begin{enumerate}
        \item For $V = \mathbb{F}^n$, the {\bf canonical basis} $\{e_1, \dots, e_n\}$ is a basis of $V$.
        \item For $V = M_{m\times n}(\mathbb{F})$, $\{E_{ij}\ |\ 1 \leq i \leq m,\ 1 \leq j \leq n\}$ (here $E_{ij}$ is the matrix with $1$ on the $(i,j)$-entry and $0$ on the other entries) is a basis of $V$.
        \item For $V = \mathbb{F}[x]$, $\{x^i \ |\ i \in \mathbb{N} \cup \{0\}\}$ is a basis of $V$.
        \item As in Example \ref{eg-v123}(2), $\mathcal{B}$ is a basis of $V_3$ but not a basis of $V_1$.
    \end{enumerate}
\end{example}

The importance of basis is given by the following:
\begin{theorem} \label{thm-uniquesum}
    Let $V$ be a vector space over $\mathbb{F}$, and $\mathcal{B} \subseteq V$ is a subset. Then 
    \begin{center}
    \fbox{$\mathcal{B}$ is a basis of $V$ iff all $v \in V$ can be {\bf uniquely} expressed by a element in $\mathrm{Span}(\mathcal{B})$.}
    \end{center}
\end{theorem}
\begin{proof}
    We firstly explain what it means for a vector $v \in V$ to be {\bf uniquely} expressed by a element in $\mathrm{Span}(\mathcal{B})$: Namely, if 
    \begin{equation} \label{eq-uniquelc}
    v = \alpha_1v_1 + \dots + \alpha_nv_n = \beta_1v_1' + \dots + \beta_mv_m' \tag{*}
    \end{equation}
    for some {\it nonzero} $\alpha_i, \beta_j \in \mathbb{F}$ and $\{v_1, \dots v_n\}, \{v_1', \dots, v_m'\} \subseteq \mathcal{B}$ has no repeated elements, then one has 
    \begin{enumerate}
        \item $n = m$,
        \item after some reordering of the indices, $v_i = v_i'$ and $\alpha_i = \beta_i$ for all $i$.
    \end{enumerate}
    

    \noindent $(\Rightarrow) $If $\mathcal{B}$ is a basis of $V$, by definition, $v \in\mathrm{Span}(\mathcal{B})$ for all $v \in V$. As for uniqueness, suppose \eqref{eq-uniquelc} holds. Then by reordering the indices, we assume that
    $$\{v_1,\dots, v_n\} \cap \{v_1', \dots, v_m'\} = \{v_1 = v_1', \dots, v_k = v_k'\}.$$
    and by subtraction, one has
    $$(\alpha_1 - \beta_1)v_1 + \dots + (\alpha_k - \beta_k) v_k + \alpha_{k+1} v_{k+1} + \dots \alpha_{k+1} v_{n} + (-\beta_{k+1})v_{k+1}' + \dots + (-\beta_{m}) v_{m}' = {\bf 0}$$
    Now by applying linear independence of $\mathcal{B}$ on the finite subset
    $$\{v_1 = v_1', \dots, v_k = v_k', v_{k+1}, \dots, v_n, v_{k+1}', \dots, v_m'\},$$
    the above equation only has zero solutions
    $$\alpha_1 - \beta_1 = \dots = \alpha_k - \beta_k = 0$$
    and
    $$\alpha_{k+1} = \dots =\alpha_n = \beta_{k+1} = \dots = \beta_m = 0.$$ But the latter is impossible since we assume that all $\alpha_i, \beta_j \neq 0$, so one must have $m = n = k$, and 
    $$\alpha_i = \beta_i \quad \quad \text{for all}\ 1 \leq i \leq k.$$

    \noindent $(\Leftarrow) $If all $v \in V$ can be expressed by elements in $\mathrm{Span}(\mathcal{B})$, then $\mathcal{B}$ is obviously a spanning set. Now suppose on contrary that $\mathcal{B}$ is {\bf not} linearly independent, then there exists $\{b_1,\dots, b_n\} \subseteq \mathcal{B}$ and $\gamma_1, \dots, \gamma_n$ not all zeros such that
    $$\gamma_1b_1 + \dots + \gamma_nb_n = {\bf 0}$$
    By reordering the indices in the above expression, we assume $\gamma_1 \neq 0$ without loss of generality. Then
    $$v := b_1 = (-\frac{\gamma_2}{\gamma_1}) b_2 + \dots + (-\frac{\gamma_n}{\gamma_1})b_n$$
    are two different expressions of $v$ in $\mathrm{Span}(\mathcal{B})$, a contradiction. Then $\mathcal{B}$ is both a spanning set and linearly independent, and hence forms a basis of $V$.
\end{proof}

The following theorem are given in MAT2042 in the finite dimensional case:
\begin{theorem} \label{thm-basisextension}
    Let $V$ be a vector space over $\mathbb{F}$. Then the following holds:
    \begin{enumerate}
        \item $V$ has a basis.
        \item (Basis Extension Theorem) Let $\mathcal{L}$ be a linearly independent set in $V$, then one can extend $\mathcal{L}$ to $\mathcal{B} = \mathcal{L} \sqcup \mathcal{L}'$ such that $\mathcal{B}$ is a basis of $V$.
        \item Let $\mathcal{S}$ be a linearly independent set in $V$, then there exists a subset $\mathcal{B} \subseteq \mathcal{S}$ such that $\mathcal{B}$ is a basis of $V$.
        \item All bases of $V$ have the same cardinality.
    \end{enumerate}
\end{theorem}
In this course, the {\bf cardinality} of any set is equal to one of the following three possibilities:
\begin{itemize}
    \item a finite number $n \in \mathbb{N}$;
    \item infinity $\infty$ (and countable);
    \item infinity $\infty$ (and {\bf un}countable)
\end{itemize}

Given the last statement of the theorem above, we have:
\begin{definition}
    Let $V$ be a vector space over $\mathbb{F}$. Then the {\bf dimension} $\dim(V)$ of $V$ is the cardinality of any basis of $V$.
\end{definition}
\begin{example}
\begin{enumerate}
    \item $\dim(\mathbb{F}^n) = n$; 
    \item $\dim(M_{m \times n}(\mathbb{F})) = mn$;
    \item $\dim(\mathbb{F}[x]) = \infty$ (and countable).
    \item For $V_3$ in Example \ref{eg-v123}, $\dim(V_3) = \infty$ (and countable).
    \item $\dim(C^{\infty}(\mathbb{R})) = \infty$ (and uncountable). Note that $\mathcal{L} = \{e^{rx}\ |\ r \in \mathbb{R}\}$ is a linearly independent set in $C^{\infty}(\mathbb{R})$.
\end{enumerate}
\end{example}

\section{Internal Direct Sum}
In the following sections, we will give some constructions of new vector spaces from old ones.

\begin{definition}[Internal Sum]
    Let $V$ be a vector space over $\mathbb{F}$, and $\{W_i\ |\ i \in I\}$ be a collection of vector subspaces of $V$. The {\bf internal sum} is defined by:
    $$\sum_{i \in I} W_i := \{w_{i_1} + \dots + w_{i_k}\ |\ w_{i_l} \in W_{i_l},\ \{i_1, \dots, i_k\}\subseteq I,\ k \in \mathbb{N}\}$$
\end{definition}

Note that $\sum_{i \in I} W_i$ is a vector subspace of $V$, and if $\mathcal{S}_i$ spans $W_i$ for all $i \in I$, then $\bigcup_{i \in I} \mathcal{S}_i$ spans $\sum_{i \in I} W_i$. However, if $\mathcal{L}_i$ is linearly independent in $W_i$ for all $i \in I$, then $\bigcup_{i \in I} \mathcal{L}_i$ may not be linearly independent.

To see so, let $V = \mathbb{R}^3$, $W_1 = \mathrm{Span}\left\{\begin{pmatrix} 1\\ 0\\0 \end{pmatrix}, \begin{pmatrix} 0\\ 1\\0 \end{pmatrix}\right\}$ and $W_2 = \mathrm{Span}\left\{\begin{pmatrix} 1\\ 1\\1 \end{pmatrix}, \begin{pmatrix} 0\\ 0\\1 \end{pmatrix}\right\}$. Then
$W_1 + W_2 = V$
but $\left\{\begin{pmatrix} 1\\ 0\\0 \end{pmatrix}, \begin{pmatrix} 1\\ 0\\0 \end{pmatrix}, \begin{pmatrix} 1\\ 1\\1 \end{pmatrix}, \begin{pmatrix} 0\\ 0\\1 \end{pmatrix}\right\}$ is linearly dependent.

\begin{definition}
    Let $V$ be a vector space over $\mathbb{F}$, and $\{W_i\ |\ i \in I\}$ be a collection of vector subspaces of $V$. We say
    $\sum_{i \in I} W_i = \bigoplus_{i \in I} W_i$ is a {\bf direct internal sum} if for any $\{i_1, \dots, i_k\} \subseteq I$ and any $w_{i_l} \in W_{i_l}$, 
    $$w_{i_1} + \dots + w_{i_k} = {\bf 0} \quad \quad \Leftrightarrow \quad \quad w_{i_1} = \dots = w_{i_k} = {\bf 0}.$$
\end{definition}
In the paragraph above the definition, one has 
$$w_1 := \begin{pmatrix} 1\\ 1\\0 \end{pmatrix} = \begin{pmatrix} 1\\ 0\\0 \end{pmatrix} + \begin{pmatrix} 0\\ 1\\0 \end{pmatrix} \in W_1, \quad \quad w_2 = \begin{pmatrix} -1\\ -1\\0 \end{pmatrix} = (-1)\begin{pmatrix} 1\\ 1\\0 \end{pmatrix} + \begin{pmatrix} 0\\ 0\\1 \end{pmatrix} \in W_2$$ 
with $w_1 + w_2 = {\bf 0}$ yet $w_1, w_2 \neq {\bf 0}$. So the sum $W_1 + W_2$ is {\bf not} direct.

\smallskip
More generally, for {\bf two} vector subspaces $W_1, W_2 \leq V$, one can check from that above definition that
$$W_1 + W_2 = W_1 \oplus W_2$$
iff $W_1 \cap W_2 = \{\bf 0\}$. 
\begin{theorem}
    Let $V$ be a vector space over $\mathbb{F}$, and $\{W_i\ |\ i \in I\}$ be a collection of vector subspaces of $V$ such that
    $\bigoplus_{i \in I} W_i$ is a direct sum. Suppose $\mathcal{B}_i$ is a basis of $W_i$, then $$\mathcal{B} = \bigsqcup_{i\in I} \mathcal{B}_i$$ is a basis of $\bigoplus_{i \in I} W_i$.
\end{theorem}
\begin{proof}
To begin with, we show that all $v \in V$ can be uniquely written as elements of the form $w_{i_1} + \dots + w_{i_k}$ for some $w_{i_l} \in W_{i_l}$. Indeed, suppose 
$$v = w_{i_1} + \dots + w_{i_n} = w_{j_1}' + \dots + w_{j_m}',$$
for some nonzero vectors $w_{\bullet} \in W_{\bullet}$. Then by reordering the indices and subtraction as in the proof of Theorem \ref{thm-uniquesum}, one assumes 
that 
$$\{i_1, \dots, i_n\} \cap \{j_1, \dots, j_m\} = \{i_1 = j_1, \dots, i_k = j_k\}$$ 
and
$${\bf 0} = (w_{i_1}-w_{j_1}') + \dots + (w_{i_k}-w_{i_k}') + w_{i_{k+1}} + \dots + w_{i_n} - w_{j_{k+1}}' - \dots - w_{j_m},$$
where each term in the summand is in the same vector subspace $W_{\bullet} \leq V$. By the definition of direct internal sum, all the summand in the above equation must be ${\bf 0}$, and one must have
$m = n = k$ and $w_{i_1} = w_{j_1}'$, $\cdots$, $w_{i_k} = w_{i_k}'$.

Now we proceed to proving that $\mathcal{B}$ is a basis. In view of Theorem \ref{thm-uniquesum}, it suffices to show that all $v \in V $ can be uniquely written as an element in $\mathrm{Span}(\mathcal{B})$. By definition of direct internal sum, it is obvious that $v \in \mathrm{Span}(\mathcal{B})$. Moreover, in such a case, one has
$$v = b_{i_1} + \dots + b_{i_k}, \quad \quad b_{i_l} \in \mathrm{Span}(\mathcal{B}_{i_l}).$$
But $b_{i_l} = \mathrm{Span}(\mathcal{B}_{i_l}) = W_{i_l}$, so the arguments in the beginning of the proof implies that the $W_{i_l}$'s appearing in the summand of $v$ is unique, and each $b_{i_l} \in W_{i_l}$ is uniquely determined. Furthermore, the expression of each $b_{i_l}$ by the linear combination of $\mathcal{B}_{i_l}$ is also unique since it is a basis of $W_{i_l}$. So the expression of $v$ in terms of $\mathrm{Span}(\mathcal{B})$ is unique, and the result follows.
\end{proof}

As an immediate consequence of the above theorem, one has:
\begin{corollary}
 Let $V$ be a vector space over $\mathbb{F}$, and $\{W_i\ |\ i \in I\}$ be a collection of vector subspaces of $V$ such that $\bigoplus_{i \in I} W_i$ is a direct sum. Then
    $$\dim\left(\bigoplus_{i \in I} W_i\right) = \sum_{i \in I} \dim(W_i)$$
    (here we do not distinguish countable and uncountable $\infty$).
\end{corollary}

\section{External Direct Sum} \label{sec-extdirectsum}
\begin{definition}
Let $\{V_i\ |\ i \in I\}$ be a collection of vector spaces over $\mathbb{F}$. The {\bf external direct sum} is defined by
$$\bigoplus_{i \in I} V_i := \{f: I \to \cup_{i \in I} V_i \ |\ f(i) \in V_i \ \text{such that only finitely many}\ f(i)\ \text{is nonzero}\}.$$
The addition and scalar multiplication on $\bigoplus_{i \in I} V_i$ is given by:
\begin{itemize}
\item If $f, g \in \bigoplus_{i \in I} V_i$, then $f+g \in \bigoplus_{i \in I} V_i$ is defined by $(f+g)(i) := f(i)+g(i)$ for all $i \in I$.
\item If $f \in \bigoplus_{i \in I} V_i$, then $\alpha f \in \bigoplus_{i \in I} V_i$ is defined by $(\alpha f)(i) := \alpha(f(i))$ for all $i \in I$.
\end{itemize}
and the zero vector ${\bf 0}$ is given by ${\bf 0}(i) := {\bf 0}_{V_i}$ for all $i \in I$.
\end{definition}

\begin{remark}\
    \begin{itemize}
        \item Any $f \in \bigoplus_{i \in I} V_i$ is uniquely determined by the image $f(i) \in V_i$ of $\mathbb{F}$. For example, if $I = \{1,2,  \dots, n\}$ is a finite set, we can write:
        $$f \longleftrightarrow (\overbrace{f(1)}^{\in V_1}, \overbrace{f(2)}^{\in V_2}, \dots, \overbrace{f(n)}^{\in V_n})$$
        or if $I = \mathbb{N}$, we can write:
        $$f \longleftrightarrow (f(1), f(2), \dots, )$$
        Informally, we will interpret $\bigoplus_{i \in I} V_i$ as:
        $$\quad \quad \quad \bigoplus_{i \in I} V_i \longleftrightarrow \{ (\cdots, \overbrace{v_i}^{position\ i}, \cdots )\ |\ v_i \in V_i,\ \text{only finitely many}\ v_i\ \text{not equal to}\ {\bf 0}_{V_i}\}$$

        \item If $I$ is a finite set, then there only only finitely many $f(i)$'s, and hence the condition that only finitely many $f(i) \neq {\bf 0}_{V_i}$ is vacuous.

        \item Under the above interpretation of $\bigoplus_{i \in I} V_i$, if
        $$\quad u \longleftrightarrow (\cdots, u_i, \cdots, u_j, \cdots ),\quad \quad v \longleftrightarrow (\cdots, v_i, \cdots, v_j, \cdots )$$
        Then the addition and scalar multiplication given by the definition above can be understood as:
        $$\quad \quad \quad u+v \longleftrightarrow (\cdots, u_i + v_i, \cdots, u_j+v_j, \cdots ), \quad \alpha u \longleftrightarrow (\cdots, \alpha u_i, \cdots, \alpha u_j, \cdots ).$$
    \end{itemize}
\end{remark}

\begin{example}
    Let $I = \mathbb{N}$ and $V_i = \mathbb{R}$ for all $i$. Then
    $$\bigoplus_{i \in \mathbb{N}} \mathbb{R} \longleftrightarrow \{(\alpha_1, \alpha_2, \dots)\ |\ \alpha_i \in \mathbb{R}, \text{only finitely many}\ \alpha_i \neq 0\}$$
    is isomorphic to $V_3$ given in Example \ref{eg-v123} (we have not defined what isomorphism means in this course, but you should know what it is about in MAT2042).
\end{example}

The condition that $\bigoplus_{i \in I} V_i$ allows only finitely many nonzero element seems artificial, but it is natural in the sense that one only allows a finite sum for linear combination and the definition of internal sum. In particular, it is essential for the following:
\begin{theorem}
    Let $\{V_i\ |\ i \in I\}$ be a collection of vector spaces over $\mathbb{F}$. Suppose $\mathcal{B}_i := \{b_{i}^j\ |\ j \in J_i\}$ is a basis of $V_i$ for each $i$, consider $f_{i}^j \in \bigoplus_{i \in I} V_i$ given by
    \begin{align*}
f_i^j(k) := \begin{cases} b_i^j  & \text{if}\ k = i\\ 
{\bf 0}_{V_k}  & \text{if}\ k \neq i\end{cases}
    \end{align*}
(Informally, one can interpret $f_i^j \longleftrightarrow (\cdots, {\bf 0},{\bf 0},  \overbrace{b_i^j}^{position\ i}, {\bf 0}, {\bf 0}, \cdots)$). Then
    $$\mathcal{B} := \{f_{i}^{j}\ |\ i \in I,\ j \in J_i\}$$
is a basis of $\bigoplus_{i \in I} V_i$.
\end{theorem}
\begin{proof}
    For linear independence, suppose $(i_1, j_1), \dots, (i_k,j_k)$ be such that $j_l \in J_{i_l}$ for all $l$, consider
    $$\alpha_1 f_{i_1}^{j_1} + \dots + \alpha_k f_{i_k}^{j_k} = {\bf 0}.$$
    For each $1 \leq l \leq k$, write $i_l := \iota$. Suppose
    $$\{l'\ |\ i_{l'} = \iota\} = \{l, x_1, \dots, x_m\}$$
    Then one has
    \begin{align*}
    (\alpha_1 f_{i_1}^{j_1} + \dots + \alpha_k f_{i_k}^{j_k})(\iota) &= {\bf 0}(\iota)\\
    \alpha_{l} f_{\iota}^{j_l}(\iota)+ \alpha_{x_1} f_{\iota}^{j_{x_1}}(\iota) + \dots + \alpha_{x_m} f_{\iota}^{j_{x_m}}(\iota) &= {\bf 0}_{V_{\iota}}\\
    \alpha_{l} b_{\iota}^{j_l} + \alpha_{x_1} b_{\iota}^{j_{x_1}} + \dots + \alpha_{x_m} b_{\iota}^{j_{x_m}} &= {\bf 0}_{V_{\iota}}
    \end{align*}
But $\{b_{\iota}^j\ |\ j \in I_{\iota}\}$ is a basis of $V_{\iota}$, hence it is linearly independent, and hence $(\alpha_{x_1} = \dots = \alpha_{x_m} =)\  \alpha_l = 0$. Note that we can apply the same argument for all $1 \leq l \leq k$, so one has $\alpha_1 = \dots = \alpha_k = 0$.

As for spanning set, consider $f \in \bigoplus_{i \in I} V_i$. By definition of direct external sum, the set 
$$\{i \in I\ |\ f(i) \neq {\bf 0}_{V_i}\} = \{i_1, \dots, i_k\}$$
is finite, with
$$f(i_l) = \alpha_{l,1} b_{i_l}^{j_{l,1}} + \dots + \alpha_{l,n_l} b_{i_l}^{j_{l,n_l}} \in V_{i_l} = \mathrm{Span}(\mathcal{B}_{i_l})$$
Then one can check that
$$f = (\alpha_{1,1} f_{i_1}^{j_{1,1}} + \dots + \alpha_{1,n_1} f_{i_1}^{j_{1,n_1}}) + \cdots + (\alpha_{k,1} f_{i_k}^{j_{k,1}} + \dots + \alpha_{1,n_1} f_{i_k}^{j_{k,n_k}})$$
is in $\mathrm{Span}(\mathcal{B})$ (note that it is a finite sum).
\end{proof}


\begin{corollary}
   Let $\{V_i\ |\ i \in I\}$ be a collection of vector spaces over $\mathbb{F}$. Then 
   $$\dim(\bigoplus_{i \in I} V_i) = \sum_{i \in I} \dim(V_i).$$
\end{corollary}

\section{External Direct Product}
As we discussed before, one may remove the finitely many nonzero condition. In such a case, we have
\begin{definition}
Let $\{V_i\ |\ i \in I\}$ be a collection of vector spaces over $\mathbb{F}$. The {\bf external direct product} is defined by
$$\prod_{i \in I} V_i := \{f: I \to \cup_{i \in I} V_i \ |\ f(i) \in V_i\}.$$
The addition, scalar multiplication and ${\bf 0}$ on $\prod_{i \in I} V_i$ is given exactly as in that of external direct sum.
\end{definition}

\begin{example}
The vector 
    $$v := (1,1, \dots ) \in \prod_{i \in \mathbb{N}} \mathbb{R}$$
but is not in $\bigoplus_{i \in \mathbb{N}} \mathbb{R}$. More generally, one has $V_1 \cong \prod_{i \in \mathbb{N}} \mathbb{R}$ for the $V_1$ defined in Example \ref{eg-v123}.

Also, by the discussions in Section \ref{sec-extdirectsum}, the set 
$\mathcal{B} := \{f_i\ |\ i \in \mathbb{N}\}$
given by 
$$f_i(k) := \delta_{ik} = \begin{cases} 1 & if\ i = k \\ 0 & if\ i \neq k \end{cases}$$ is a basis of $\bigoplus_{i \in \mathbb{N}} \mathbb{R}$ (informally, $f_i = (0, \dots, 0, \overbrace{1}^{i-position}, 0, \dots, )$). However, {\bf $\mathcal{B}$ is only linearly independent but  not a spanning set in $\prod_{i \in \mathbb{N}} \mathbb{R}$} - namely
\begin{center}
    $v$ ``$=$'' $f_1 + f_2 + \dots$ is an infinite sum, so $v \notin \mathrm{Span}(\mathcal{B})$.
\end{center}
\end{example}


\section{Quotient Spaces}
Let $V$ be a vector space over $\mathbb{F}$, and $W \leq V$. Define an equivalence relationship $\sim$ by
$$v \sim v' \quad \Longleftrightarrow \quad v - v' \in W.$$
The equivalence class with representative $v \in V$ is defined by
$$v + W := \{v' \in V\ |\ v' \sim v\}$$
We call $v + W$ a {\bf coset with representative $v$}.

\begin{proposition} \label{prop-samecoset}
Let $v + W$, $u + W$ be cosets.
\begin{itemize}
    \item $v + W = \{v + w\ |\ w \in W\}$.
    \item As subsets of $V$, either $(v + W) = (u + W)$ are equal or $(v + W) \cap (u + W) = \phi$ are disjoint.
    \item $(v + W) = (u + W)$ iff $u = v+w$ for some $w \in W$.
\end{itemize}
\end{proposition}

\begin{example} \label{eg-xyplane}
    Let $V = \mathbb{R}^3$, $W = \{\begin{pmatrix} x \\ y \\ 0 \end{pmatrix}\ |\ x,y \in \mathbb{R}\}$ be the $xy$-plane. Then the coset
    $\begin{pmatrix} a \\ b \\ c \end{pmatrix} + W$ is the horizontal plane in $\mathbb{R}^3$ elevated/lowered to level $c$. In particular, one has
    $$\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} + W = \begin{pmatrix} 5 \\ 10 \\ 1 \end{pmatrix} + W = \begin{pmatrix} \pi \\ e \\ 1 \end{pmatrix} + W.$$
\end{example}

\begin{definition}
    Let $V$ be a vector space over $\mathbb{F}$, and $W \leq V$. The {\bf quotient space} $V/W$ is defined by the set of cosets:
    $$V/W := \{v + W\ | v \in V\}$$
    (i.e. a `vector' in $v + W \in V/W$ is a coset), with addition and scalar multiplication given by
    \begin{itemize}
        \item $(v+W)+(u+W) := (v+u)+W$;
        \item $\alpha \cdot (v+W) := (\alpha v) + W$
    \end{itemize}
\end{definition}

\begin{remark}
    Although the arithmetic is simple in $V/W$, one needs to be careful that one may have {\bf different} expressions for the {\bf same} element in $V/W$. In such a case, one needs to show that we get the same addition and scalar multiplication even if we use different representatives.

    For instance, suppose 
    \begin{center}
        $u + W = u' + W$ \quad and \quad $v + W = v'+W$ 
    \end{center}
    (for possibly $u \neq u'$ and $v \neq v'$), one needs to show that
    $$(u+W)+(v+W) = (u'+W) + (v'+W).$$
    To see so, note that
    \begin{align*}
    &(u+W)+(v+W) = (u'+W) + (v'+W) \\
      \Leftrightarrow\ &(u+v) + W = (u'+v') +W   \\
      \Leftrightarrow\ &(u+v)-(u'+v') \in W   \\
      \Leftrightarrow\ &(u-u')+(v-v') \in W   
    \end{align*}
    where the second $\Leftrightarrow$ follow from Proposition \ref{prop-samecoset}. On the other hand,
    , since $u + W = u' + W$ and $v + W = v'+W$ one can apply Proposition \ref{prop-samecoset} again to conclude that
    $$u - u',\ v-v' \in W$$
    and hence $(u-u')+(v-v') \in W$ since $W \leq V$ is a vector subspace.
\end{remark}

\begin{example} \
\begin{enumerate}
    \item Let $V = \mathbb{R}^3$ and $W$ is the $xy$-plane as in Example \ref{eg-xyplane}. We have seen that there are a lot of repetitions $v + W = v' + W$. But they are all be reduced to
    $$V/W := \left\{ \begin{pmatrix} 0 \\ 0 \\ z \end{pmatrix} + W\ |\ z \in \mathbb{R}\right\}$$
    and the operations are given by 
    \begin{align*} 
    \left(\begin{pmatrix} 0 \\ 0 \\ z_1 \end{pmatrix} + W\right) + \left(\begin{pmatrix} 0 \\ 0 \\ z_2 \end{pmatrix} + W\right) &= \begin{pmatrix} 0 \\ 0 \\ z_1+z_2 \end{pmatrix} + W,\\ \alpha\left(\begin{pmatrix} 0 \\ 0 \\ z \end{pmatrix} + W\right) &= \begin{pmatrix} 0 \\ 0 \\ \alpha z \end{pmatrix} + W.
    \end{align*}
    So the space is `isomorphic' (once again we have not defined it in this course yet) to $\mathbb{R}$.

    \item     Let $V = \mathbb{F}[x]$, and $W = \{p(x)\ |\ p(x)\ \text{is divisible by}\ (x^2+1)\} = \{(x^2+1)q(x)\ |\ q(x) \in \mathbb{F}[x]\}$ (check the latter is a vector subspace). Then the elements in $V/W$ are of the form
    $p(x) + W$. To remove all repetitions, note that division algorithm of polynomials give
    $$p(x) = (x^2+1)q(x) + r(x)$$
    for $r(x) = ax+b$ of degree less than $2 = \deg(x^2+1)$. Then one has
    $$p(x) +W = r(x) + \overbrace{(x^2+1)q(x)}^{\in W} + W = (ax+b) + W,$$
    where the last equality comes from the last statement of Proposition \ref{prop-samecoset}. In other words,
    $$V/W = \{(ax+b) + W\ |\ a, b \in \mathbb{F}\}$$
    Note that $\{1 + W, x + W\}$ is a basis of $V/W$.

    \item Let $V = \prod_{i \in \mathbb{N}} \mathbb{R}$ and $W = \{(\alpha_1, \alpha_2, \dots) \in V \ |\ \alpha_1  = 0\}$. Then all $(\alpha_1, \alpha_2, \dots) \in V$ can be written as
    $$(\alpha_1, \alpha_2, \dots) = (\alpha_1, 0, 0, \dots) + \overbrace{(0,\alpha_2, \alpha_3, \dots)}^{\in W}$$
    and hence one has 
    $$V/W = \{(\alpha,0,0,\dots) + W\ |\ \alpha \in \mathbb{R}\}.$$
\end{enumerate}
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Linear Transformation}
\section{Basic Definitions}
The notion of linear transformation is the same as in MAT2042. We will quickly go through them in this section.
\begin{definition}[Linear Transformation]
    Let $V$ and $W$ be vector spaces over $\mathbb{F}$. A {\bf linear transformation} from $V$ to $W$ is a map $T:V \to W$ satisfying:
    $$T(\alpha v_1 + \beta v_2) = \alpha T(v_1) + \beta T(v_2)$$
    for all $\alpha, \beta \in \mathbb{F}$ and $v_1, v_2 \in V$.
\end{definition}

\begin{example} \label{eg-lineartrans} \
    \begin{enumerate}
        \item (matrix transformation) Let $A \in M_{m \times n}(\mathbb{F})$, $T: \mathbb{F}^n \to \mathbb{F}^m$ given by 
        $$\quad \quad T(x) := Ax$$
        is a linear transformation.
        \item $T: M_{n \times n}(\mathbb{F}) \to \mathbb{F}$ given by 
        $$T(M) := M_{ij}$$ 
        is a linear transformation.
        \item (trace) $tr: M_{n \times n}(\mathbb{F}) \to \mathbb{F}$ given by 
        $$tr(M) := M_{11}+\dots + M_{nn}$$ 
        is a linear transformation.
        \item (determinant is {\bf not} linear) Let $\mathrm{char}(\mathbb{F}) = 0$ and $\det: M_{n \times n}(\mathbb{F}) \to \mathbb{F}$ given by 
        $$\det(M) := \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \prod_{i=1}^n M_{i,\sigma(i)}$$ 
        is {\bf not} a linear transformation for $n \geq 2$, since
        $\det(2M) = 2^n\det(M) \neq 2\det(M)$ in general.
        \item (differentiation) $D: C^{\infty}(\mathbb{R}) \to C^{\infty}(\mathbb{R})$ given by 
        $$D(f) := f'$$ 
        is a linear transformation.
        \item (integration) $I: C^{\infty}(\mathbb{R}) \to C^{\infty}(\mathbb{R})$ given by 
        $$I(f) := \int_{a}^{x} f(t)dt$$ 
        is a linear transformation.
    \end{enumerate}
\end{example}

\begin{definition}
    Let $V, W$ be vector spaces over $\mathbb{F}$.
\begin{enumerate}
    \item The set of all linear transformations 
    $$\mathcal{L}(V,W) := \{T:V \to W\ |\ T\ \text{is linear transformation}\}$$
    has a vector space structure given by: for $T,S \in \mathcal{L}(V,W)$ and $\alpha \in \mathbb{F}$,
    \begin{itemize}
        \item $(T+S) \in \mathcal{L}(V,W)$ is defined by $(T+S)(v) := T(v) + S(v)$;
        \item $\alpha T \in \mathcal{L}(V,W)$ is defined by $(\alpha T)(v) := \alpha(T(v))$.
        \end{itemize}
    \item If $W = V$, we write $\mathcal{L}(V) := \mathcal{L}(V,V)$, and $T \in \mathcal{L}(V)$ is called a {\bf linear operator} of $V$.
    \item If $W = \mathbb{F}$ (treated as a 1-dimensional vector space), we write $V^* := \mathcal{L}(V,\mathbb{F})$ the {\bf dual vector space} of $V$. And $f \in V^*$ is called a {\bf linear functional} of $V$.
\end{enumerate} 
\end{definition}

The following theorem is well-known and has the same proof as in MAT2042:
\begin{theorem}
    Let $V, W, U$ be vector spaces over $\mathbb{F}$, and $T:V \to W$, $S: W \to U$ are linear transformations. Then the following holds:
    \begin{enumerate}
        \item $T({\bf 0}_V) = {\bf 0}_W$.
        \item The composition $S \circ T: V \to U$ is also a linear transformation.
        \item Suppose $T$ is bijective, then its inverse $T^{-1}: W \to V$ is also a linear transformation. In such a case we call $V \cong W$ are {\bf isomorphic}, and $T$ is an {\bf isomorphism} between $V$ and $W$.
        \item Let $\mathcal{B}$ be a basis of $V$, then $T$ is uniquely determined by the values 
        $$\{T(b)\ |\ b \in \mathcal{B}\}.$$
    \end{enumerate}
\end{theorem}

\begin{remark} \label{rmk-basislineartrans}
As a converse to the last statement of the above theorem, suppose $\mathcal{B} = \{ b_i\ |\ i \in I\}$ is a basis of $V$, and $\{w_i\ |\ i \in I\}$ is any subset of $W$. Then one can define a linear transformation
$T:V \to W$ given by
$$T(\alpha_1b_{i_1} + \dots + \alpha_kb_{i_k}) := \alpha_1w_{i_1} + \dots + \alpha_kw_{i_k}$$
for all $\alpha_l \in \mathbb{F}$ and all finite subset $\{i_1,\dots,i_k\} \subseteq I$. In particular, this is the unique linear transformation satisfying
$$T(b_i) = w_i$$
for all $i \in I$.
\end{remark}

\begin{definition}[Kernel and Image]
    Let $T: V \to W$ be a linear transformation.
    \begin{itemize}
        \item The {\bf kernel} of $T$ is defined by  $ \ker(T) := \{ v \in V \mid T(v) = {\bf 0}_W \} $
        \item The {\bf image} of $T$ is defined by $ \text{im}(T) := \{ T(v) \in W \mid v \in V \} $
    \end{itemize}
\end{definition}

The following result should be well-known in MAT2042:
\begin{theorem}
Let $T:V \to W$ be a linear transformation.
\begin{enumerate}
    \item $\ker(T) \leq V$, $\mathrm{im}(T) \leq W$.
    \item $T$ is injective $\iff$ $\ker(T) = \{{\bf 0}_V\}$.
    \item $T$ is surjective $\iff$ $\mathrm{im}(T) = W$.
\end{enumerate}
\end{theorem}

\begin{example} \label{eg-kerim}
Suppose $\dim(V) = \dim(W) = n$ with $\mathcal{B} = \{v_1, \dots, v_n\}$ and $\mathcal{C} = \{w_1, \dots, w_n\}$ be bases of $V$ and $W$ respectively. Then $T:V \to W$ defined by
$$T(v_i) := w_i$$
(c.f. Remark \ref{rmk-basislineartrans}) is an isomorphism of vector space by checking its kernel and image. The same result holds if $\dim(V)$ and $\dim(W)$ are of countable infinite dimension.

Conversely, if $T: V \to W$ is an isomorphism, then $\dim(V) = \dim(W)$. More precisely, if $\{b_i\ |\ i \in I\}$ is a basis of $V$, then one can check (in Homework) that $\{T(b_i)\ |\ i \in I\}$ is a basis of $W$. 
\end{example}

Finally, we have an important theorem in MAT2042. We will give an alternative proof of it using quotient spaces.
\begin{theorem}[Rank-Nullity Theorem] \label{thm-rn}
    Let $V$ be a finite dimensional vector space over $\mathbb{F}$, and $T:V \to W$ be a linear transformation. Then
    $$\dim(\ker(T)) + \dim(\mathrm{im}(T)) = \dim(V).$$
\end{theorem}
There is an $\dim(V) = \infty$ version of the above theorem. But for practical applications one usually take $\dim(V) < \infty$.

\section{Linear Transformation on Quotient Spaces}
In the previous chapter, we have constructions of new vector spaces from old ones. In this section and the next section, we will construct new linear transformations from old ones.
\begin{proposition}
    Let $V$ be a vector space over $\mathbb{F}$, and $V' \leq V$. The map $\pi_{V'}: V \to V/V'$ given by
    $$\pi_{V'}(v) := v + V'$$
    is a linear transformation and is called {\bf canonical projection}, with $\ker(\pi_{V'}) = V'$ and $\mathrm{im}(\pi_{V'})= V/V'$.
\end{proposition}

\begin{proposition} \label{prop-universalquot}
    Let $T: V \to U$ be a linear transformation. Suppose $S \leq \ker(T)$, then one can define a linear transformation
    $$\overline{T}: V/S \to U$$
    given by 
    $$\overline{T}(v+S) := T(v).$$ 

    In other words, $T = \overline{T} \circ \pi$, i.e. the following diagram commutes:
    \smallskip
\begin{center}
\begin{tikzcd}[%
        ,every arrow/.append style={to}
        ,every label/.append style={font=\normalsize}
        ]
    V \arrow[r, "\pi"] \arrow[dr, "T"'] & V/S \arrow{d}{\overline{T}} \\
     &  U
  \end{tikzcd}
\end{center}
\end{proposition}
\begin{proof}
For the well-definedness of $\overline{T}$, suppose $v_1+S =v_2+S$, then $v_1-v_2 = s \in S$. By Proposition \ref{prop-samecoset}, hence
\begin{center}
    $T(v_1-v_2) = T(s) = {\bf 0}_{W}$ \\
    $T(v_1) = T(v_2)$ \\
    $\overline{T}(v_1+S) = \overline{T}(v_2+S)$.
\end{center}
Therefore, $\overline{T}$ is well-defined.

Then we need to check that $\overline{T}$ is a linear transformation:
\begin{align*}
    \overline{T}(\alpha(v+S)+\beta(u+S)) &= \overline{T}((\alpha v)+S+(\beta u)+S) \\
    &= \overline{T}((\alpha v+\beta u)+S) \\
    &= T(\alpha v+\beta u) \\
    &= \alpha T(v) + \beta T(u) \\
    &= \alpha \overline{T}(v+S) + \beta \overline{T}(u+S)
\end{align*}
for all $v, u \in V$ and $\alpha , \beta \in \mathbb{F}$.
\end{proof}

\begin{theorem}
    Let $T: V \to W$ be a linear transformation. Then there is an isomorphism
    $$V/\ker(T) \cong \mathrm{im}(T).$$
\end{theorem}
    \begin{proof}
By applying Proposition \ref{prop-universalquot} with $S = \ker(T)$, one has a linear transformation $ \overline{T} : V/\ker(T) \to \mathrm{im}(T)$ given by $$ \overline{T}(v+\ker(T)):=T(v) $$
    To conclude the proof, we need to check that $\overline{T}$ is bijective:
    \begin{itemize}
        \item{$\overline{T}$ is surjective:} \\
            Surjectivity is obviously true.
        \item{$\overline{T}$ is injective:} \\
            In this case, one only needs to check $\ker(\overline{T}) = \{{\bf 0}\}$. To see so,
            \begin{align*}
            \quad \quad \quad \overline{T}(v+\ker(T)) = {\bf 0}\
            \Leftrightarrow\ T(v) = {\bf 0}\
            \Leftrightarrow\ v \in \ker(T)\ 
            \Leftrightarrow\ v + \ker(T) = {\bf 0}_{V/\ker(T)} \end{align*}
    \end{itemize}        
    \end{proof}

\begin{corollary}
    The rank-nullity theorem (Theorem \ref{thm-rn}) holds.
\end{corollary}    
\begin{proof}
    By Homework, one has $$\dim(V/\ker(T)) = \dim(V) - \dim(\ker(T)).$$ Also, one has $$\dim(V/\ker(T)) = \dim(\mathrm{im}(T))$$ by Example \ref{eg-kerim}. So the result follows.    
\end{proof}


\section{Dual Spaces}
In this section, we will study some properties and linear transformations related to dual vector spaces $V^* = \mathcal{L}(V,\mathbb{F})$. 

\begin{definition}
    Let $V$ be a vector space over $\mathbb{F}$, and $\mathcal{B} = \{b_i\ |\ i \in I\}$ be a basis of $V$. For each $i \in I$, let $f_i \in V^*$ be given by
    $$f_i(b_j) := \delta_{ij}$$
    (c.f. Remark \ref{rmk-basislineartrans}). Define
    $$\mathcal{B}^* := \{f_i\ | i \in I\}$$
\end{definition}
Note that $\mathcal{B}^*$ and $\mathcal{B}$ have the same cardinality.

\begin{example} \
    \begin{enumerate}
        \item Let $V = \mathbb{F}^n$ and $\mathcal{B} = \{e_1, \dots, e_n\}$ be the canonical basis of $V$. Then $f_i \in V^*$ is defined by
        $$f_i\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} = x_i.$$
        Check that $f_i$ satisfies the condition given by the above definition.

        \item Let $V = M_{n \times n}(\mathbb{F})$ and $\mathcal{B} = \{E_{ij}\ |\ 1 \leq i, j \leq n\}$. Then
        $$f_{ij}(M) := M_{ij}$$
        and hence the trace transformation in Example \ref{eg-lineartrans} satisfies $tr = f_{11} + \dots + f_{nn}$.
    \end{enumerate}
\end{example}

We hope that $\mathcal{B}^*$ is a basis of $V^*$. To check if it is the case, we begin by proving the following:
\begin{proposition}
    $\mathcal{B}^*$ is linearly independent in $V^*$.
\end{proposition}
\begin{proof}
    Let $\{f_{n_i}\}_{1 \leq i \leq n}$ be a finite subset of $\mathcal{B}^*$, and $\alpha_i \in \mathbb{F}$ be such that 
    $$ \sum_{i=1}^{m} \alpha_if_{n_i} = {\bf 0} \quad \text{(where}\ {\bf 0} \in V^*\ \text{is the zero map)}.$$
    Then for each $1 \leq j \leq n$, we have 
    $$\sum_{i=1}^{m} \alpha_i f_{n_i} (b_{n_j}) = 0 \quad \text{(where 0 is the number zero)} $$
    In other words,
    $$\alpha_j = \sum_{i=1}^{m} \alpha_i \delta_{n_i, n_j} = 0.$$
    Therefore we conclude that $\alpha_j=0$ for each $j$, and the result follows.
    \end{proof}

\begin{corollary}
    Let $V$ be a finite dimensional vector space over $\mathbb{F}$ with basis $\mathcal{B}$. Then $\mathcal{B}^*$ is a basis of $V^*$, and $V^* \cong V$ are isomorphic.
\end{corollary}
\begin{proof}
    By Homework, one check that 
    $$\dim(V^*) = \dim(\mathcal{L}(V,\mathbb{F})) = \dim(V) \cdot \dim(\mathbb{F}) = \dim(V) \cdot 1 = \dim(V) = n.$$ 
    Now $\mathcal{B}^*$ is a linearly independent set with $\dim(V^*) = n$ elements. By basis extension (Theorem \ref{thm-basisextension}(2)), one can extend $\mathcal{B}^*$ to a basis $\mathcal{B}^* \sqcup \mathcal{E}$ of $V^*$. But $|\mathcal{B}^*| = |\mathcal{B}|=n$, so $\mathcal{E} = \phi$ is empty, otherwise we will have a basis having more than $n$ elements, violating Theorem \ref{thm-basisextension}(4).

    The last statement of the corollary is given in Example \ref{eg-kerim}.
\end{proof}

How about the case when $\dim(V) = \infty$? We still have $\mathcal{B}^*$ linear independent, but it is no longer a spanning set in general:
\begin{example}
    Let $V = \mathbb{F}[x]$ and $\mathcal{B} = \{x^i\ |\ i \in \mathbb{N} \cup \{0\}\}$. Then $\mathcal{B}^* = \{f_i\ |\ i \in \mathbb{N} \cup \{0\}\}$ with
    $$f_i(\alpha_n x^n + \dots + \alpha_1 x + \alpha_0) := \alpha_i.$$
    Now consider $\phi \in V^*$ given by $\phi(p(x)) := p(1)$. Then we claim that $\phi \notin \mathrm{Span}(\mathcal{B}^*)$ - suppose on contrary
    $$\phi = \gamma_0 f_0 + \dots + \gamma_k f_k,$$
    then applying $x^{k+1}$ on both sides yield:
    \begin{align*}
    \phi(x^{k+1}) &= \gamma_0 f_0(x^{k+1}) + \dots + \gamma_k f_k(x^{k+1}) \\
    1^{k+1} &= \gamma_0 \cdot 0 + \dots + \gamma_k \cdot 0 \\
    1 &= 0
    \end{align*}
    which gives a contradiction.
\end{example}
Indeed, one can check that if $\dim(V) = \infty$ is countable , then $\dim(V^*) = \infty$ is uncountable. We omit the details here.

\section{Annihilator of a Set}
\begin{definition}
    Let $S \subset V$ be a subset. Then {\bf annihilator} of $S$ is 
    $$\mathrm{Ann}(S) := \{f \in V^*\ |\ f(s) = 0\ \text{for all}\ s \in S\}.$$
\end{definition}

\begin{example}\
    \begin{enumerate}
        \item Let $V=\mathbb{F}^{3}$, $\mathcal{B}=\{e_1,e_2,e_3\}$ be the canonical basis and $\mathcal{B}^{*}=\{f_1,f_2,f_3\}$ be the corresponding dual basis. 
        For $S=\left\{ \begin{pmatrix}1\\2\\0\end{pmatrix} \right\}$, one has
        $$f_3\begin{pmatrix}1\\2\\0\end{pmatrix}=0\quad\quad\text{and}
        \quad\quad(6f_1-3f_2)\begin{pmatrix}1\\2\\0\end{pmatrix}=6\cdot 1-3\cdot 2=0,$$
        hence $f_3,6f_1-3f_2\in\mathrm{Ann}(S)$.
        \item Let $V=\mathbb{F}[x]$ and $S=\mathrm{Span}\{1,x\}$. Then $f\in V^{*}$ given by
        $$f(p):=p''(x)$$
        is in $\mathrm{Ann}(S)$.
    \end{enumerate}
\end{example}

\begin{proposition}
Let $V$ be a vector space over $\mathbb{F}$.
    \begin{enumerate}
        \item If $S \subset S'$ then $\mathrm{Ann}(S) \supset \mathrm{Ann}(S')$.
        \item If $W_1, W_2 \leq V$, then 
        \begin{itemize}
            \item $\mathrm{Ann}(W_1 \cap W_2) = \mathrm{Ann}(W_1) + \mathrm{Ann}(W_2)$
            \item $\mathrm{Ann}(W_1 + W_2) = \mathrm{Ann}(W_1) \cap \mathrm{Ann}(W_2)$
        \end{itemize}
        \item If $\dim(V) < \infty$ and $W \leq V$, then $\dim(\mathrm{Ann}(W)) = \dim(V) - \dim(W)$.
    \end{enumerate}
\end{proposition}
By Example \ref{eg-kerim}, if $V$ is finite dimensional, then $\mathrm{Ann}(W)$ and $V/W$ have the same dimension and hence 
$$\mathrm{Ann}(W) \cong V/W.$$
However, the isomorphism given by Example \ref{eg-kerim} is not {\bf natural} (i.e. the isomorphism involves a choice of basis). Moreover, it cannot be generalized to infinite dimensional cases. We wish to construct a natural isomorphism instead, but it will be on the dual space
$$\mathrm{Ann}(W) \cong (V/W)^*.$$

\begin{lemma}[Complementation]
    Let $V$ be a vector space over $\mathbb{F}$, and $W \leq V$. Then there exists a complementary subspace $U \leq V$ such that 
    $$V = W \oplus U.$$
\end{lemma}
\begin{proof}
    Consider the set
    $$\mathcal{S} := \{U'\ |\ U' \leq V, \quad U' \cap W = \{\bf 0\}\}.$$
    Then 
    \begin{itemize}
    \item $\mathcal{S}$ is an ordered set by inclusion as sets; and 
    \item For all $U' \in \mathcal{S}$, the sum $W \oplus U'$ is direct
    \end{itemize}
    By Zorn's Lemma, $\mathcal{S}$ has a maximal element $U$. We claim that 
    $$V = W \oplus U.$$
    Suppose on contrary that there exists $v \in V \backslash (W \oplus U)$. Then by letting $\widetilde{U} := \mathrm{Span}(U \cup \{v\})$, one checks that $\widetilde{U} > U$, and $\widetilde{U} \cap W = \{\bf 0\}$. 
    In other words, $\widetilde{U} \in \mathcal{S}$, violating the maximality of $U$.    
\end{proof}

\begin{proposition}
    Suppose $V = W \oplus U$ as in the Lemma above, then
    $$V/W \cong U.$$
\end{proposition}
\begin{proof}
    Let $p: V \to U$ be defined as follows: for all $v \in V$, $v = w + u$ for some unique choices of $w \in W$ and $u \in U$. Then
    $$p(v) = p(w+u) := u.$$
    One checks that $p$ is a linear transformation, with $\ker(p) = W$ and $\mathrm{im}(p) = U$. Then the result follows from the first isomorphism theorem.
\end{proof}

\begin{theorem}
    Let $V$ be a vector space over $\mathbb{F}$, and $W \leq V$. Then one has a natural isomorphism 
    $$(V/W)^* \cong \mathrm{Ann}(W)$$
\end{theorem}
\begin{proof}
    Take $V = W \oplus U$ as in the complementation lemma, then one has $V/W \cong U$ by the above proposition, and hence it suffices to check that
    $$U^* \cong \mathrm{Ann}(W)$$
    (check that if $V \cong V'$, then $V^* \cong (V')^*$). 

    We now construct $T: U^* \to \mathrm{Ann}(W)$ - for $f \in U^*$, let
    $\widetilde{f}: V \to \mathbb{F}$ by
    $$\widetilde{f}(v) = \widetilde{f}(w+u) := f(u),$$
    where $v = w +u$ is the unique expression of $v$. Then one defines
$$T: U^* \to \mathrm{Ann}(W)$$
    $$T(f) := \widetilde{f}.$$
    There are a few things to check:
    \begin{enumerate}
        \item $\widetilde{f} \in V^*$: let $v = w + u, v' = w' + u'$ be the unique expressions of $v, v' \in V$. Then        
        \begin{align*}
        \widetilde{f}(\alpha v + \beta v') &= \widetilde{f}((\alpha w + \beta w') + (\alpha u + \beta u'))\\ &= f(\alpha u + \beta u')\\ &= \alpha f(u) + \beta f(u')\\ &= \alpha \widetilde{f}(v) + \beta \widetilde{f}(v') \end{align*}
        \item $\widetilde{f} \in \mathrm{Ann}(W)$: for all $w \in W$, $w = {\bf 0} + w$ is its unique expression. So
        $$\widetilde{f}(w) = \widetilde{f}({\bf 0} + w) = f(\bf 0) = 0$$
        \item $T: U^* \to \mathrm{Ann}(W)$ is a linear transformation: for  $f, g \in U^*$, 
        \begin{align*}
        T(\alpha f+ \beta g)(v) &= \widetilde{(\alpha f+ \beta g)}(w+u)\\ 
        &= (\alpha f+ \beta g)(u)\\  &= \alpha f(u)+ \beta g(u) \\
        &= \alpha \widetilde{f}(v)+ \beta \widetilde{g}(v) \\
        &= (\alpha \widetilde{f}+ \beta \widetilde{g})(v). \end{align*}
        So $T(\alpha f+ \beta g) = \alpha \widetilde{f}+ \beta \widetilde{g}$.

        \item $T$ is injective: Suppose $T(f) = \widetilde{f}$ is zero, then
        $\widetilde{f}(w+u) = f(u) = 0$ for all $v = w + u$. Hence $f \in U^*$ is the zero transformation.

        \item $T$ is surjective: for any $g \in \mathrm{Ann}(W)$, then for all $v = w + u \in V$, one has
         $$g(v) = g(w+u) = g(w)+g(u) = g(u).$$
        Define $f \in U^*$ by $f(u) := g(u)$. Then $\mathbb{F}$ is obviously linear, and 
        $$T(f)(v) = \widetilde{f}(v) = f(u) = g(u) = g(v)$$
        for all $v \in V$. So $g = T(f) \in \mathrm{im}(T)$.
    \end{enumerate}
\end{proof}

\section{Universal Properties}
In this section, we will put our constructions of new vector spaces (quotient space, direct sum, direct product) and new linear transformations from the old ones {\bf under a panoramic perspective}. 

\medskip
The following proposition is just re-statement of Proposition \ref{prop-universalquot}:
\begin{proposition} \label{prop-quot}
    Let $V$ be a vector space over $\mathbb{F}$, and $W \leq V$. Consider
    the collection of all linear transformations $\phi$ with $W \leq \ker(\phi)$, i.e.
    $$\mathcal{C}_{qs} := \{(X,\phi)\ |\ \phi: V \to X \quad satisfying \quad W \leq \ker(\phi)\}.$$
    Then
    \begin{itemize}
        \item[(a)] Let $\pi_W: V \to V/W$ be the canonical projection. Then
        $$(V/W, \pi_W) \in \mathcal{C}_{qs}.$$
        \item[(b)] For $T: V \to U$ such that $(U,T) \in \mathcal{C}_{qs}$, there is a uniquely defined $\beta: V/W \to U$ satisfying
        \begin{center}
        $\beta(v+W) := T(v) \quad \Rightarrow \quad T = \beta \circ \pi_W$.
        \end{center}
    \end{itemize}
In other words, there is a unique $\beta$ such that following diagram commutes:
\begin{center}
\begin{tikzcd}[%
        ,every arrow/.append style={to}
        ,every label/.append style={font=\normalsize}
        ]
    V \arrow[r, "\pi_W"] \arrow[dr, "T"'] & V/W \arrow{d}{\beta} \\
     &  U
  \end{tikzcd}
\end{center}
\end{proposition}
\begin{proof}
Statement (a) is easy - since $W \leq \ker(\pi_W) = W$. 
As for (b), since $W \leq \ker(\phi)$ by the fact that $(U,\phi) \in \mathcal{C}_{qs}$, so the hypothesis of Proposition \ref{prop-universalquot} is satisfied with $S = W$, and one can take $\beta = \overline{T}: V/W \to U$ in the proposition with $\beta(v+W) = \beta \circ \pi_W(v) = T(v)$ to obtain the result.
\end{proof}

As for {\it external direct sum}, one has the following:
\begin{proposition}
    Let $\{V_i\ |\ i \in I\}$ be collection of vector spaces over $\mathbb{F}$. Consider the collection of all linear transformations $\phi_j: V_j \to X$, i.e.
    $$\mathcal{C}_{ds} := \left\{(X, \{\phi_j\}_{j \in I})\ |\ \phi_j : V_j \to X \right\}.$$
    Then
    \begin{itemize}
        \item[(a)] For all $j \in I$, let $\iota_j: V_j \to \bigoplus_{i \in I} V_i$ be defined by $\iota_j(v_j) := (\cdots, {\bf 0}, \overbrace{v_j}^{position\ j}, {\bf 0}, \cdots)$. Then
        $$\left(\bigoplus_{i \in I} V_i,\ \{\iota_j\}_{j \in I} \right) \in \mathcal{C}_{ds};$$
        \item[(b)] For $T_j: V_j \to U$ such that $(U, \{T_j\}_{j \in I}) \in \mathcal{C}_{ds}$, there is a unique $\beta: \bigoplus_{i \in I} V_i \to U$ such that 
        \begin{center}
        $\beta((\cdots, v_j, \cdots)) := \sum_{j \in I} T_j(v_j) \quad \Rightarrow \quad T_j = \beta \circ \iota_j$ for all $j \in I$
        \end{center}
        (note that the sum $\sum_{j \in I} T_j(v_j)$ is finite by the definition of external direct sum).
    \end{itemize}
In other words, there is a unique $\beta$ making the following diagram commute:
\begin{center}
\begin{tikzcd}[%
        ,every arrow/.append style={to}
        ,every label/.append style={font=\normalsize}
        ]
    V_j \arrow[r, "\iota_j"] \arrow[dr, "T_j"'] & \bigoplus_{i \in I} V_i \arrow{d}{\beta} \\
     &  U
  \end{tikzcd}
\end{center}
\end{proposition}

While for {\it external direct product}, things are slightly different:
\begin{proposition}
    Let $\{V_i\ |\ i \in I\}$ be collection of vector spaces over $\mathbb{F}$. Consider the collection of all linear transformations $\psi_j: X \to V_j$, i.e.
    $$\mathcal{C}_{dp} := \left\{(X, \{\psi_j\}_{j \in I})\ |\ \psi_j : X \to V_j \right\}.$$
    Then
    \begin{itemize}
        \item[(a)] For all $j \in I$, let $\tau_j: \prod_{i \in I} V_i \to V_j$ be defined by $\tau_j((\cdots, \overbrace{v_j}^{position\ j}, \cdots)) := v_j$. Then
        $$\left(\prod_{i \in I} V_i,\ \{\tau_j\}_{j \in I} \right) \in \mathcal{C}_{dp};$$
        \item[(b)] For $T_j: U \to V_j$ such that $(U, \{T_j\}_{j \in I}) \in \mathcal{C}_{dp}$, one has $\gamma: U \to \prod_{i \in I} V_i$  such that
        \begin{center}
        $\gamma(u) := (\cdots, T_i(u), \cdots) \quad \Rightarrow \quad T_j = \tau_j \circ \gamma$ for all $j \in I$
        \end{center}
    \end{itemize}
In other words, there is a unique $\gamma$ making the following diagram commute:
\begin{center}
\begin{tikzcd}[%
        ,every arrow/.append style={to}
        ,every label/.append style={font=\normalsize}
        ]
    U \arrow[dr, "T_j"] \arrow[d, "\gamma"'] \\
    
    \prod_{i \in I} V_i \arrow[r, "\tau_j"'] 
     &  V_j
  \end{tikzcd}
\end{center}
\end{proposition}

\section{Rough Introduction to Category Theory} \label{sec-univ}
Category theory is used to describe similarities between different branches of mathematics. These branches have something in common - (1) certain `objects' with some specified structures, and (2) certain `maps' between these objects preserving their structures. Examples include:
\begin{itemize}
    \item(MAT 2040) $n$-vectors $\{\mathbb{R}^n\}_{n \in \mathbb{N}}$ and matrix transformations $\{A: \mathbb{R}^n \to \mathbb{R}^m\}$;
    \item(MAT 2042) Vectors spaces $\{V\}$ and linear transformations $\{T: V \to W\}$;
    \item(MAT 3004) Groups $\{G\}$ and homomorphisms $\{\phi: G \to H\}$;
    \item(MAT 4002) Topological spaces $\{X\}$ and continuous functions $\{f: X \to Y\}$;
\end{itemize}

Here is a formal definition:
\begin{definition}
    A (small) category $\mathcal {C}$ consists of the following three mathematical entities:
\begin{itemize}
\item A set of objects $\mathrm{Obj}(\mathcal{C})$ (or simply $\mathcal{C}$);
\item For every $X, Y \in \mathrm{Obj}(\mathcal{C})$, a set of morphisms  
$$\mathrm{Hom}(X,Y) := \{f: X \to Y\}$$
\item For every $X, Y, Z \in \mathrm{Obj}(\mathcal{C})$, one can compose morphisms:
$$\circ: \mathrm{Hom}(X,Y) \times \mathrm{Hom}(Y,Z) \to \mathrm{Hom}(X,Z)$$
$$(f,g) \mapsto g \circ f$$
satisfying
\begin{enumerate}
    \item For all morphisms $f, g$ and $h$, one has $(f \circ g) \circ h = f \circ (g \circ h)$.
    \item For any $X \in \mathrm{Obj}(\mathcal{C})$, there exists an identity element $1_X \in \mathrm{Hom}(X,X)$ such that
    $$1_X \circ f = f \quad \quad g \circ 1_X = g$$
    for all $f \in \mathrm{Hom}(U,X)$ and $g \in \mathrm{Hom}(X,Y)$. 
\end{enumerate}
\end{itemize}
\end{definition}

\begin{example}
Here are some examples of categories:
    \begin{enumerate}
        \item $\mathcal{C}_{set} = \{$all possible sets $A\}$, $\mathrm{Hom}(A,B) =\{$all functions $f:A \to B\}$.
        \item $\mathcal{C}_{vec} = \{$all vectors $\mathbb{R}^n\ | n \in \mathbb{N}\}$, $\mathrm{Hom}(\mathbb{R}^n,\mathbb{R}^m) =\{$all matrix transformations $A: \mathbb{R}^n \to \mathbb{R}^m\} = M_{m \times n}(\mathbb{R})$.
        \item $\mathcal{C}_{vs} = \{$all vectors spaces $V\}$, $\mathrm{Hom}(V,W) =\{$all linear transformations $T: V \to W\}$.
    \end{enumerate}
\end{example}

\begin{definition}
    Let $\mathcal{C}$ be a category. An {\bf initial object} is an object $\mathcal{I} \in \mathrm{Obj}(\mathcal{C})$ such that for any $X \in \mathrm{Obj}(\mathcal{C})$, there exists exactly one
    $$i_x: \mathcal{I} \to X  \quad \quad in\ \mathrm{Hom}(\mathcal{I},X).$$
A {\bf terminal object} is an object $\mathcal{T} \in \mathrm{Obj}(\mathcal{C})$ such that for any $X \in \mathrm{Obj}(\mathcal{C})$, there exists exactly one
    $$t_x: X \to \mathcal{T} \quad \quad in\ \mathrm{Hom}(X,\mathcal{T}).$$
\end{definition}

\begin{example}
    Consider $\mathcal{C}_{vs}$. Then $\mathcal{I} = \{\bf 0 \}$ is an initial object. Namely, for all $W \in \mathrm{Obj}(\mathcal{C}_{vs})$, 
    there is only one possible linear transformation $i_W: \mathcal{I} \to W$ given by:
    $$i_W(\bf 0) := \bf 0_W$$
    Similarly, $\mathcal{T} = \{\bf 0 \}$ is also a terminal object, since there is only one possible linear transformation $t_W: W \to \mathcal{T}$
    given by:
    $$i_W(w) := {\bf 0} \quad \quad for\ all\ w\in W. $$
\end{example}

In general, initial objects and terminal objects may not exist in a category. But if it does, it possesses some very nice properties: 
\begin{remark}
\begin{enumerate}
    \item Suppose $\mathcal{I}$ is an initial object, then $i_{\mathcal{I}}: \mathcal{I} \to \mathcal{I}$ must be the identity map $i_{\mathcal{I}} = 1_{\mathcal{I}}$. 
    
    \item Similarly, if $\mathcal{T}$ is a terminal object, then $t_{\mathcal{T}}: \mathcal{T} \to \mathcal{T}$ must be the identity map $t_{\mathcal{T}} = 1_{\mathcal{T}}$. 

    \item Suppose $\mathcal{I} \in \mathrm{Obj}(\mathcal{C})$ is an initial object. Then for any $V, W \in \mathrm{Obj}(\mathcal{C})$ and $\beta \in \mathrm{Hom}(V,W)$, $\beta \circ i_V \in \mathrm{Hom}(\mathcal{I},W)$. But there is only one element $i_W \in \mathrm{Hom}(\mathcal{I},W)$ by the definition of initial object, so
    $$i_W = \beta \circ i_V,$$
    i.e. the following diagram commutes for all $V, W, X \in \mathrm{Obj}(\mathcal{C})$:
    \begin{center}
\begin{tikzcd}[%
        ,every arrow/.append style={to}
        ,every label/.append style={font=\normalsize}
        ]
    \mathcal{I} \arrow[r, "i_V"] \arrow[dr, "i_W"] \arrow[ddr, "i_X"'] & V \arrow{d}{\beta} \\
     &  W \arrow{d}{\beta'} \\
     & \begin{matrix} X  \\ \vdots \end{matrix} 
  \end{tikzcd}
\end{center}

    \item Similarly, if $\mathcal{T} \in \mathrm{Obj}(\mathcal{C})$ is an initial object. Then for any $V, W \in \mathrm{Obj}(\mathcal{C})$ and $\gamma \in \mathrm{Hom}(V,W)$, $\iota_W \circ \gamma \in \mathrm{Hom}(V,\mathcal{T})$. But there is only one element $t_V \in \mathrm{Hom}(V,\mathcal{T})$ by the definition of terminal object, so
    $$i_V = i_W \circ \gamma,$$
    i.e. the following diagram commutes for all $U, V, W \in \mathrm{Obj}(\mathcal{C})$:
\begin{center}
\begin{tikzcd}[%
        ,every arrow/.append style={to}
        ,every label/.append style={font=\normalsize}
        ]

    \begin{matrix} \vdots \\ U\end{matrix} \arrow[d, "\gamma'"'] \arrow[ddr, "t_U"] \\    
    V \arrow[dr, "t_V"'] \arrow[d, "\gamma"'] \\
    
    W \arrow[r, "t_W"'] 
     &  \mathcal{T}
  \end{tikzcd}
\end{center}
\end{enumerate}
\end{remark}

\begin{proposition}
    Let $V$ be a vector space over $\mathbb{F}$, and $W \leq V$ be a fixed vector subspace of $V$. Consider the category
    $$\mathcal{C}_{qs} := \{(X,\phi)\ |\ \phi:V \to X \quad such\ that\quad W \leq \ker(\phi)\}$$
    with 
    $$\mathrm{Hom}((X,\phi),(Y,\psi)) := \{\beta:X \to Y\ |\ \psi = \beta \circ \phi\}$$ 
    for $(X,\phi),(Y,\psi) \in \mathcal{C}_{qs}$. Then
    $$\mathcal{I}_{qs} = (V/W,\pi_W)$$
    is an initial object in $\mathcal{C}_{qs}$.
 \end{proposition}
\begin{proof}
    By Proposition \ref{prop-quot}, for each $(U,T) \in \mathcal{C}_{qs}$, there exists a unique $\beta = \overline{T}: V/W \to U$ such that $T = \beta \circ \pi_W$. So  
    $$\mathrm{Hom}((V/W,\pi_W),(U,T)) := \{\beta\}$$ 
    has exactly one element.
\end{proof}
Similarly, one has
\begin{proposition}
    Let $\{V_i\ |\ i \in I\}$ be collection of vector spaces over $\mathbb{F}$. Consider the category
    $$\mathcal{C}_{ds} := \left\{(X, \{\phi_j\}_{j \in I})\ |\ \phi_j : V_j \to X \right\}.$$
    with
$$\mathrm{Hom}((X,\{\phi_j\}_{j \in I}),(Y,\{\phi_j'\}_{j \in I})) := \{\beta:X \to Y\ |\ \phi_j = \beta \circ \phi_j'\ \text{for all}\ j\}$$ 
for $(X,\{\phi_j\}_{j \in I}), (Y,\{\phi_j'\}_{j \in I}) \in \mathrm{Obj}(\mathcal{C}_{ds})$. Then 
$$\mathcal{I}_{ds} = \left(\bigoplus_{i \in I} T_i(v_i), \{\iota_j\}_{j \in I}\right)$$ 
is an initial object in $\mathcal{C}_{ds}$.
\end{proposition}
As for external direct product, one has
\begin{proposition}
    Let $\{V_i\ |\ i \in I\}$ be collection of vector spaces over $\mathbb{F}$. Consider the category
    $$\mathcal{C}_{dp} := \left\{(X, \{\psi_j\}_{j \in I})\ |\ \psi_j : X \to V_j \right\}.$$
    with 
$$\mathrm{Hom}((X,\{\psi_j\}_{j \in I}),(Y,\{\psi_j'\}_{j \in I})) := \{\gamma:X \to Y\ |\ \psi_j = \psi_j' \circ \gamma\ \text{for all}\ j\}$$ 
 for $(X,\{\psi_j\}_{j \in I}), (Y,\{\psi_j'\}_{j \in I}) \in \mathrm{Obj}(\mathcal{C}_{ds})$. Then 
$$\mathcal{I}_{dp} = \left(\prod_{i \in I} T_i(v_i), \{\tau_j\}_{j \in I}\right)$$ 
is a terminal object in $\mathcal{C}_{dp}$.
\end{proposition}
These observations seems very complicated, but they will become very useful in understand tensor products and linear transformations on tensor products in the next chapter.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Tensor Product and the Determinant}

\section{Motivation} We begin by giving some motivations on studying tensor products:

\begin{definition}
Let $V_1, V_2, W$ be vector spaces over $\mathbb{F}$. A {\bf bilinear map} is a function $$f : V_1 \times V_2 \to W$$ satisfying
$$f(\alpha v_1 + \beta v_1', v_2) = \alpha f(v_1, v_2) + \beta f(v_1', v_2), \quad f(v_1, \alpha v_2 + \beta v_2') = \alpha f(v_1, v_2) + \beta f(v_1, v_2').$$

More generally, let $V_1, \dots, V_k, W$ be vector spaces over $\mathbb{F}$. A {\bf $k$-linear map} is a function $$f : V_1 \times \dots \times V_k \to W$$ satisfying
$$f(\cdots, v_{i-1}, \alpha v_i + \beta v_i', v_{i+1}, \cdots) = \alpha f(\cdots, v_{i-1}, v_i, v_{i+1}, \cdots) + \beta f(\cdots, v_{i-1}, v_i', v_{i+1}, \cdots).$$
for all $1 \leq i \leq k$.
\end{definition}

\begin{example} \label{eg-klinear}\
    \begin{enumerate}
        \item The inner product of real vector spaces (e.g. dot product) 
        $$\langle\ ,\ \rangle: V \times V \to \mathbb{R}$$
        is bilinear.
        \item The cross product of $\mathbb{R}^3$ 
        $$\times: \mathbb{R}^3 \times \mathbb{R}^3 \to \mathbb{R}^3$$
        is bilinear.
        \item The determinant function:
        $$\det: \overbrace{\mathbb{F}^n \times \dots \times \mathbb{F}^n}^{n\ copies} \to \mathbb{F}$$
        $$\det(v_1, \dots, v_n) := \det((v_1\ |\ \dots\ |\ v_n))$$
        is $n$-linear.
    \end{enumerate}
\end{example}

One would like to use linear algebra to study $k$-linear maps. However, they are almost always not a linear transformation if we treat $V_1 \times \dots \times V_k = \prod_{i = 1}^k V_i$ as a external direct product space:
For instance, if $f : V_1 \times V_2 \to W$ is bilinear, then 
$$f(3(v_1,v_2)) = f({\color{blue} 3}v_1, {\color{red} 3}v_2) = {\color{blue} 3}f(v_1, {\color{red} 3}v_2) = {\color{blue} 3}\cdot {\color{red} 3}f(v_1,v_2) \neq 3f(v_1,v_2).$$

Inspired by Section \ref{sec-univ}, one would like to make the following:
\begin{definition}[Universal Property of Tensor Product] 
Let $V_1, \dots, V_k$ be vector spaces over $\mathbb{F}$. Consider the category
$$\mathcal{C}_{tp} := \{(W,f)\ |\ f: V_1 \times \dots \times V_k \to W\ \text{is}\ k\text{-linear}\}$$
with
$$\mathrm{Hom}((W,f),(U,g)) := \{\beta : W \to U\ |\ g = \beta \circ f\}.$$
Then the {\bf tensor product}
$$\mathcal{I}_{tp} := (V_1 \otimes \dots \otimes V_k,\ \iota: V_1 \times \dots \times V_k \to V_1 \otimes \dots \otimes V_k)$$
(both terms to be determined later) is defined to be the initial object in $\mathcal{C}_{tp}$.
\end{definition}

\begin{remark}
    Note that there is a {\bf ``unique'' initial object} in any category $\mathcal{C}$ - more precisely, in the Homework set, you will prove that
    if $\mathcal{I}$ and $\mathcal{I}'$ are both initial objects, then there exists a unique isomorphism
    $$i_{\mathcal{I}'}: \mathcal{I} \xrightarrow{\cong} \mathcal{I}'$$
    between them. In other words, the initial object is {\bf unique up to unique isomorphism.}
 \end{remark}

By the property of initial object, we can study all $k$-linear maps
$$g: V_1 \times \dots \times V_k \to U$$
as follows:
\begin{itemize}
    \item $\mathrm{Hom}((V_1 \otimes \dots \otimes V_k,\ \iota), (U,g)) = \{\beta\}$ has only one element; so
    \item $g = \beta \circ \iota$, where 
    $$\beta: V_1 \otimes \dots \otimes V_k \to U$$ 
    is a {\bf linear transformation}
\end{itemize}
In other words,
\begin{center}
    \fbox{understand {\bf $k$-linear map} $g$ $\leftrightarrow$ understand {\bf linear transformation} $\beta$}
\end{center}

\medskip
\noindent {\bf Question:} how to construct $V_1 \otimes \dots \otimes V_k$ and  $\iota: V_1 \times \dots \times V_k \to V_1 \otimes \dots \otimes V_k$?


\section{Construction of Tensor Product Space} 
For simplicity, we will only construct $V \otimes W$ in this section. The general case with $k$ vector spaces $V_1 \otimes \dots \otimes V_k$ can be constructed similarly.

\begin{definition} 
Let $V,W$ be vector spaces over $\mathbb{F}$. Consider the set
$$\mathcal{S} = \{(v,w)\ |\ v \in V,\ w \in W\},$$
then we define the vector space
$$\mathcal{X} = \mathrm{Span}(\mathcal{S}) := \{\sum_{i=1}^k \alpha_i(v_i,w_i)\ |\ \alpha_i \in \mathbb{F},\ (v_i,w_i) \in \mathcal{S},\ k \in \mathbb{N}\},$$ with the usual addition and scalar multiplication rule, so that $\mathcal{S}$ is a basis of $\mathcal{X}$.
\end{definition}

\begin{remark}
Note that we only consider $\mathcal{S}$ as a {\bf set}, but not a vector space $V \times W$. In particular, there are no relations on the elements $(v,w) \in \mathcal{X}$. For instance $s := (v,w)$ and $s' := (2v, 2w)$ are two different elements in $\mathcal{S}$, so
    $$(2v, 2w) \neq 2(v,w)$$
in $\mathcal{X}$, since $s' \neq 2s$ for two linearly independent vectors in $\mathcal{X}$.

Similarly, $s_1 = ({\bf 0}, w)$, $s_2 = (v,{\bf 0})$ and $s_3 = (v,w)$ are three different elements in $\mathcal{S}$, so
    $$({\bf 0}, w) + (v,{\bf 0}) \neq (v,w)$$
in $\mathcal{X}$, since $s_1 + s_2 \neq s_3$ for three linearly independent vectors in $\mathcal{X}$.  

The only legitimate rule in $\mathcal{X}$ is
    $$3(v,w) + 2(v,w) = 5(v,w)$$
    (and is {\bf not} equal to $(5v,5w)$!).
\end{remark}


\begin{definition} Let $\mathcal{Y} \leq \mathcal{X}$ be a vector subspace spanned by vectors of the form
$$\{1\cdot (v_1+v_2,w) - 1\cdot (v_1,w) - 1\cdot (v_2,w)\}\quad \quad \{1\cdot (v,w_1 + w_2) - 1\cdot (v,w_1) - 1\cdot (v,w_2)\}$$
and
$$\{(\alpha v,w) - \alpha (v,w)\} \quad \quad \{(v, \alpha w) - \alpha(v,w) \}$$
for all $v,v_1,v_2 \in V$, $w, w_1, w_2 \in W$ and $\alpha \in \mathbb{F}$. 

Then the {\bf tensor product} $V \times W$ is defined by 
the quotient space
$$V \otimes W := \mathcal{X}/\mathcal{Y},$$
For $v \in V$, $w \in W$, we define $v \otimes w \in V \otimes W = \mathcal{X}/\mathcal{Y}$ by:
$$v \otimes w := (v,w) + \mathcal{Y}.$$
\end{definition}

We now see some arithmetic in $V \otimes W$:
\begin{example} \label{eg-tensorarith} 
In $V \otimes W$, one has:
\begin{align*}
(v_1 + v_2) \otimes w &= (v_1 + v_2,w) + \mathcal{Y}\\
&= ((v_1 + v_2,w) - ((v_1 + v_2,w) - (v_1,w) - (v_2,w))) + \mathcal{Y} \\
&= ((v_1,w) + (v_2,w)) + \mathcal{Y}\\
&= [(v_1,w) + \mathcal{Y}] + [(v_2,w) + \mathcal{Y}]\\
&= v_1 \otimes w + v_2 \otimes w
\end{align*}
Similarly, one has
\begin{align*}
v \otimes (w_1 + w_2) &= (v \otimes w_1) + (v \otimes w_2)\\
(\alpha v) \otimes w &= \alpha (v \otimes w)\\
v \otimes (\alpha w) &= \alpha (v \otimes w)    
\end{align*}
\end{example}

\begin{example} Let $V =W = \mathbb{R}^2$. Then $\begin{pmatrix} 3\\ 1\end{pmatrix} \otimes \begin{pmatrix} -4 \\ 2 \end{pmatrix}$ can be rewritten in term of $e_1$ and $e_2$:
\begin{align*}
\begin{pmatrix} 3\\ 1\end{pmatrix} \otimes \begin{pmatrix} -4 \\ 2 \end{pmatrix} &= (3e_1 + 2e_2) \otimes (-4e_1 + 2e_2) \\
&= (3e_1) \otimes (-4e_1 + 2e_2) + (e_2) \otimes (-4e_1 + 2e_2) \\
&= (3e_1) \otimes (-4e_1) + (3e_1) \otimes (2e_2) + (e_2) \otimes (-4e_1) + e_2 \otimes (2e_2)\\
&= -12(e_1 \otimes e_1) + 6(e_1 \otimes e_2) - 4(e_2 \otimes e_1) + 2(e_2 \otimes e_2)
\end{align*}
As an exercise, check that $e_1 \otimes e_2 + e_2 \otimes e_1$ cannot be re-written as $(ae_1 + be_2) \otimes (ce_1 + de_2)$ for any $a, b, c, d \in \mathbb{R}$.
\end{example}


\begin{remark}
We mention some fundamental differences between product space $V \times W$ and the tensor product space $V \otimes W$:
\begin{enumerate}
    \item $(v,{\bf 0}) \neq {\bf 0}_{V\times W}$ in $V \times W$, but $v \otimes {\bf 0} = {\bf 0}_{V \otimes W}$:
    $$v \otimes {\bf 0} = v \otimes (0 \cdot w) = 0(v \otimes w) = {\bf 0}_{V \otimes W}$$
    \item  $(v_1,w_1) + (v_2,w_2) = (v_1 + v_2,w_1 + w_2)$ in $V \times W$,  but $(v_1 \otimes w_1) + (v_2 \otimes w_2)$ cannot be simplified further in  general for $V \otimes W$. Therefore, a general element in $V \otimes W = \mathcal{X}/\mathcal{Y}$ is of the form:
    $$(\alpha_1 (v^{(1)}, w^{(1)}) + \dots + \alpha_l (v^{(l)}, w^{(l)})) + \mathcal{Y} \quad \in \mathcal{X}/\mathcal{Y},$$
    which is equal to
    $$[(\alpha_1v^{(1)}, w^{(1)}) + \mathcal{Y}] + \dots + [(\alpha_lv^{(l)}, w^{(l)}) + \mathcal{Y}]$$
    by the addition rule of quotient space and the calculations in Example \ref{eg-tensorarith} above. {\bf So, a general element in $V \otimes W$ is:}
    $$\widetilde{v}^{(1)} \otimes w^{(1)} + \dots + \widetilde{v}^{(l)} \otimes w^{(l)}$$
    where $\widetilde{v}^{(i)} = \alpha_i v^{(i)} \in V$ is an arbitrary vector. 
\end{enumerate}
\end{remark}



Now we prove that our definition of $V \otimes W = \mathcal{X}/\mathcal{Y}$ satisfies the universal property:
\begin{theorem} Let $V \otimes W = \mathcal{X}/\mathcal{Y}$ be as defined above. Consider the map 
$$\iota : V \times W \to V \otimes W$$ 
defined by
$$\iota(v,w) := v \otimes w$$
Then $\iota$ is a bilinear map, and $(V \otimes W, \iota)$ is the initial object for $\mathcal{C}_{tp}$.
\end{theorem}
\begin{proof}
    We firstly check that if $\iota$ is a bilinear map. By Example \ref{eg-tensorarith}, one has
    \begin{align*}
        \iota(v_1+v_2, w) = (v_1+v_2)\otimes w &= v_1 \otimes w + v_2 \otimes w = \iota(v_1, w) + \iota(v_2, w) \\
        \iota(v, w_1+w_2) = v\otimes (w_1+w_2) &= v \otimes w_1 + v \otimes w_2 = \iota(v, w_1) + \iota(v, w_2) \\
        \iota(\alpha v, w) = (\alpha v)\otimes w &= \alpha (v \otimes w) = \alpha \iota(v, w) \\
        \iota(v, \alpha w) = v\otimes (\alpha w) &= \alpha (v \otimes w) = \alpha \iota(v, w).
    \end{align*}
    Therefore, $\iota$ is a bilinear map.
    
    To show that $(V \otimes W, \iota)$ is the initial object for $\mathcal{C}_{tp}$, one needs to show that for any {\bf bilinear map}
    $$f : V \times W \to U,$$
    there is a unique {\bf linear transformation}
    $$\beta : V \otimes W \to U$$
    such that $f = \beta \circ \iota$, that is
    $$\beta (v \otimes w) = f(v, w).$$

    \medskip
    Let $f : V \times W \to U$ be bilinear. Define a linear transformation
    $$\Phi : \mathcal{X} (:= \mathrm{Span}(\mathcal{S})) \to U$$
    given by 
    $$\Phi(v, w) := f(v,w)$$
    for all $(v,w) \in \mathcal{S}$ (recall $\mathcal{S}$ is a basis of $\mathcal{X}$, and by Remark \ref{rmk-basislineartrans}, we describe the linear transformation $\Phi$ by specifying its image under the basis of $\mathcal{X}$). Then 
    \begin{align*}
        \Phi((v_1+v_2, w)-(v_1, w)-(v_2, w)) &= \Phi(v_1+v_2, w)- \Phi(v_1, w)- \Phi(v_2, w) \\
        &= f(v_1+v_2, w)-f(v_1, w)-f(v_2, w) \\
        &= 0
    \end{align*}
    hence $(v_1+v_2, w)-(v_1, w)-(v_2, w) \in \mathrm{ker}(\Phi).$
    Similarly,
    \begin{align*}
        (v, w_1+w_2)-(v, w_1)-(v, w_2) &\in \mathrm{ker}(\Phi) \\
        (\alpha v, w)-\alpha (v, w) &\in \mathrm{ker}(\Phi) \\
        (v, \alpha w)-\alpha (v, w) &\in \mathrm{ker}(\Phi).
    \end{align*}
    Therefore $\mathcal{Y} \leq \mathrm{ker}(\Phi)$.
    By Proposition \ref{prop-quot}, there is a unique linear transformation
    $$\overline{\Phi} : \mathcal{X}/\mathcal{Y} \to U$$
    such that
    $$\overline{\Phi}((v, w)+\mathcal{Y}) = f(v, w).$$
    Then one can take $\beta := \overline{\Phi} : V \otimes W \to U$ such that
    $$\beta(v \otimes w) = \overline{\Phi}(v \otimes w) = f(v, w).$$
\end{proof}

\begin{corollary}[Universal Property of Tensor Product]
    Let $f:V \times W \to U$ be a bilinear map. Then there exists a unique linear transformation $\beta: V\otimes W \to U$ such that 
    $$\beta(v \otimes w) = f(v,w)$$
    for all $v \in V$ and $w \in W$. 
\end{corollary}
In other words, $\beta$ contains all information about $\mathbb{F}$.


\section{Basis of Tensor Product}
Let $V, W$ be finite-dimensional vector spaces over $\mathbb{F}$, with 
$\{v_1, \dots , v_n\}$, $\{w_1, \dots , w_m\}$ being bases of $V$ and $W$. In this section, we will prove that 
$$\mathcal{B} := \{v_i \otimes w_j | 1 \leq  i \leq  n, 1 \leq  j \leq  m\}$$
is a basis of $V \otimes W$.
\begin{proposition} $\mathcal{B}$ spans the tensor product space $V \otimes W$.
\end{proposition}
\begin{proof}
Note that a general element in $V \otimes W$ is of the form $v^{(1)} \otimes w^{(1)} + \dots + v^{(r)} \otimes w^{(r)}$. So it suffices to express each $v \otimes w := v^{(l)} \otimes w^{(l)}$ in terms of linear combinations of $v_i \otimes w_j$.

Suppose that $v = \alpha_1v_1 + \dots + \alpha_nv_n$ and $w = \beta_1w_1 + \dots + \beta_mw_m$. So
\begin{align*}
v \otimes w &= (\alpha_1v_1 + \dots + \alpha_nv_n) \otimes w \\
&= (\alpha_1v_1) \otimes w + \dots + (\alpha_nv_n) \otimes w \\
&= \alpha_1(v_1 \otimes w) + \dots + \alpha_n(v_n \otimes w) 
\end{align*}
For each $v_i \otimes w$, $v_i \otimes w = \beta_1(v_i \otimes w_1) + \dots + \beta_m(v_i \otimes w_m)$. Therefore,
\begin{align*}
v \otimes w =
\sum_{i=1}^n \sum^m_{j=1} \alpha_i \beta_j (v_i \otimes w_j)
\end{align*}
and the result follows.
\end{proof}

\begin{theorem}
    $\mathcal{B}$ is linearly independent in $V \otimes W$. So it is a basis of $V \otimes W$.
\end{theorem}
\begin{proof}
Suppose
    \begin{align*}
\sum_{i=1}^n \sum^m_{j=1} \alpha_{ij} (v_i \otimes w_j) = {\bf 0}
\end{align*}
for some $\alpha_{ij} \in \mathbb{F}$, then we will use universal property to show that all $\alpha_{ij}$ are zeros.

Let $\{\phi_1, \dots ,\phi_n\}$ be the dual basis of $V^*$, and $\{\psi_1, \dots ,\psi_m\}$ be the dual basis of $W^*$. For $1 \leq p \leq n$ and $1 \leq q \leq m$, define
$$\pi_{p,q} : V \times W \to \mathbb{F}$$
by $\pi_{p,q}(v,w) := \phi_p(v) \psi_q(w)$. Then it is easy to check that $\pi_{p,q}$ is bilinear: for instance,
\begin{align*}
\pi_{p,q}(\alpha v_1 + \beta v_2,w) &= \phi_p( \alpha v_1 + \beta v_2) \psi_q(w) \\
&= (\alpha \phi_p(v_1) + \beta\phi_p(v_2)) \psi_q(w) \\
&= \alpha \phi_p(v_1) \psi_q(w) + \beta\phi_p(v_2) \psi_q(w) \\
&= \alpha \pi_{p,q}(v_1,w) + \beta\pi_{p,q}(v_2,w).
\end{align*}
Therefore, $(\mathbb{F}, \pi_{p,q}) \in \mathcal{C}_{tp}$. By the universal property of the tensor product, there is a unique linear transformation
$$\Pi_{p,q} : V \otimes W \to \mathbb{F}$$
with $\pi_{p,q}(v,w) = \Pi_{p,q} \circ \iota(v,w)$.
In other words,
$$\Pi_{p,q}(v \otimes w) = \pi_{p,q}(v,w) = \phi_p(v) \psi_q(w).$$
Applying the mapping $\Pi_{p,q}$ to
$$\sum_{i=1}^n \sum^m_{j=1} \alpha_{ij} (v_i \otimes w_j) = {\bf 0}$$
one has:
\begin{align*}
\Pi_{p,q}\left(\sum_{i=1}^n \sum^m_{j=1} \alpha_{ij} (v_i \otimes w_j)\right) &= \Pi_{p,q}({\bf 0}) \\
\sum_{i=1}^n \sum^m_{j=1} \alpha_{ij} \Pi_{p,q}\left((v_i \otimes w_j)\right) &= 0 \\
\sum_{i=1}^n \sum^m_{j=1} \alpha_{ij} \pi_{p,q}(v_i, w_j) &= 0 \\
\sum_{i=1}^n \sum^m_{j=1} \alpha_{ij} \phi_{p}(v_i)\psi_q(w_j) &= 0 \\
\alpha_{pq} &= 0
\end{align*}
for all $p, q$. So the result follows.
\end{proof}

\begin{remark} \label{rmk-inftensor}
    Note that the above proofs work perfectly well for infinite-dimensional vector spaces. Namely: 
    Suppose $\{v_i\ |\ i \in I\}$ is a basis of $V$ and $\{w_j\ |\ j \in J\}$ is a basis of $W$. Then
    $$\mathcal{B} := \{v_i \otimes w_j\ |\ i\in I,\ j \in J\}$$
    is a basis of $V \otimes W$.
\end{remark}


\begin{corollary}
If $\dim(V), \dim(W) < \infty$, then $\dim(V \otimes W) = \dim(V)\dim(W)$.
\end{corollary}

In the proof of the above theorem, one finds it very helpful to construct a linear transformation on $V \otimes W$ by a bilinear map on $V \times W$.
We do one more example of such trick:

\begin{theorem}
    Let $V, W$ be vector spaces over $\mathbb{F}$ (not necessarily finite dimensional). Then 
    $$V \otimes W \cong W \otimes V.$$
\end{theorem}
\begin{proof}
    Define
$$\phi : V \times W \to W \otimes  V$$
by $\phi(v,w) := w \otimes v$. Then one can check that $\phi$ is bilinear map. So universal property implies that there exists a linear transformation
$$ \Phi : V \otimes W \to  W \otimes  V$$
satisfying $\phi(v,w) = \Phi \circ \iota(v,w)$, i.e.
$$\Phi(v \otimes w) = w \otimes v.$$
Similarly, one can also construct a linear transformation $$\Psi: W \otimes V \to V \otimes W$$ satisfying
$$\Psi(w \otimes v) = v \otimes w.$$
Then for a general element $v^{(1)} \otimes w^{(1)} + \dots + v^{(r)} \otimes w^{(r)} \in W \otimes V$,
\begin{align*}
\Psi \circ \Phi(v^{(1)} \otimes w^{(1)}+ \dots + v^{(r)} \otimes w^{(r)})
&= \Psi (\Phi(v^{(1)} \otimes w^{(1)})+ \dots + \Phi(v^{(r)} \otimes w^{(r)})) \\
&= \Psi (w^{(1)} \otimes v^{(1)}+ \dots + w^{(r)} \otimes v^{(r)}) \\
&= \Psi(w^{(1)} \otimes v^{(1)})+ \dots + \Psi(w^{(r)} \otimes v^{(r)}) \\ 
&= v^{(1)} \otimes w^{(1)}+ \dots + v^{(r)} \otimes w^{(r)}
\end{align*}
So $\Psi \circ \Phi$ is the identity map. And similarly, $\Phi \circ \Psi$
is also an identity map. So they are inverses to each other, and the result follows.
\end{proof}

\section{Linear Transformation on Tensor Product Spaces}
As in the discussions on vector spaces in the last chapter, once we have constructed new vector spaces, we will construct new linear transformations.

\begin{proposition} 
Let $V, V', W, W'$ be vector spaces over $\mathbb{F}$. Suppose that $T : V \to V'$ and $S : W \to W'$ are linear transformations, then there exists a unique linear transformation
$$T \otimes  S : V \otimes W \to V' \otimes W'$$
satisfying $(T \otimes  S)(v \otimes  w) = T(v) \otimes  S(w)$. 
\end{proposition}
\begin{proof} 
Once again, we use universal property to help. Define the map
$$T \times  S : V \times W \to V' \otimes W'$$
by $(T \times  S)(v,w) := T(v) \otimes  S(w)$.
Then the map is bilinear, and as before there exists a unique linear transformation
$$T \otimes S : V\otimes W \to V' \otimes W'$$
satisfying $(T \otimes  S)(v \otimes  w) = T(v) \otimes  S(w)$.
\end{proof}

\begin{proposition} The operation $T \otimes S$ satisfies all the properties of tensor product, i.e.
$$(\alpha T_1 + \beta T_2) \otimes  S = \alpha(T_1 \otimes  S) + \beta(T_2 \otimes  S) \quad  
T \otimes  (\alpha S_1 + \beta S_2) = \alpha(T \otimes  S_1) + \beta (T \otimes  S_2)$$
Therefore, the notation “$\otimes$” in the definition of $T \otimes  S$ is justified.
\end{proposition}
\begin{proof}
By Remark \ref{rmk-inftensor}, $v_i \otimes w_j$ forms a basis of $V \otimes W$. Therefore, one only needs to check that both sides of the equation gives the same image upon applying $v_i \otimes w_j$. For instance,
\begin{align*}
((\alpha T_1 + \beta T_2) \otimes  S)(v_i \otimes w_j) &= 
(\alpha T_1 + \beta T_2)(v_i) \otimes  S(w_j) \\
&= 
(\alpha T_1(v_i) + \beta T_2(v_i)) \otimes  S(w_j)\\
&= 
\alpha (T_1(v_i) \otimes  S(w_j)) + \beta (T_2(v_i) \otimes  S(w_j)) \\
&=
\alpha(T_1 \otimes  S)(v_i \otimes w_j) + \beta(T_2 \otimes  S)(v_i \otimes w_j).
\end{align*}

\end{proof}

\begin{remark}
    Let $V, V', W, W'$ be vector spaces over $\mathbb{F}$, with
    \begin{itemize}
        \item $\mathcal{A} := \{v_i\ |\ 1 \leq i \leq n\}$ is a basis of $V$.
        \item $\mathcal{A}' := \{v_j' \ |\ 1 \leq j \leq n'\}$ is a basis of $V'$.
        \item $\mathcal{B} := \{w_k \ |\ 1 \leq k \leq m\}$ is a basis of $W$.
        \item $\mathcal{B}' := \{w_l' \ |\ 1 \leq l \leq m'\}$ is a basis of $W'$.
    \end{itemize}
    Suppose $T:V \to V'$ has a matrix representation $T_{\mathcal{A}'\mathcal{A}} = A = (a_{ij})$, and $S: W \to W'$ has a matrix representation $T_{\mathcal{B}'\mathcal{B}} = B = (b_{kl})$. 
    
    Take
    $$\mathcal{D} := \{v_1 \otimes w_1, \dots, v_1 \otimes w_m, \quad \cdots, \quad v_n \otimes w_1, \dots, v_n \otimes w_m\}$$
    $$\mathcal{D}' := \{v_1' \otimes w_1', \dots, v_1' \otimes w_{m'}', \quad \cdots, \quad v_{n'}' \otimes w_1', \dots, v_{n'}' \otimes w_{m'}'\}$$
    as {\bf ordered} bases of  $V \otimes W$ and $V' \otimes W'$ respectively. Then the matrix representation of 
    $T \otimes S: V \otimes W \to V' \otimes W'$ is the {\bf Kronecker product}:
    $$T_{\mathcal{D}'\mathcal{D}} = \begin{pmatrix}
        a_{11}B & a_{12}B & \dots & a_{1n}B \\
        a_{21}B & a_{22}B & \dots & a_{2n}B \\
        \vdots & \vdots & & \vdots \\ 
        a_{n'1}B & a_{n'2}B & \dots & a_{n'n}B 
    \end{pmatrix}$$
    We omit the proof here.
\end{remark}

\section{Exterior Tensor Product}
Before going to the definition of exterior product, let us briefly mention some results on multilinear tensor product $V_1 \otimes \dots \otimes V_k$ without proving them:

\begin{itemize}
    \item {\bf General vector:}
    $$v_1^{(1)} \otimes \dots \otimes v_k^{(1)} + \dots + v_1^{(r)} \otimes \dots \otimes v_k^{(r)}$$
    \item {\bf $k$-linearity:}
    \begin{align*}
    &v_1 \otimes \dots \otimes (\alpha v_i + \beta v_i') \otimes \dots \otimes v_k\\ 
    =\ &\alpha(v_1 \otimes \dots \otimes v_i \otimes \dots \otimes v_k) + \beta(v_1 \otimes \dots \otimes v_i' \otimes \dots \otimes v_k)\end{align*}
    \item {\bf Dimension:}
    $$\dim(V_1 \otimes \dots \otimes V_k) = \dim(V_1) \times \dots \times \dim(V_k)$$
    \item {\bf Universal property:} Let 
    $$\iota: V_1 \times \dots \times V_k \to V_1 \otimes \dots \otimes V_k$$
    $$\iota(v_1, \dots, v_k) := v_1 \otimes \dots \otimes v_k,$$
    then for any {\it $k$-linear map} $f: V_1 \times \dots \times V_k \to U$, there exists a unique {\it linear transformation} 
    $$\beta: V_1 \otimes \dots \otimes V_k \to U$$
    satisfying
    $$\beta(v_1 \otimes \dots \otimes v_k) = f(v_1, \dots, v_k).$$
    \item {\bf Linear transformation:} Let $T_i: V_i \to W_i$ be linear transformations. Then there exists a unique linear transformation
    $$T_1 \otimes \dots \otimes T_k: V_1 \otimes \dots \otimes V_k \to W_1 \otimes \dots \otimes W_k$$
    satisfying
    $$T_1 \otimes \dots \otimes T_k(v_1 \otimes \dots \otimes v_k) = T_1(v_1) \otimes \dots \otimes T_k(v_k).$$
\end{itemize}
In the case when $V_1 = \dots = V_k = V$, $W_1 = \dots = W_k = W$ and $T_1 = \dots = T_k = T$, we will use the shortand e
$$T^{\otimes k} : V^{\otimes k}  \to W^{\otimes k}$$

\begin{definition} A $k$-linear map $f:V \times \dots \times V \to W$ is called {\bf alternating} if
$$f(v_1, \dots , v_i , \dots , v_j , \dots , v_k) = {\bf 0}_W$$
whenever $v_i = v_j$ for some $i \neq j$.
\end{definition}
\begin{example}
The cross product and determinant function in Example \ref{eg-klinear} are alternating, since $v \times v = {\bf 0}$ and the determinant of a matrix with 2 identical columns is equal to $0$.
\end{example}

\begin{lemma}
    If $f:V \times \dots \times V \to W$ is an alternating map, then
    $$f(v_1, \dots , v , \dots , w , \dots , v_k) = - f(v_1, \dots , w , \dots , v , \dots , v_k)$$
    for all $v_i, v, w \in V$
\end{lemma}
\begin{proof}
    Note that $f(v_1, \dots, v+w, \dots, v+w, \dots, v_k) = 0$, hence one has
\begin{align*}
&f(v_1, \dots, v, \dots, v, \dots, v_k) + f(v_1, \dots, v, \dots, w, \dots, v_k) \\
+ &f(v_1, \dots, w, \dots, v, \dots, v_k) + f(v_1, \dots, w, \dots, w, \dots, v_k) = {\bf 0}    
\end{align*}
by $k$-linearlity of $\mathbb{F}$. Note that the first and the last term are zero since $\mathbb{F}$ is alternating, so
$$f(v_1, \dots, v, \dots, w, \dots, v_k) + f(v_1, \dots, w, \dots, v, \dots, v_k) = 0$$    
and the result follows.
\end{proof}

\begin{definition}
Let $V$ be a vector space over $\mathbb{F}$. Consider the vector subspace $\mathcal{U} \leq V^{\otimes k}$ spanned by vectors of the form
$$\{v_1  \otimes  \dots \otimes v \otimes \dots \otimes v \otimes \dots \otimes  v_k\}$$
for all $v, v_1, \dots, v_k \in V$. Then the {\bf $k$-exterior product} of $V$ is the quotient space
$$\wedge^k V := V^{\otimes k}/\mathcal{U}.$$
We write $v_1 \wedge \dots \wedge v_k := v_1 \otimes \dots \otimes v_k + \mathcal{U} \in \wedge^k V$, and 
$$\iota: V \times \dots \times V \to \wedge^k V$$
is defined by $\iota(v_1, \dots, v_k) := v_1 \wedge \dots \wedge v_k$.
\end{definition}

By definition, one can check that 
\begin{itemize}
    \item $\wedge^k V$ is $k$-linear:
    \begin{align*}
&v_1  \wedge  \dots  \wedge (\alpha v_i + \beta v_i' )  \wedge  \dots  \wedge v_k \\
=\ &\alpha(v_1  \wedge  \dots  \wedge v_i  \wedge  \dots  \wedge v_k) + \beta(v_1  \wedge  \dots  \wedge v_i'  \wedge  \dots  \wedge v_k)\end{align*}
for $i = 1, \dots, k.$
\item $\wedge^k V$ is alternating:
\begin{align*}
v_1  \wedge  \dots  \wedge v  \wedge  \dots  \wedge v  \wedge  \dots  \wedge v_k &:= v_1 \otimes  \dots \otimes v \otimes  \dots \otimes v \otimes  \dots \otimes v_k +\mathcal{U} \\
&= {\bf 0} + \mathcal{U}\\
&= {\bf 0}_{\wedge^k V}
\end{align*}
\item $\wedge^k V$ changes sign by swapping two entries:
$$v_1 \wedge  \dots \wedge v \wedge  \dots \wedge w \wedge  \dots \wedge v_k =-(v_1 \wedge  \dots \wedge w \wedge  \dots \wedge v \wedge  \dots \wedge v_k)$$
\end{itemize}

\begin{theorem}[Universal Property of Exterior Product]
    Let $V$ be a vector space over $\mathbb{F}$. Consider the category
    $$\mathcal{C}_{ep} := \{(W,f)\ |\ f: V \times \dots \times V \to W\ \text{is}\ k\text{-alternating}\}$$
with
$$\mathrm{Hom}((W,f),(U,g)) := \{\beta : W \to U\ \text{linear transformation}\ |\ g = \beta \circ f\}.$$
Then the $k$-exterior product $\mathcal{I}_{ep} := (\wedge^k V,\ \iota: V \times \dots \times V \to \wedge^k V)$ is the initial object in $\mathcal{C}_{ep}$. 

In other words, for all {\bf $k$-alternating maps} $f: V \times \dots \times V \to U$, there exists a {\bf linear transformation} 
$$\beta: \wedge^k V  \to U$$
such that $f = \beta \circ \iota$, i.e. $\beta(v_1 \wedge \dots \wedge v_k) = f(v_1, \dots, v_k)$.
\end{theorem}

\begin{example}
Let $T: V \to W$ be a linear transformation, one can define
$$f: V \times \dots \times V \to \wedge^k W$$
by $f(v_1, \dots, v_k) := T(v_1) \wedge \dots \wedge T(v_k).$
Then one can easily check that $\mathbb{F}$ is $k$-alternating, and the universal property of exterior product implies that there exists 
$$T^{\wedge k} := \beta: \wedge^k V \to \wedge^k W$$
satisfying $T^{\wedge k}(v_1 \wedge \dots \wedge w_k) = T(v_1) \wedge \dots \wedge T(v_k)$.
\end{example}

\section{The Determinant} \label{sec-det}
We are now in the position to define the determinant of a linear operator $T:V \to V$ for a vector space $V$ with $\dim(V) = n < \infty$.

\begin{lemma}
Let $V$ be a vector space over $\mathbb{F}$ with $\dim(V) = n$. For $0 \leq k \leq n$, one has
$$\dim(\wedge^k V) = \begin{pmatrix} n \\ k\end{pmatrix} = \frac{n!}{k!(n-k)!}$$
\end{lemma}
\begin{proof}
    
    
    
    Suppose $\{v_1 , \dots, v_n\}$ is a basis of $V$, we claim that 
    $$\mathcal{E} := \{v_{i_1} \wedge \dots \wedge v_{i_k}\ |\ 1 \leq i_1 < \dots < i_k \leq n\}$$
    is a basis of $\wedge^k V$.

    It is easy to see $\mathcal{E}$ spans $\wedge^k V$: Namely, recall the canonical projection
    $$\pi_{\mathcal{U}}: V^{\otimes k} \to \wedge^k V = V^{\otimes k}/\mathcal{U}$$
    given by 
    $$\pi_{\mathcal{U}}(v^{(1)} \otimes \dots \otimes v^{(k)}) = (v^{(1)} \otimes \dots \otimes v^{(k)}) + \mathcal{U} = v^{(1)} \wedge \dots \wedge v^{(k)}$$
    is surjective, so it maps a spanning set of $V^{\otimes k}$ to a spanning set of $\wedge^k V$. Since 
    $$\{v_{j_1} \otimes \dots \otimes v_{j_k}\ |\ 1 \leq j_{l} \leq n\ \text{for all}\ l\}$$
    is a basis (and hence a spanning set) of $V^{\otimes k}$, 
    $$\{v_{j_1} \wedge \dots \wedge v_{j_k}\ |\ 1 \leq j_{l} \leq n\ \text{for all}\ l\}$$
    is a spanning set of $\wedge^k V$. But it is easy to see that each element in the above set is either zero (if there are repeated terms), or (if there are no repeated terms) it is equal to an element in $\mathcal{E}$ up to a sign.

    The proof of linear independence is left as an exercise - for instance, one can follow the proof of linear independence in the tensor product section.
\end{proof}
Therefore, in the particular case when $k = n$, $\dim(\wedge^n V) = 1$, and is spanned by 
$$\zeta:= v_1 \wedge \dots \wedge v_n$$ 
for any choice of basis $\{v_1, \dots, v_n\}$ of $V$.

\begin{definition}
    Let $V$ be a finite dimensional vector space over $\mathbb{F}$ with $\dim(V) = n$, and $T:V \to V$ be a linear operator. 
    
    Then $\wedge^n V = \mathrm{Span}(\zeta)$ is one-dimensional, and the linear operator
    $$T^{\wedge n}: \wedge^n V \to \wedge^n V$$
    defined by
    $$T^{\wedge n}(u_1 \wedge \dots \wedge u_n) := T(u_1) \wedge \dots \wedge T(u_n)$$ satisfies
    $$T^{\wedge n}(\zeta) = \Delta_T \cdot \zeta$$
    for some scalar $\Delta_T$. We define the {\bf determinant} of $T$ by
    $$\det(T) := \Delta_T.$$
\end{definition}

\begin{example}
    Let $T : \mathbb{F}^2 \to \mathbb{F}^2$ be given by $T\begin{pmatrix} x \\ y \end{pmatrix} := \begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}$. Then $\wedge^2 \mathbb{F}^2 = \mathrm{Span}(e_1 \wedge e_2)$, and
    $$T^2: \wedge^2 \mathbb{F}^2 \to \wedge^2 \mathbb{F}^2$$
    satisfies
\begin{align*}
    T^{\wedge 2}(e_1 \wedge e_2) &= T(e_1) \wedge T(e_2) \\
    &= \begin{pmatrix} a \\ c \end{pmatrix} \wedge \begin{pmatrix} b \\ d \end{pmatrix} \\
    &= (ae_1 + ce_2) \wedge (be_1 + de_2) \\
&= (ae_1)\wedge(be_1) + (ae_1)\wedge(de_2) + (ce_2)\wedge(de_1) + (ce_2)\wedge(de_2) \\
&= 0 + (ad)e_1 \wedge e_2 + (bc)e_2 \wedge e_1 + 0 \\
&= (ad - bc)e_1 \wedge e_2
\end{align*}
Therefore, $\det(T) = ad - bc$ as expected.
\end{example}

More generally, suppose 
   Let $T : \mathbb{F}^n \to \mathbb{F}^n$ be given by 
   $$T(v) := Av, \quad where\quad A = \begin{pmatrix} a_1 | a_2 | \dots | a_n \end{pmatrix}$$
   By writing $\det(A) := \det(T)$ as in MAT2042, one has
   $$T^{\wedge n}(e_1 \wedge e_2 \wedge \dots \wedge e_n) = a_1 \wedge a_2 \wedge \dots \wedge a_n = \det(A) (e_1 \wedge e_2 \wedge \dots \wedge e_n)$$
   So if $A$ have identical columns $a_i = a_j$ for $i \neq j$, one has
   $a_1 \wedge a_2 \wedge \dots \wedge a_n = {\bf 0}$ and hence 
   $\det(A) = 0$.

   Also, if $A' = \begin{pmatrix} a_2 | a_1 | \dots | a_n \end{pmatrix}$ and $T'(v) := A'v$. Then 
   \begin{align*}
   \det(A') (e_1 \wedge e_2 \wedge \dots \wedge e_n) &= (T')^{\wedge n}(e_1 \wedge e_2 \wedge \dots \wedge e_n) \\
   &= a_2 \wedge a_1 \wedge \dots \wedge a_n \\ 
   &= - (a_1 \wedge a_2 \wedge \dots \wedge a_n) \\
   &= -\det(A) (e_1 \wedge e_2 \wedge \dots \wedge e_n)\end{align*}
   and hence $\det(A') = -\det(A)$.

By similar arguments, one can conclude the following defining properties of the determinant:
\begin{corollary}
    Let $A \in M_{n\times n}(\mathbb{F})$. Then the following holds:
\begin{itemize}
    \item $\det(I_{n\times n})= 1$;
    \item $\det\begin{pmatrix} a_1 | \dots  | a_i | \dots | a_j | \dots | a_n \end{pmatrix} = - \det\begin{pmatrix} a_1 | \dots  | a_j | \dots | a_i | \dots | a_n \end{pmatrix}$;
    \item For any $\alpha \in \mathbb{F}$, $\det\begin{pmatrix} a_1 | \dots  | \alpha a_i | \dots | a_n \end{pmatrix} = \alpha \det\begin{pmatrix} a_1 | \dots  | a_i | \dots | a_n \end{pmatrix}$;
\end{itemize}
\end{corollary}

As another advantage of understanding determinant in such a way, we have
\begin{theorem}
    Let $S, T: V \to V$ be linear operators. Then
    $$\det(S \circ T) = \det(S)\det(T)$$
\end{theorem}
\begin{proof}
Pick any basis $\{v_1 \wedge \dots \wedge v_n\}$ of $\wedge^n V$, then
\begin{align*}
\det(T \circ S)(v_1 \wedge \dots \wedge v_n) &= (T \circ S)^{\wedge n}(v_1 \wedge \dots \wedge v_n) \\
&= (T \circ S)(v_1) \wedge \dots \wedge (T \circ S)(v_n) \\
&= T(S(v_1)) \wedge \dots \wedge T(S(v_n)) \\
&= (T^{\wedge n}) ((S(v_1) \wedge \dots \wedge S(v_n))\\
&= (T^{\wedge n}) \circ (S^{\wedge n}) (v_1 \wedge \dots \wedge v_n)\\
&= (T^{\wedge n})(\det(S)(v_1 \wedge \dots \wedge v_n))\\
&= \det(S)T^{\wedge n}(v_1 \wedge \dots \wedge v_n)\\
&= \det(S)\det(T)(v_1 \wedge \dots \wedge v_n)
\end{align*}
and hence $\det(T \circ S) = \det(T)\det(S)$.
\end{proof}

\begin{corollary}
    Let $A, B \in M_{n \times n}(\mathbb{F})$, then
    $$\det(AB) = \det(A)\det(B).$$
\end{corollary}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Modules}
\section{Definition of Modules} Modules can be seen as a generalization of ``vector spaces over a ring $R$''. The theory can get very general for different kinds of $R$'s, so we would like to focus on a specific kind of rings:
\begin{definition}
    Let $R$ be a unital commutative ring. A {\bf zerodivisor} of $R$ is a nonzero element $a \in R \backslash \{0\}$ such that there exists $b \in R\backslash \{0\}$ such that
    $$a \cdot b = 0.$$

    If $R$ has no zerodivisors, we call $R$ an {\bf integral domain (ID)}. 
\end{definition}

\begin{example}
Here are some (non)-examples of integral domains:
    \begin{enumerate}
        \item Let $R=\mathbb{Z}_{6}$, then $[2]\cdot [3]=[0]$, so $[2]$ and $[3]$ are zero-divisors. In other words, $R$ is {\bf not} an ID.
    \item $\mathbb{Z}$ is an ID.
    \item All fields $\mathbb{F}$ are IDs - namely, for all nonzero $a \in \mathbb{F} \backslash \{0\}$, one always has $a^{-1}$ such that $a^{-1} \cdot a = 1$. Therefore, 
    $$a \cdot b = 0 \quad \Rightarrow \quad a^{-1} \cdot a \cdot b = a^{-1} \cdot 0 \quad \Rightarrow \quad 1 \cdot b = 0  \quad \Rightarrow \quad b = 0$$
    which implies there are no $b \neq 0$ such that $a \cdot b = 0$.
    \item If $R$ is an ID, then the polynomial ring $R[x]$ is also an ID.
    \end{enumerate}
 \end{example}

Unless specified otherwise, we will focus on the case when $R$ is an integral domain.


\begin{definition} \label{def-module}
    Let $(R,+,\cdot)$ be an integral domain. {\bf A (left) $R$-module} is a set $M$ with operations
    $$+: M \times M \to M \quad and \quad \cdot : R \times M \to M$$
    such that the following holds:
    \begin{itemize}
        \item $(M,+)$ is an abelian group, i.e.
            \begin{enumerate}
                \item $(m_1+m_2)+m_3 = m_1+(m_2+m_3)$.
                \item There exists ${\bf 0}_{M}$ such that ${\bf 0}_{M}+m = m = m+{\bf 0}_{M}$.
                \item For every $m \in M$, there exists $(-m) \in M$ such that $m+(-m) = (-m)+m = {\bf 0}_{M}$.
                \item $m_1+m_2 = m_2+m_1$ for any $m_1, m_2 \in M$.
            \end{enumerate}
        \item For every $r,s \in R$, $u,v \in M$, we have
            \begin{enumerate}
                \item[(5)] $r \cdot (u+v) = r \cdot u + r \cdot v$.
                \item[(6)] $(r+s) \cdot u = r \cdot u + s \cdot u$.
                \item[(7)] $(r \cdot s) \cdot u = r \cdot (s \cdot u)$.
                \item[(8)] $1 \cdot u = u$.
            \end{enumerate}
    \end{itemize}
\end{definition}

\begin{example}\label{eg-importanteg}
    \begin{enumerate}
        \item Let $R = \mathbb{F}$, then all vector spaces $V$ over $\mathbb{F}$ is an $R$-module.
        \item For any $R$, $R^{n} = \{ (r_1,\cdots,r_n) \ |\ r_i \in R \}$ with
            \begin{itemize}
                \item $(r_1,\cdots,r_n)+(s_1,\cdots,s_n) := (r_1+s_1,\cdots,r_n+s_n)$;
                \item $r \cdot (r_1,\cdots,r_n) := (r \cdot r_1,\cdots,r \cdot r_n)$
            \end{itemize}
            is an $R$-module.
        \item Let $R = \mathbb{Z}$, then $M = \mathbb{Z}_{n}$ with
            \begin{itemize}
                \item $[a]+[b]:=[a+b]$;
                \item $r \cdot [a]:=[r \cdot a]$
            \end{itemize}
            is an $R$-module.
%        \item (Important Example) Let $V$ be a vector space over $\mathbb{F}$ and $T: V \to V$ is a linear operator. Then $V$ is an $\mathbb{F}[x]=R$-module by
%            \begin{itemize}
%                \item $v+w:=v+w$ (usual vector addition);
 %               \item $(a_0+a_1 x+\cdots+a_n x^{n})\cdot v:=(a_0 I+a_1 T+\cdots+a_n T^{n})\cdot v =a_0 v+a_1 T(v)+\cdots+a_n T^{n}(v)$
%            \end{itemize}
%            for any $v,w \in V$ and $p(x)=a_0+\cdots+a_n x^{n} \in R$. 

 %           \medskip
 %           \noindent By Cayley-Hamilton Theorem in MAT2042, the characteristic polynomial $\chi_T(x) \in \mathbb{F}[x]$ satisfies
%            $$\chi_T(T)v = {\bf 0}$$
%            for all $v \in V$. In other words, one has
%            $\chi_{T}(x)\cdot v= {\bf 0}$ for any $v \in V$.
    \end{enumerate}
\end{example}

\begin{remark}
    In linear algebra, a single nonzero vector $\{ v \}$ is always linear independent. This is not true anymore for modules! For instance, in Example (3) above, 
    $$n \cdot [a] = [0]$$
    so $\{[a]\}$ is not linearly independent. Also, in Example (4) above,
    $$\chi_T(x) \cdot v = {\bf 0},$$
    so $\{v\}$ is not linearly independent.
\end{remark}

\section{Submodules and Ideals}
\begin{definition}
    Let $(M,+,\cdot)$ be an $R$-module. A subset $I \subseteq M$ is called a {\bf submodule} of $M$ if
    $$+|_{I \times I}:I \times I \to I \quad and \quad \cdot |_{R \times I}:R \times I \to I.$$
    
    In other words, $(I, +|_{I \times I}, \cdot|_{R \times I})$ is a submodule of M if
    $$i+j \in I \quad and \quad r \cdot I \in I$$
    for all $i,j \in I$ and $r \in R$.
\end{definition}

\begin{example} 
        For any $R$, let $M=R^{n}$, then 
        $$I:=\{ (r_1,r_2,\cdots,r_k,0,\cdots,0) \ |\ r_i \in R \}$$ 
        is a submodule of $M$.
\end{example}

In the special case when one take $M = R$ as an $R$-module, we have the following definition for submodules of $R$, which is of {\it utmost importance} in both module theory and abstract algebra.

\begin{definition}[Ideals]
    Let $R$ be an integral domain, and $M=R^{1}=R$ is an $R$-module. The submodules $I \lhd M(=R)$ are called {\bf ideals} of $R$. In other words, $I \lhd R$ if and only if 
    \begin{center}
    $i+j \in I$ \quad and  \quad $r \cdot i \in I$ 
    \end{center}
    for any $i,j \in I$ and $r \in R$.
\end{definition}

\begin{example}
    Let $R=\mathbb{Z}=M$, then
    \begin{align*}
        I_1&=\langle \langle 2 \rangle \rangle = \{ 2 \cdot k \ |\ k \in \mathbb{Z} \} = 2 \mathbb{Z} \lhd \mathbb{Z}, \\
        I_2&=\langle \langle 6, 9 \rangle \rangle = \{ 6 \cdot a+9 \cdot b \ |\ a,b \in \mathbb{Z} \} = 3 \mathbb{Z} = \langle \langle 3 \rangle \rangle \lhd \mathbb{Z}.
    \end{align*}
\end{example}

\section{Spanning Set and Linear Independence}
\begin{definition}[Spanning Set]
    Let $M$ be an $R$-module and $B \subseteq M$ is a subset. Then the {\bf submodule generated by $B$} is 
    $$\langle \langle B \rangle \rangle (=Span_{R}(B)):=\{ r_1 \cdot b_1+\cdots+r_n \cdot b_n \ |\ b_i \in B, r_i \in R, n \in \mathbb{N} \}.$$
    More precisely,
    \begin{itemize}
        \item If $\left| B\right|=n<\infty$, we say $\langle\langle B\rangle\rangle$ is {\bf n-generated}, or {\bf finitely generated}.
        \item If $B=\{b\}$, then $\langle\langle b\rangle\rangle$ is {\bf 1-generated}, or is the {\bf cyclic submodule} generated by $b$.
    \end{itemize}
\end{definition}

\begin{example}\label{eg-zxnotpid}
    \begin{enumerate}
        \item Let $R=\mathbb{Z}$, $M=\mathbb{Z}_{n}$. Then $M=\langle\langle 1\rangle\rangle$ is 1-generated (cyclic).
        \item For any $R$, $M=R^{n}$ is generated by $M=\langle\langle e_1,\cdots ,e_n\rangle\rangle$, where 
        $$e_i := (0,\dots,0,\overbrace{1}^{i\text{-th coordinate}},0,\dots,0).$$
        \item Let $R=\mathbb{Z}[x]$, then $M=R$ is 1-generated since $M=\langle\langle 1\rangle\rangle$. However, 
        \begin{align*}
            M^{'}&:=\{p(x_1,x_2,\cdots)\ |\ p(x_1,x_2,\cdots) \text{ has an even constant term}\}\\
            &=\langle\langle 2,x\rangle\rangle
        \end{align*}
        is a submodule of $M$ with 2 generators which cannot be reduced to $$\langle\langle2,x\rangle\rangle=\langle\langle f\rangle\rangle.$$
        So even if $M$ is n-generated, $M^{'}\leq M$ may have more than n generators!
        \item Let $R=\mathbb{Z}[x_1,x_2,\cdots]$, then $M=R$ is 1-generated.
        Consider
        \begin{align*}
            M^{'}&:=\{p(x_1,x_2,\cdots)\ |\ p(0,0,\cdots)=0 \text{ has zero constant term}\}\\
            &=\langle\langle \{x_1,x_2,\cdots\}\rangle\rangle
        \end{align*}
        which is {\bf NOT} finitely generated. So even if $M$ is finitely generated, $M^{'}\leq M$ may not be finitely generated!
    \end{enumerate}
\end{example}

\begin{definition}[Principal Ideal Domain]\
    \begin{enumerate}
        \item An ideal $I \lhd R$ is called {\bf principal} if $I=\langle \langle a \rangle \rangle$ is 1-generated.
        \item An integral domain $R$ is called {\bf Principal Ideal Domain (PID)} if all ideals $I \lhd R$ are 1-generated.
    \end{enumerate}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item $R=\mathbb{Z}$ is a PID, and $I\lhd \mathbb{Z}$ are of the form $I=n\mathbb{Z}$.
        \item $R=\mathbb{F}[x]$ is a PID.
        \item $R=\mathbb{Z}[x]$ is {\bf NOT} a PID by Example \ref{eg-zxnotpid}(3).
    \end{enumerate}
\end{example}

For the later part of this course, we will study the special case when $R$ is a Principal Ideal Domain in full detail. In particular, we will get (generalizations of) Primary Decomposition Theorem and Jordan Normal Form Theorem in the setting of $R$-modules with $R$ being a PID.

\begin{definition}[Linear Independence]
    Let $M$ be an $R$-module. A subset $S\subseteq M$ is {\bf linearly independent} if for any subset $\{s_1,\cdots,s_k\}\subseteq S$, one has
    $$r_1s_1+\cdots+r_ks_k=0\quad\Leftrightarrow\quad r_1=\cdots=r_k=0.$$
    Otherwise, we say $S$ is {\bf linearly dependent}.
\end{definition}

\begin{example} \label{eg-ld}
    Linear dependence for modules and vector spaces are {\bf different}.
    \begin{enumerate}
        \item (A nonzero vector $\{v\}$ can be linearly dependent.)

        Let $R=\mathbb{Z}$ and $M=\mathbb{Z}_{n}$, then $[1]\in M$ is linearly dependent since $$n\cdot[1]=[0].$$
        \item ($\{v_1,v_2\}$ is linearly dependent does not implies $v_1$ is a scalar multiple of $v_2$.)

        Let $R=\mathbb{Z}$ and $M=R=\mathbb{Z}$, then $\{2,3\}$ is linearly dependent since
        $$6\cdot 2+(-4)\cdot 3=0,$$
        but $2\neq r\cdot 3$ for any $r \in \mathbb{Z}$.
    \end{enumerate}
\end{example}


\section{Torsion and Annihilators}
As we saw in Example \ref{eg-ld}(1), ``bad'' behavior happens if there exists $r\neq0$ such that $$r\cdot m=0$$
for some $m\neq0$. So we make the following:
\begin{definition}
    Let $M$ be an $R$-module. The set 
    $$M_{tor}:=\{m\in M\ |\ r\cdot m=0 \text{ for some }r\in R\setminus\{0\}\}$$
    is called the {\bf torsion elements} in $M$.
    \begin{itemize}
        \item If $M_{tor}=\{0\}$, we say $M$ is {\bf torsion-free}.
        \item If $M_{tor}=M$, we say $M$ is a {\bf torsion module}.
    \end{itemize}
\end{definition}

\begin{example} \label{eg-torsionelements}\
%    \begin{enumerate}
     Let $R=\mathbb{Z}$ and $M=\mathbb{Z}\oplus\mathbb{Z}_{3}=\{(a,[b])\ |\ a\in\mathbb{Z},[b]\in\mathbb{Z}_{3}\}$.Then
        \begin{align*}
            M_{tor}&=\{(a,[b])\ |\ (r\cdot a,r \cdot[b])=(0,[0])\ \text{for some nonzero}\ r \in \mathbb{Z}\}\\
            &=\{(0,[b])\ |\ [b]\in\mathbb{Z}_{3}\}.
        \end{align*}
%        \item Recall the important example in Example \ref{eg-importanteg}(4), where $V$ is an $R=\mathbb{F}[x]$-module given by
%        $$p(x)\cdot v:=p(T)\cdot v$$
%        Since $\chi_{T}(x)\cdot v=\chi_{T}(T)\cdot v=0$ for all $v \in V$, so one can take $r = \chi_T(x)$ in the definition of torsion elements to conclude that
%        $$V_{tor}=V$$
%        is a torsion module.
%    \end{enumerate}
\end{example}

\begin{proposition} \label{torsionsubmodule}
    Let $R$ be an ID, and $M$ be an $R$-module. Then $M_{tor} \leq M$ is a submodule.
\end{proposition}

\begin{proof}
    Let $t_1,t_2\in M_{tor}$, in other words, there exists $r_1,r_2\neq0$ such that 
    $$r_1\cdot t_1=r_2\cdot t_2=0.$$
    Since $R$ is ID, $r_1,r_2\neq0$ implies $r_1\cdot r_2\neq0$. Then
    \begin{align*}
        r_1r_2\cdot(p\cdot t_1+q\cdot t_2)&=r_1r_2p\cdot t_1+r_1r_2q\cdot t_2\\
        &=r_2p\cdot(r_1\cdot t_1)+r_1q\cdot(r_2\cdot t_2)\\
        &=r_2p\cdot0+r_1q\cdot0\\
        &=0
    \end{align*}
    for any $p,q\in R$, and we have $p\cdot t_1+q\cdot t_2\in M_{tor}$. Hence $M_{tor} \leq M$.
\end{proof}

In the definition of torsion elements, we look at which $m \in M$ gets `killed' by some $r \in R$. We now change our perspective by looking at which $r \in R$ `kills' an element $m \in M$:
\begin{definition}[Annihilator]
    Let $M$ be an $R$-module.
    \begin{itemize}
        \item The {\bf annihilator} of $m\in M$ is 
        $$\mathrm{Ann}(m):=\{r\in R\ |\ r\cdot m=0\}.$$
        \item The {\bf annihilator} of $M$ is 
        $$\mathrm{Ann}(M):=\{r\in R\ |\ r\cdot m=0\text{ for all }m\in M\}.$$
    \end{itemize}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item If $M$ is torsion-free, then $\mathrm{Ann}(M)=\{0_{R}\}$ for all $m\in M$.
        \item On the other extreme, if $M = M_{tor}$ is a torsion module, then
        $\mathrm{Ann}(m) \neq \{0_R\}$ for all $m \in M$. However, this {\bf does not} necessarily imply $\mathrm{Ann}(M) \neq \{0_R\}$ in general!
\end{enumerate}
\end{example}

\begin{example}[Minimal Polynomials for Linear Operators]
As a special case of torsion module, consider our important Example \ref{eg-importanteg}(2) with $R=\mathbb{F}[x]$ and $M=V$. We have already seen from there that
$\chi_T(x) \in \mathrm{Ann}(v)$ for all $v \in V$. So one has
$$\chi_T(x) \in \mathrm{Ann}(V).$$

More generally, for any linear operator $T:V \to V$, define the {\bf minimal polynomial $m_T(x)$ of $T$} such that
\begin{itemize}
    \item[(1)] $m_T(x)$ is monic, i.e. the leading power coefficient of $m_T(x)$ is $1$.
    \item[(2)] $m_T(T) v = {\bf 0}$ for all $v \in V$.
    \item[(3)] $m_T(x)$ is the polynomial of smallest positive degree such that (1) and (2) holds.
\end{itemize}

A possible candidate satisfying (1) and (2) is the characteristic polynomial $\chi_T(x)$. However, there may be polynomials with smaller degree such that both (1) and (2) holds. In general, one has:
\begin{center}
\fbox{If $f(x) \in \mathbb{F}[x]$ satisfies $f(T)v = {\bf 0}\ \forall\ v \in V$ (e.g. $f(x) = \chi_T(x)$), then $m_T(x) | f(x)$}
\end{center}
Under this perspective, one has
        \begin{align*}
            \mathrm{Ann}(V)&:=\{f(x) \in\mathbb{F}[x] \ |\ f(x)\cdot v = {\bf 0}\ \text{for all}\ v \in V \}\\
            &= \{ f(x)\ |\ m_T(x)| f(x)\}\\
            &= \{ f(x) = m_T(x)p(x) \ |\ p(x) \in \mathbb{F}[x]\}\\
            &=\langle\langle m_{T}(x)\rangle\rangle
        \end{align*}
\end{example}
    
\begin{proposition}
    Let $M$ be a $R$-module, and $m\in M$. Then $\mathrm{Ann}(m) \lhd R$ is an ideal.
\end{proposition}

\begin{proof}
    Let $a,a'\in \mathrm{Ann}(m)$, one has $a\cdot m=a'\cdot m={\bf 0}$. Then
    $$(a+a')\cdot m=a\cdot m+a'\cdot m={\bf 0}+{\bf 0}={\bf 0}$$
    $$(ra)\cdot m=r\cdot(a\cdot m)=r\cdot{\bf 0}={\bf 0}$$
    for all $r\in R$, hence $a+a'\in\mathrm{Ann}(m)$ and $r\cdot a\in\mathrm{Ann}(m)$.
\end{proof}
The same proposition and the same argument hold for $\mathrm{Ann}(M)$.

\section{Basis and Free Modules}

\begin{definition}
    Let $M$ be an $R$-module, We say $M$ is a {\bf free module} if there exists a linearly independent spanning set $\mathcal{B}$ of $M$.
\end{definition}

\begin{example}\
    \begin{enumerate}
        \item (Not all $M$ has a basis) Let $R=\mathbb{Z}$, then $M=\mathbb{Z}_n$ is not free since every element in M is a torsion element.
        \item For any ring $R$, $M = R^n$ is free with $\mathcal{B}=\{e_1,e_2,...,e_n\}$.
        \item ($M$ is free module does {\bf NOT} imply $N\leq M$ is a free module) Let $R=\mathbb{Z}\times \mathbb{Z}$ (with addition and multiplication defined in the usual way), then $M=R$ is a free module by (2). However, the submodule
        $$N:= \{(a,0)| a\in \mathbb{Z}\} \leq M$$
        is no longer free. (Since $(0,1)\cdot (a,0)= (0,0)$, no single element in $N$ is linearly independent.)
        
    \end{enumerate}
\end{example}

Free modules (over $R$) behave as nicely as vector space (over $\mathbb{F}$). For instance, we have:

\begin{theorem}
    Let $M$ be a free $R$-module with basis $\mathcal{B}$, then 
    \begin{enumerate}
        \item Every $m\in M$ can be {\bf uniquely} expressed as $$m=r_1b_1+r_2b_2+\cdots+ r_kb_k$$ where $r_i\in R$, $b_i\in \mathcal{B}$, $k\in \mathbb{N}$.
        \item $\mathcal{B}$ is a minimal spanning set of $M$ (i.e, if you remove an element $b\in B$, $\mathcal{B} - \{b\}$ is {\bf NOT} a spanning set).
        \item $\mathcal{B}$ is a maximal linearly independent set of $M$ (i.e. If we add $m\in M$ to $\mathcal{B}$, then $\mathcal{B}\cup \{m\}$ is linear dependence).
    \end{enumerate}
\end{theorem}

A natural question to ask is that whether all bases of a free module $M$ has the same {\bf cardinality}. The answer of this question is yes, as we will see below. The proof requires the use of quotient modules, which is given by:
\begin{definition}[Quotient Module]
    Let $M$ be an $R$-module, and $N\leq M$ is a submodule. The {\bf quotient module} $M/N$ is an $R$-module
    $$M/N:=\{m+M \ |\ m\in M\},$$
    with $+$ and $\cdot$ defined by
    \begin{align*}
        (m_1+N)+(m_2+N)&:=(m_1+m_2)+N\\
        r\cdot (m+N)&:=(r\cdot m)+N
    \end{align*}
    for all $r\in R$.
\end{definition}

\begin{example}
    Let $R=\mathbb{Z}$, $M=\mathbb{Z}$ and $N:=\langle\langle n\rangle\rangle=n\mathbb{Z}$. Then
    \begin{align*}
        M/N&=\mathbb{Z}/n\mathbb{Z}\\
        &=\{a+n\mathbb{Z}\ |\ a\in \mathbb{Z}\}\\
        &=\{0+n\mathbb{Z},1+n\mathbb{Z},\cdots,(n-1)+n\mathbb{Z}\}\\
        &=\{\overline{0},\overline{1},\cdots,\overline{n-1}\}
    \end{align*}
    with
    $$\overline{2}+\overline{3}=(2+n\mathbb{Z})+(3+n\mathbb{Z})=(2+3)+n\mathbb{Z}=\overline{2+3}$$
    $$4\cdot \overline{3}=\overline{4\cdot3}=\overline{12}.$$
    So $M/N=\{\overline{0},\overline{1},\cdots,\overline{n-1}\}$ behaves like $\mathbb{Z}_{n}=\{[0],[1],\cdots,[n-1]\}$ (more precisely, we will say $\mathbb{Z}/n\mathbb{Z}$ and $\mathbb{Z}_n$ are {\bf isomorphic} as $\mathbb{Z}$-modules later). Therefore,
    $$n\cdot\overline{a}=\overline{n\cdot a}=\overline{0}$$
    for all $\overline{a}\in M/N$, and hence $\{\overline{a}\}$ is linearly independent. So $M/N$ has no basis, i.e. $M/N$ is {\bf NOT} free.

    In conclusion,
    $$M=\langle\langle1\rangle\rangle\text{ is free with }\mathcal{B}_{M}=\{1\}$$
    $$N=\langle\langle n\rangle\rangle\text{ is free with }\mathcal{B}_{N}=\{n\}$$
    while $M/N$ is not free.
\end{example}

\begin{theorem} \label{thm-freebasis}
    Let $M$ be a free $R$-module. Then any bases of $M$ have the same cardinality.
\end{theorem}

To prove the theorem, we need some facts from abstract algebra: Let $R$ be an integral domain, then

\begin{enumerate}
    \item For any ideal $I \lhd R$, the quotient $(R/I, +, \cdot)$ is also a ring (called {\bf quotient ring}) with
    \begin{align*}
    (r+I) + (s+I) &:= (r+s)+I \\    
    (r+I) \cdot (s+I) &:= (r\cdot s)+I    
    \end{align*}
    \item In the special case when $I \lhd R$ is a {\bf maximal} ideal (here maximal means if 
    $$I \lhd N \lhd R,$$
    then $N = I$ or $N = R$). Then the quotient ring $\mathbb{F} = (R/I,+,\cdot)$ is a field.
\end{enumerate}



\begin{proof}
Let $I\triangleleft R$ be a maximal ideal, and consider $$IM:= \{i_1m_1 + i_2m_2 +\cdots + i_km_k\ |\ i_j\in I, m_k\in M, k\in \mathbb{N} \}$$
    It is easy to check that $IM$ is a submodule of $M$. So we have a quotient $R$-module $$M/IM:= \{m+IM\ |\ m\in M\}$$ 
    In fact, $M/IM$ is an $\mathbb{F} = R/I$-module with  scalar multiplication defined by $$(r+I)\cdot (m+IM):=rm+IM$$ (check this is well-defined). In other words, {\bf $M/IM$ is a $\mathbb{F}$-vector space}.

\medskip
    Now we make the following claim:
    \begin{claim}
    If 
    $$\mathcal{B}=\{b_s\ |\ s\in S\} \quad \quad \mathcal{C}=\{c_j\ |\ j \in J\}$$ are two bases of $M$ (as $R$-modules), then $$\mathcal{B}'=\{b_s+IM\ |\ s\in S\} \quad \quad \mathcal{C}'=\{c_j+IM\ |\ j\in J\}$$ 
    are two bases of $M/IM$ (as $\mathbb{F}= R/I$-vector spaces).
    \end{claim}
    If the claim holds, then $|\mathcal{B}'| = |\mathcal{C}'|$ have the same cardinality by our knowledge of bases over vector spaces. Then the result follows since $|\mathcal{B}| = |\mathcal{B}'|$ and $|\mathcal{C}| = |\mathcal{C}'|$.

\bigskip
    \noindent{\it {\bf Proof of claim.}}
    \begin{itemize}
        \item $\mathcal{B}'$ is a {\bf spanning set}. \\
         Take any $m+IM\in M/IM$, since $\langle\langle\mathcal{B}\rangle\rangle = M$
            \begin{align*}
                m+IM&=(r_1b_1+\cdots+r_nb_n)+IM \\
                    &=(r_1+I)(b_1+IM)+\cdots+(r_n+I)(b_n+IM)
            \end{align*}
        \item $\mathcal{B}'$ is {\bf linearly independent}.\\
         Suppose $(r_1+I)(b_1+IM)+\cdots+(r_n+I)(b_n+IM)= {\bf 0}$
        \begin{align*}
            & \Rightarrow r_1b_1+\cdots+r_nb_n+IM={\bf 0} \\
            & \Rightarrow r_1b_1+\cdots+r_nb_n \in IM \\
            & \Rightarrow r_1b_1+\cdots+r_nb_n=i_1m_1+\cdots+i_km_k\in IM \\
            & \Rightarrow r_1b_1+\cdots+r_nb_n=i_1(r_1^{(1)}b_1+\cdots+r_1^{(n)}b_n)+\cdots +i_k(r_k^{(1)}b_1+\cdots+r_k^{(n)}b_n) \\
            & \Rightarrow r_1b_1+\cdots+r_nb_n=(i_1r_1^{(1)}+\cdots+i_kr_k^{(1)})b_1+\cdots+(i_1r_1^{(n)}+\cdots i_kr_k^{(n)})b_n
        \end{align*}
        By the fact that $\mathcal{B}$ is linear independent, one has
        \begin{align*}
             r_1 &=r_1^{(1)}i_1+\cdots+r_k^{(1)}i_k \\
             r_2 &=r_1^{(2)}i_1+\cdots+r_k^{(2)}i_k \\
             &\vdots \\
             r_n &=r_1^{(n)}i_1+\cdots+r_k^{(n)}i_k 
        \end{align*}
        But $i_1, \dots, i_k \in I$ and $I \lhd R$ is an $R$-module, so each summand $r_{\star}^{(\bullet)}i_{\ast} \in I$ in the above equations. Hence we conclude that $r_1,r_2,\cdots,r_n \in I$, that is 
        $$r_1+I=r_2+I=\cdots=r_n+I=0.$$
    \end{itemize}
\end{proof}

Now we are safe to make the following:
\begin{definition}
    Let $M$ be a free $R$-module, The {\bf rank} of $M$ is the cardinality of any choice of basis $\mathcal{B}$ of $M$.
\end{definition}

\section{Homomorphisms} 
Homomorphism can be seen as the analog of linear transformation for $R$-modules:

\begin{definition} [Homomorphism]
    Let $M$, $N$ be $R$-modules. A map $\phi:M\to N$ is an $R$-{\bf homomorphism} if
    $$\phi(r_1\cdot m_1+r_2\cdot m_2)=r_1\cdot\phi(m_1)+r_2\cdot\phi(m_2)$$
    for all $r_1,r_2\in R$ and $m_1,m_2\in M$.

    The set of all $R$-homomorphism is denoted as 
    $$\mathrm{Hom}_{R}(M,N):=\{\phi:M\to N\ |\ \phi\text{ is an }R\text{-homomorphism}\},$$
    which is an analog of $\mathcal{L}(V,W)$ for modules. And as in the case of $\mathcal{L}(V,W)$ for vector spaces, $\mathrm{Hom}_{R}(M,N)$ is an $R$-module given by:
    $$(\phi+\psi)(m):=\phi(m)+\psi(m)\quad\quad\text{and}\quad\quad(r\cdot\phi)(m):=r\cdot\phi(m).$$
\end{definition}

\begin{theorem}
    Let $M$ be a free $R$-module with basis $\mathcal{B}=\{b_i\ |\ i\in I\}$, then for any $R$-homomorphism $\phi:M\to N$, $\phi$ is uniquely determined by the elements:
    $$\{\phi(b_i)\in N\ |\ b_i\in \mathcal{B}\}.$$

    Conversely, let $\{n_i\in N\ |\ i\in I\}$ be a collection of elements in $N$. Then there is a (unique) $R$-homomorphism $\phi:M\to N$ satisfying
    $$\phi(b_i)=n_i.$$
    More precisely, $\phi$ is defined by
    $$\phi(r_1\cdot b_{i_1}+\cdots+r_k\cdot b_{i_k}):=r_1\cdot n_{i_1}+\cdots+r_k\cdot n_{i_k},$$
    for all $r_l\in R$ and $b_{i_l}\in\mathcal{B}$.
\end{theorem}

\begin{definition} [Kernel and Image]
    Let $\phi:M\to N$ be an $R$-homomorphism, then the {\bf kernel} and {\bf image} of $\phi$ are defined by:
    \begin{enumerate}
        \item $\mathrm{ker}(\phi):=\{m\in M\ |\ \phi(m)=0_N\}\leq M$,
        \item $\mathrm{im}(\phi):=\{\phi(m)\in N\ |\ m\in M\}\leq N$.
    \end{enumerate}
\end{definition}

\begin{theorem}
    Let $\phi\in\mathrm{Hom}_{R}(M,N)$,
    \begin{enumerate}
        \item $\phi$ is {\bf injective}  $\Leftrightarrow$  $\mathrm{ker}(\phi)=\{0_M\}$,
        \item $\phi$ is {\bf surjective}  $\Leftrightarrow$  $\mathrm{im}(\phi)=N$.
    \end{enumerate}
\end{theorem}

\begin{definition}
    If $\phi\in\mathrm{Hom}_{R}(M,N)$ is bijective, we say $\phi$ is an {\bf isomorphism} between $M$ and $N$.
\end{definition}

As in the case of vector spaces, we have the {\bf correspondence} theorem and {\bf isomorphism} theorems for $R$-modules:

\begin{theorem} [Correspondence Theorem]
    Let $N\leq M$ be $R$-modules. Then there is a 1-1 correspondence between
    \begin{align*}
        \{S\ |\ N\leq S\leq M\}\quad&\leftrightarrow\quad\{X\ |\ 0\leq X\leq M/N\}\\
        S\quad&\rightarrow\quad S/N\\
        \bigcup_{x\in X}(x+N)\quad&\leftarrow\quad X
    \end{align*}
\end{theorem}

\begin{theorem} [First Isomorphism Theorem]
    Let $\phi:M\to N$ be an $R$-homomorphism. Then the map $\overline{\phi}:M/\mathrm{ker}(\phi)\to \mathrm{im}(\phi)$ defined by
    $$\overline{\phi}(m+\mathrm{ker}(\phi)):=\phi(m)$$
    is an $R$-isomorphism between $M/\mathrm{ker}(\phi)$ and $\mathrm{im}(\phi)$.
\end{theorem}

Similarly one can construct new $R$-modules from old ones using direct sum, direct product, tensor product as in the case of vector spaces.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Noetherian Rings and Noetherian Modules}
\section{Basic Definitions}
In this section, we will specialize our attention to a certain kind of integral domains $R$. In particular, we want to {\bf avoid} the following situation:

\begin{example}
Let $R=\mathbb{F}[x_1,x_2,\cdots]$. Consider the $R$-module
$$M=R=\langle\langle1\rangle\rangle$$ 
which is {\bf 1-generated}. But the submodule $N\leq M$ with zero constant term 
$$N=\langle\langle\{x_1,x_2,\cdots\}\rangle\rangle$$ 
is {\bf not finitely generated}.
\end{example}

In other words, we want the following holds: If $M$ is finitely generated, then any $N\leq M$ is also finitely generated.

\begin{definition} [Noetherian Module]
 Let $M$ be an $R$-module. We say M is a {\bf noetherian module} if it satisfies the ascending chain condition (ACC) of submodules: Let 
 $$N_1\leq N_2\leq N_3\leq\cdots$$ 
 be an ascending chain of submodules of $M$. Then the chain must become equal somewhere, i.e. there must be some $k$ such that 
 $$N_{k-1}<N_k=N_{k+1}= N_{k+2} = \cdots.$$
\end{definition}

\begin{example}
    Let $R=\mathbb{F}[x_1,x_2,\cdots]$ and $M=R=\langle\langle1\rangle\rangle$. Then
    $$\langle\langle x_1\rangle\rangle < \langle\langle x_1,x_2\rangle\rangle<\langle\langle x_1, x_2, x_3\rangle\rangle<\cdots$$
    does not satisfy (ACC)! So $M=R$ is {\bf NOT} noetherian.
\end{example}

\begin{theorem}
    Let $M$ be an $R$-module. Then 
    \begin{center}
        $M$ is noetherian \ $\Leftrightarrow$ \ all submodules $N\leq M$ are finitely generated.
    \end{center}
\end{theorem}

\begin{proof}
    ($\Rightarrow$) Suppose $M$ satisfies (ACC). Then for any $N\leq M$, take $n_1\in N$ and define
    $$N_1:=\langle\langle n_1\rangle\rangle,$$
    then we have $N_1\leq N$. If $N_1=\langle\langle n_1\rangle\rangle=N$, then $N$ is 1-generated. Otherwise, take $n_2\in N\setminus N_1$, and define
    $$N_2:=\langle\langle n_1,n_2\rangle\rangle,$$
    so that $N_1\leq N_2\leq N$.

    Continuing the argument to get $N_{i}$'s, we claim that there exists $k\in \mathbb{N}$ such that $N_k:=\langle\langle n_1,\cdots,n_k\rangle\rangle=N$, so that $N$ is $k$-generated.

    Suppose not, then we will have 
    $$N_1<N_2<\cdots<N_k<N_{k+1}<\cdots$$
    where $N_i=\langle\langle n_1,\cdots,n_i\rangle\rangle$, which violates (ACC), so it contradicts our assumption that $M$ is noetherian.

    \medskip
    ($\Leftarrow$) Suppose all $N\leq M$ are finitely generated. Let 
    $$N_1\leq N_2\leq\cdots\leq N_k\leq\cdots$$
    be a sequence of submodules of $M$. We wish to show that  $N_k=N_{k+1}=\cdots$ for some $k$, so that the sequence satisfies (ACC). Consider
    $$N:=\bigcup_{i\in\mathbb{N}}N_i.$$
    Then one can easily show that $N\leq M$, and hence $N=\langle\langle p_1,\cdots,p_l\rangle\rangle$ is finitely generated by hypothesis.

    For each $1\leq x\leq l$, $p_x\in N_{i_x}$ for some $i_x\in\mathbb{N}$. Let $k:=\mathrm{max}\{i_1,\cdots,i_l\},$
    so that for all $1 \leq x \leq l$, $p_x \in N_{i_x} \leq N_k$. Consequently,
    \begin{align*}
            &p_1,\cdots,p_l \in N_k  \\  
        \Rightarrow\ &\langle\langle p_1,\cdots,p_l \rangle\rangle\leq N_k\\
        \Rightarrow\ &N\leq N_k\\
        \Rightarrow\ &N\leq N_k\leq N_{k+1}\leq\cdots\leq N\\
        \Rightarrow\ &N_k=N_{k+1}=\cdots=N.
    \end{align*}
    In other words, the sequence satisfies (ACC), and the result follows.
\end{proof}

\begin{corollary} \label{fgforideals}
Let $R$ be an integral domain. Then
\begin{center}
    $R$ is noetherian \ $\Leftrightarrow$ \ all $I\lhd R$ are finitely generated.
\end{center}
    In particular, if $R$ is PID, then (by definition) all $I=\langle\langle a\rangle\rangle\lhd R$ are 1-generated, so $R$ is noetherian (in short, PID $\Rightarrow$ neotherian).
\end{corollary}

\section{Noetherian Rings and Noetherian Modules}
The reason why we care whether $R$ is noetherian or not is due to the following:
\begin{theorem}
    Let $R$ be an integral domain.
    \begin{enumerate}
        \item $R$ is noetherian if and only if all finitely generated $R$-modules $M$ are noetherian. In other words,
        \begin{center}
        $M$ is finitely generated \ $\Rightarrow$ \ all $N\leq M$ are finitely generated.    
        \end{center}
        \item Moreover, $R$ is PID if and only if
        \begin{center}
        $M$ is $k$-generated \ $\Rightarrow$ \ all $N\leq M$ are $k$-generated.    
        \end{center}
    \end{enumerate}
\end{theorem}

\begin{proof}
    ($\Leftarrow$) is obvious by taking
    $$M=R=\langle\langle 1\rangle\rangle.$$
    in both (1) and (2).

    \medskip
    ($\Rightarrow$) Assume $R$ is noetherian (or PID for (2)). Let $M:=\langle\langle m_1,\cdots,m_n\rangle\rangle$ be finitely generated $R$-module. We wish to show that 
    \begin{enumerate}
        \item All submodules $N\leq M$ is also finitely generated, and 
        \item If $R$ is PID, then $N$ is $n$-generated.
    \end{enumerate}
    

    \noindent {\bf A reduction step}: Consider an $R$-homomorphism $\phi:R^n\to M = \langle\langle m_1,\cdots,m_n\rangle\rangle$ defined by
    $$\phi((0,\cdots,\overbrace{1}^{i\text{-th position}},\cdots,0)):=m_i.$$
    Obviously, $\phi$ is surjective and
    $$\phi^{-1}(N):=\{\alpha\in R^n \ |\ \phi(\alpha)\in N\}$$
    is a submodule of $R^n$.
    
    \medskip
    Now we make the following claim:
    \begin{claim}\
        \begin{enumerate}
            \item All submodules $Q \leq R^n$ are finitely generated.
            \item If $R$ is PID, all submodules $Q\leq R^n$ is $n$-generated.
        \end{enumerate}
    \end{claim}
    Assuming the claim holds, then 
    $$\phi^{-1}(N):=\langle\langle\alpha_1,\cdots,\alpha_q\rangle\rangle$$ 
    is finitely generated (with $q=n$ if $R$ is PID). Hence, $N=\langle\langle\phi(\alpha_1),\cdots,\phi(\alpha_q)\rangle\rangle$
    is finitely generated. So the theorem proved.

    \bigskip
    \noindent{\it {\bf Proof of claim.}}
    We use induction on $n$ -- when $n=1$, then obviously all $Q\lhd R^1$ is finitely generated by Corollary \ref{fgforideals}. Moreover, if $R$ is PID, then $Q=\langle\langle a\rangle\rangle$ is 1-generated by the definition of PID.

    \medskip
    By induction hypothesis, assume claim holds for all submodules $Q_k\leq R^k$ for all $k<n$. Now consider the case of $R^n$, where $Q\leq R^n$ is a submodule of $R^n$. \\
    
    Let
    $$S_1:=\{(0,\cdots,0,a_n)\ |\ \text{there exists }a_1,\cdots,a_{n-1}\in R\text{ such that }(a_1,\cdots,a_{n-1},a_n)\in Q\},$$
    then one can check easily that
    $$S_1\leq\Tilde{R}:=\{(0,\cdots,0,r)\ |\ r\in R\}\cong R^1$$
    is a submodule. By induction hypothesis, there exists $r_1,\cdots,r_l\in R$ with
    $$\mathcal{Y}_1:=\{(0,\cdots,0,\gamma_1),\cdots,(0,\cdots,0,\gamma_l)\}$$
    such that $S_1=\langle\langle\mathcal{Y}_1\rangle\rangle$ is finitely generated (if $R$ is PID, $l=1$).\\
    On the other hand, consider
    $$S_2:=\{q\in Q\ |\ q=(b_1,\cdots,b_{n-1},0)\text{ for some }b_1,\cdots,b_{n-1}\in R\},$$
    then once again one can check that
    $$S_2\leq\widetilde{R}^{n-1}:=\{(r_1,\cdots,r_{n-1},0)\ |\ r_i\in R\}\cong R^{n-1}$$
    is a submodule. By induction hypothesis, there exists a finite set $\mathcal{Y}_2$ of $Q$ such that $S_2=\langle\langle\mathcal{Y}_2\rangle\rangle$ (if $R$ is PID, $|\mathcal{Y}_2|=n-1$).

    \medskip
    For any $q=(q_1,\cdots,q_{n-1},q_n)\in Q$, by definition of $S_1$, $(0,\cdots,0,q_n)\in S_1 = \langle \langle \mathcal{Y}_1 \rangle \rangle$. So there exists $r_1, \dots, r_l \in R$ such that$$(0,\cdots,0,q_n)=r_1(0,\cdots,0,\gamma_1)+\cdots+r_l(0,\cdots,0,\gamma_l),$$
    that is, $r_1 \gamma_1 + \cdots + r_l \gamma_l = q_n$. 
    Therefore
    \begin{align*}
        q =(q_1,\cdots,q_{n-1},q_n) =(q_1,\cdots,q_{n-1},r_1 \gamma_1 + \cdots + r_l \gamma_l).
    \end{align*}
    Moreover, for any $(0,\cdots,0,\gamma_x)\in\mathcal{Y}_1\leq S_1$, there exists $(\beta_{x,1},\cdots,\beta_{x,n-1},\gamma_x)\in Q$ for all $1\leq x\leq l$ by definition of $S_1$. Then one has
    \begin{align*}
        q'&:=q-\sum_{x=1}^{l}r_x(\beta_{x,1},\cdots,\beta_{x,n-1},\gamma_x)\\
        &=(q_1-\sum_{x=1}^{l}r_x\beta_{x,1},\cdots,q_{n-1}-\sum_{x=1}^{l}r_x\beta_{x,n-1},0)\in Q,
    \end{align*}
    and hence $q'\in S_2=\langle\langle\mathcal{Y}_2\rangle\rangle$. Consequently,
    \begin{align*}
        &\quad q=\sum_{x=1}^{l}r_x(\beta_{x,1},\cdots,\beta_{x,n-1},\gamma_x)+q'\\
        \Rightarrow &\quad q\in\left\langle\left\langle\underbrace{\left\{ 
        \begin{matrix}
            (\beta_{1,1},\cdots,\beta_{1,n-1},\gamma_1)\\
            ,\cdots,\\
            (\beta_{l,1},\cdots,\beta_{l,n-1},\gamma_l)\\
        \end{matrix}
    \right\}}_{l<\infty\text{ is finite and }l=1\text{ if $R$ is PID}}
    \cup\underbrace{\{\mathcal{Y}_2\}}_{|\mathcal{Y}_2|\text{ is finite and }|\mathcal{Y}_2|=n-1\text{ if $R$ is PID}}\right\rangle\right\rangle
    \end{align*}
    \medskip
    Therefore $Q=\left\langle\left\langle \left\{ 
    \begin{matrix}
        (\beta_{1,1},\cdots,\beta_{1,n-1},\gamma_1)\\
        ,\dots,\\
        (\beta_{l,1},\cdots,\beta_{l,n-1},\gamma_l)\\
    \end{matrix}
    \right\}\cup\{\mathcal{Y}_2\} \right\rangle\right\rangle$ is finitely generated and is $n$-generated if $R$ is PID.
\end{proof}

Here is an important theorem for noetherian rings, which will not be proved in this course:
\begin{theorem} [Hilbert Basis Theorem]
    If $R$ is noetherian, then the polynomial ring $R[x]$ is also noetherian.
\end{theorem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Modules over Principal Ideal Domain}
\section{Basic Definitions}
In this section, we will study the case when $R$ is PID, i.e. all ideals (submodules) $I\lhd R$ are 1-generated, i.e. $I=\langle a \rangle$ (from this Section on, we will write the ideals of $R$ by $\langle a \rangle$ instead of $\langle \langle a \rangle \rangle$. 

By our discussions in the previous section, we know that $R$ is noetherian. More precisely, if $M=\langle\langle m_1,\cdots,m_k\rangle\rangle$ is $k$-generated $R$-module, then $N=\langle\langle n_1,\cdots,n_k\rangle\rangle\leq M$ is also $k$-generated. On the other hand, $R$ possesses some very nice properties from the perspective of abstract algebra:

\noindent{\it {\bf Some other facts on PID (MAT3004).}}
In abstract algebra, we study whether one can uniquely factorize elements in a unital commutative $R$ into prime factors. For example, when $R=\mathbb{Z}$, we have:

\begin{theorem} [Fundamental Theorem of Arithmetic]
    All elements $n\in \mathbb{Z}$ can be uniquely factorized into prime numbers {\bf up to $\pm1$},
    $$n= \pm p_1^{e_1}\cdots p_k^{e_k}$$
    where $p_i$ are prime numbers.
\end{theorem}

\begin{example}
    $-120=(-1)\cdot2^3\cdot3\cdot5=(-2)\cdot(2)\cdot2\cdot(-3)\cdot(-5)$ are the same factorization of $-120$ up to $\pm1$.
\end{example}

In order to generalize the above theorem for all $R$, $\pm 1$ is replaced by {\bf unit of $R$}
$$U(R):=\{a\in R\ |\text{ there exists }b\in R\text{ such that }a\cdot b=1\}.$$
In particular, $U(\mathbb{Z}) = \{\pm 1\}$. 

As for a generalization of prime elements in $R$, Here is one possibility:

\begin{definition}
    Let $R$ be an integral domain. An element $r\in R$ is {\bf irreducible} if
    $$r=a\cdot b$$
    then $a\in U(R)$ or $b\in U(R)$.
\end{definition}

\begin{example}\
    \begin{enumerate}
        \item For $R=\mathbb{Z}$ all prime numbers $p\in\mathbb{Z}$ is irreducible since
        $$p=a\cdot b\quad\Rightarrow\quad\{a,b\}=
            \{p,1\} \quad \text{or}\quad
            \{-p,-1\}$$
        so either $a$ or $b$ equals to $\pm1$.
        \item For $R=\mathbb{R}[x]$, $x^2+1$ is irreducible.\\
        \item But in $R=\mathbb{C}[x]$, 
        $$x^2+1=(x-i)(x+i)$$ is reducible.
    \end{enumerate}
\end{example}

Now generalize the fundamental theorem of arithmetic from $\mathbb{Z}$ to any PID $R$:

\begin{theorem}
    Let $R$ be PID. Then
    \begin{enumerate}
        \item $r\in R$ is irreducible $\Leftrightarrow$ $r$ is prime. So we can (and we will) take the definition of prime to be that of irreducible for $R$.
        \item Up to units $U(R)$ of $R$, all elements $r\in R$ can be factorized uniquely into 
        $$r=p_1^{e_1}\cdots p_k^{e_k}$$
        where all $p_i$'s are primes ($=$ irreducibles) in $R$.
    \end{enumerate}
\end{theorem}

\section{Separating Free and Torsion Part}
The goal is to clarify all finitely generated $M$ over a PID $R$.

\begin{theorem}
    Let $R$ be a PID, and $M$ is a finitely generated $R$-module. Then
    $$M = M_{free}\oplus M_{tor}$$
    where
    \begin{itemize}
        \item $M_{free}$ is a finitely generated free module.
        \item $M_{tor}:=\{m\in M\ |\ r\cdot m=0\text{ for some }0\neq r\in R\}$ is the torsion submodule of $M$.
    \end{itemize}
\end{theorem}

The proof of this theorem is split into two parts:
\begin{enumerate}
    \item $M/M_{tor}$ is a free module,
    \item $M\cong M/M_{tor}\oplus M_{tor}$
\end{enumerate}

\medskip
We focus on (1) first. 
\begin{lemma}
Retain the above settings. Then the quotient module $M/M_{tor}$ is torsion-free.
\end{lemma}
\begin{proof}
Suppose $m+M_{tor}\in (M/M_{tor})_{tor}$, i.e.
    \begin{align*}
        &\quad r\cdot (m+M_{tor})={\bf 0}\text{ for some }0\neq r\in R\\
        \Rightarrow &\quad(r\cdot m)+M_{tor}={\bf 0}\\
        \Rightarrow &\quad r\cdot m\in M_{tor}\\
        \Rightarrow &\quad\text{There exists }0\neq s\in R\text{ such that }s\cdot(r\cdot m)={\bf 0}\text{ in }M\\
        \Rightarrow &\quad(s\cdot r)\cdot m={\bf 0}\text{ in }M\ \text{for}\ 0 \neq s\cdot r \in R\\
        \Rightarrow &\quad m\in M_{tor}\\
        \Rightarrow &\quad m+M_{tor}={\bf 0}\text{ in }M/M_{tor}.
    \end{align*}

    
    Therefore $(M/M_{tor})_{tor}=\{{\bf 0}\}$, i.e. $M/M_{tor}$ is torsion-free.
\end{proof}

\begin{proposition}
    Let $R$ be a PID, and $M$ is finitely generated $R$-module. Then
    \begin{center}
        $M$ is torsion free \ $\Leftrightarrow$ \ $M$ is free.
    \end{center}
    (Consequently, $M/M_{tor}$ is torsion free $\Rightarrow$ $M$ is free.)
\end{proposition}

\begin{proof}
    ($\Leftarrow$) If $M$ is free, i.e. $M=\langle\langle b_1,\cdots,b_k\rangle\rangle$
    where $\{b_1,\cdots,b_k\}$ is a basis of $M$. Suppose 
    $$m=r_1b_1+\cdots+r_kb_k\in M_{tor}$$
    i.e. there exists $0\neq r\in R$ such that
    $$r\cdot m=(rr_1)\cdot b_1+\cdots+(rr_k)\cdot b_k.$$
    By linear dependence of $b_1,\cdots,b_k$, $rr_i=0$ for any $1\leq i\leq k$. Then $r_i=0$ since $R$ is ID, and hence
    $$m=0\cdot b_1+\cdots+0\cdot b_k={\bf 0}.$$
    So $M_{tor}=\{{\bf 0}\}$, i.e. $M$ is torsion-free.

    ($\Rightarrow$) Suppose $M$ is torsion-free. We  will prove that $M$ is free by induction on the number of generators of $M$.

    $n=1$: Suppose $M=\langle\langle m\rangle\rangle$. Then since $M$ is torsion-free,
    $$r\cdot m\neq0\text{ for any }r\neq0.$$
    Therefore $\{m\}$ is a basis of $M$.

    $n=2$: Suppose $M=\langle\langle u,v\rangle\rangle$. If $\{u,v\}$ is linearly independent, then we are done. Otherwise, we have $r,s\in R$ are nonzero such that $r\cdot u+s\cdot v={\bf 0}$.\\
    Consider
    $$sM:=\langle\langle su,sv\rangle\rangle=\langle\langle su,(-r)u\rangle\rangle\leq\langle\langle u\rangle\rangle.$$
    Since $R$ is PID, $sM=\langle\langle u'\rangle\rangle\leq\langle\langle u\rangle\rangle$ is 1-generated. Moreover, $M$ is torsion-free implies $sM$ is torsion-free. By induction hypothesis, $sM=\langle\langle u'\rangle\rangle$ is free. Then one can construct an $R$-homomorphism
    $$\phi: M \to sM$$
    defined by $\phi(m) := s\cdot m$, which is obviously surjective, and is injective since $M$ is torsion-free. Therefore,
    $$M \cong sM$$
    is a free module. 

    Suppose the statement holds for $1\leq k< n$. Consider an $n$-generated torsion-free module 
    \begin{center}
        $M=\langle\langle m_1,\cdots,m_n\rangle\rangle$
    \end{center}
    If $\{m_1,\cdots,m_k\}$ is linearly independent, then it is a basis of $M$, and hence $M$ is free. Otherwise, assume $\{m_1,\cdots,m_k\}$ is a maximal linear independent subset for some $1\leq k < n$ (after some reordering if necessary).
    Then for $m_{k+1},m_{k+2},\cdots,m_{n}$, there exists non-zero elements $r_{k+1},r_{k+2}, \cdots, r_{n}\in R$ and $\star, \ast \in R$ not all zeros such that
    \begin{align*}
        r_{k+1}m_{k+1}+{\star}m_1+\cdots +{\star}m_{k} &=0 \\
         &\vdots \\
        r_nm_n+ {\ast }m_1+\cdots + {\ast }m_{k} &=0
    \end{align*}
    Let $a=r_{k+1}\cdot\cdots\cdot r_n\neq0$, then one has $am_{k+1}, \dots, am_n \in \langle \langle m_1, \dots, m_k \rangle \rangle$ by the above equations. So one has
    \begin{center}
        $aM = \langle\langle am_1,\cdots, am_k, am_{k+1},\cdots, am_n\rangle\rangle\leq\langle\langle m_1,\cdots,m_k\rangle\rangle$
    \end{center}
    Since $R$ is $PID$, $aM$ is $k$-generated as a submodule of $k$-generated module i.e. $aM =\langle\langle v_1,\cdots,v_k\rangle\rangle$, and it is torsion-free since $M$ is torsion-free. By applying induction hypothesis, we can conclude that $aM$ is free.\\
    Consequently, 
    $$\phi: M \to aM$$
    defined by $\phi(m) := a\cdot m$ is bijective, and hence $M \cong aM$ is a free module as desired.
\end{proof}

\begin{remark}
    If $R$ is not a PID, then there are examples of $M$ such that
    \begin{center}
        $M$ is torsion-free $\not\Rightarrow$ $M$ is free.
    \end{center}
    As an example, take $R=\mathbb{Z}[x]$ (not PID) and
    $$M=\langle\langle 2,x\rangle\rangle=\{2p(x)+xq(x)\}$$
    is a submodule of $R^1$. Then
    \begin{itemize}
        \item $M$ is torsion-free:

        Suppose $2p(x)+xq(x)\in M_{tor}$
        \begin{align*}
            \Rightarrow &\quad r(x)\cdot(2p(x)+xq(x))={\bf 0}\text{ for some $0\neq r(x)\in\mathbb{Z}[x]$}\\
            \Rightarrow &\quad r(x)=0\text{ or }2p(x)+xq(x)={\bf 0}\text{ since }\mathbb{Z}[x]\text{ is ID}\\
            \Rightarrow &\quad2p(x)+xq(x)={\bf 0}\\
            \Rightarrow &\quad M_{tor}=\{{\bf 0}\}\text{, i.e. }M\text{ is torsion-free.}
        \end{align*}
        \item $M$ is {\bf NOT} free: Note that
        $$M=\langle\langle 2,x\rangle\rangle\neq\langle\langle f\rangle\rangle$$
        for any polynomial $f\in \mathbb{Z}[x]$. Then suppose on contrary that $M=\langle\langle\mathcal{B}\rangle\rangle$ is free with $\mathcal{B}$ being a basis of $M$. Then $|\mathcal{B}|>1$ by our earlier discussions. So one can take $b_1(x),b_2(x)\in\mathcal{B}\leq M$ so that
        $$b_2(x)\cdot b_1(x)+(-b_1(x))\cdot b_2(x)=0,$$
        that is, $b_1(x),b_2(x)$ are linearly dependent, which raise a contradiction.
    \end{itemize}
\end{remark}


\begin{proposition}
    Let $M$ be an $R$-module, and $N\leq M$ submodule. Suppose the quotient module $M/N$ is free. Then there exists $N'\leq M$ s.t.
    \begin{enumerate}
        \item $N'\cong M/N$
        \item $M=N'\oplus N$ (internal direct sum)
    \end{enumerate}
    Particularly, $M\cong M/N\oplus N$ (external direct sum)
\end{proposition}
\begin{proof}
    Since $M/N$ is free, there is a basis $\{b_i+N|i\in I\}$ of $M/N$. In particular, for all $m\in M$, we have
    \begin{align*}
        \quad m+N = r_1(b_{i_1}+N)+\cdots+r_k(b_{i_k}+N) =(r_1b_{i_1}+\cdots+r_kb_{i_k})+N\end{align*}
    which is equivalent to 
     \begin{align*}
     m = r_1b_{i_1}+\cdots+r_kb_{i_k}+n \quad\text{for some $n\in N$} \quad \quad \quad (\ast)
    \end{align*}
    Let $N':=\langle\langle\{b_i|i\in I\}\rangle\rangle$ and consider an $R$-homomorphism
    $$\psi: N'\rightarrow M/N$$
    defined by $\psi(b_i) := b_i+N$. Then one has:
    \begin{itemize}
        \item $\psi$ is surjective since $\{b_i+N\}$ spans $M/N$.
        \item $\psi$ is injective: Let $r_1b_{i_1}+\cdots+r_kb_{i_k} \in \ker(\psi)$. Then
        \begin{align*}
        \psi(r_1b_{i_1}+\cdots+r_kb_{i_k}) &= {\bf 0}_{M/N},\\
        \Rightarrow \quad r_1(b_{i_1}+N)+\cdots+r_k(b_{i_k}+N) &= {\bf 0}_{M/N}
        \end{align*}
        Then $\{b_i+N\}$ is linearly independent implies $r_1=r_2=\cdots=r_k= 0$
and hence 
$$r_1b_{i_1}+\cdots+r_kb_{i_k} = {\bf 0}_M.$$
        So $\mathrm{ker}(\psi) = \{{\bf 0}_M\}$, i.e. $\psi$ is injective.
    \end{itemize}
So $N' \cong M/N$, and (1) is proved.

\medskip
    As for (2), we've seen that $M=N+N'$ by ($\ast$). To see the internal sum is direct, suppose $\tilde{n}= s_1b_{j_1}+\cdots+s_lb_{j_k}\in N'\cap N$. Then
    \begin{align*}
        \quad s_1b_{j_1}+\cdots+s_lb_{j_k}+N&={\bf 0}_{M/N}\\
        \Rightarrow \quad s_1(b_{j_1}+N)+\cdots+s_l(b_{j_{k}}+N) &= {\bf 0}_{M/N}.
    \end{align*}
    Since $\{b_j|j\in I\}$ is linearly independent, one has
    \begin{align*}
        &\quad s_1=\cdots= s_l=0\\
        \Rightarrow &\quad \tilde{n}={\bf 0}+\cdots+{\bf 0}={\bf 0}\in N'\cap N.
    \end{align*}
    Hence $M=N\oplus N'$.        
\end{proof}

\begin{theorem}
    Let R be a PID, and M is finitely generated $R$-module, Then there exixts $M_{free}\leq N$, such that $M_{free}$ is a finitely generated free module, and 
    $$M=M_{free}\oplus M_{tor}$$
    Moreover, if $M=F\oplus T$, for some free module $F\leq M$ and torsion module $T\leq M$, we have 
    $$T=M_{tor},\quad F\cong M_{free}(\cong R^n)$$
\end{theorem}

\begin{proof}
    By the above two propositions, we already have 
    $M= M_{free} \oplus M_{tor}$ with $M_{free} = N'\cong M/M_{tor}$.

    As for uniqueness, since $T$ is a submodule consisting only of torsion elements, one has $T\leq M_{tor}$. For the opposite inclusion, suppose $\tau = f+t\in M_{tor}\leq M=F\oplus T$
    for $f \in F$ and $t \in T$. Then
    \begin{align*}
        &\Rightarrow f=\tau-t \in M_{tor} \\
        &\Rightarrow f\in M_{tor} \cap F \\
        &\Rightarrow f=0 \text{ (since } F\ \text{is free implies}\ F\ \text{is torsion-free)}
    \end{align*}
    Consequently, $\tau=t\in T$ and hence $T = M_{tor}$, and one has
    $$M=M_{free}\oplus M_{tor}=F\oplus M_{tor}.$$
    By second isomorphism theorem, one immediately has $$M_{free}\cong F$$
    and the theorem follows.
\end{proof}

\section{Cyclic Modules and Primary Decomposition}
Now we focus on the case when $M=M_{tor}$ is torsion module. Consider the easiest case when
$M=\langle\langle v \rangle\rangle$ is 1-generated (cyclic) then
$$\mathrm{Ann}(M)=\mathrm{Ann}(v)=\{r\in R\ |\ rv=0\}$$
Since we assume $R$ is PID, the annihilators
$$\mathrm{Ann}(M)=\mathrm{Ann}(v)=\langle \alpha \rangle $$ 
are 1-generated.

\begin{proposition} \label{prop-cyclic}
    Let $R$ be PID, and $M=\langle\langle v \rangle\rangle$ is cyclic with $\mathrm{Ann}(M)=\mathrm{Ann}(v)=\langle \alpha \rangle$, for some $\alpha\in R$. Then
    \begin{enumerate}
        \item $\langle\langle v \rangle\rangle \cong R/\langle \alpha \rangle$
        \item For any $0\neq\beta \in R, \mathrm{Ann}(\beta v)=\langle \frac{\alpha}{\mathrm{gcd}(\alpha , \beta)}\rangle$
        \item $\langle \langle v\rangle \rangle = \langle\langle \beta v\rangle\rangle \Leftrightarrow \mathrm{gcd}(\alpha,\beta)=1$
    \end{enumerate}
\end{proposition}

\begin{remark} \label{rmk-bezout}
    Let $\alpha, \beta \in R$ be two nonzero elements. Since $R$ is a PID, we can uniquely factorize $\alpha$ and $\beta$ into:
    \begin{align*}
        \alpha &= up_1^{e_1}\cdots p_k^{e_k}\\
        \beta &=vq_1^{f_1}\cdots q_l^{f_l},
    \end{align*}
    where $p_i,q_j$ are primes and $u,v$ are units. So we can define their greatest common divisor $\mathrm{gcd}(\alpha,\beta)$ by collecting the common prime factor appearing in $\alpha$ and $\beta$. Note that $\mathrm{gcd}(\alpha,\beta)$ is defined up to a unit.

    Moreover, we also have a version of Bezout's Theorem on PID. Namely, for any $\alpha_1, \dots, \alpha_n \in R$ there exists $x_1, \dots, x_n \in R$ such that
         $$x_1\alpha_1 + \dots + x_n \alpha_n =\mathrm{gcd}(\alpha_1,\dots, \alpha_n)$$
        i.e. the greatest common divisor of $\alpha_1, \dots, \alpha_n$ can be written as a linear combination of $\alpha_1, \dots, \alpha_n$.
\end{remark}

\begin{proof}
    \begin{enumerate}
        \item Consider $R$-homomorphism 
        \begin{align*}
            \pi :  R&\rightarrow \langle \langle v \rangle\rangle \\
                   r&\mapsto rv
        \end{align*}
        then $\mathrm{im}(\pi)=\langle\langle v\rangle\rangle$, $\mathrm{ker}(\pi)=\mathrm{Ann}(v)=\langle \alpha \rangle $, and by isomorphism theorem we have $$\langle\langle v\rangle\rangle \cong R/\langle \alpha \rangle.$$
        \item For any $r\in \mathrm{Ann}(\beta v)$
        \begin{align*}
            r\cdot(\beta v)={\bf 0} 
            &\Leftrightarrow\quad (\beta r)\cdot v={\bf 0}\\
            &\Leftrightarrow\quad \beta r\in \mathrm{Ann}(v)=\langle \alpha\rangle\\
            &\Leftrightarrow\quad \beta r=\alpha\cdot s\quad(s\in R)\\
            &\Leftrightarrow\quad \alpha \mid\beta\cdot r\\
            &\Leftrightarrow\quad \frac{\alpha}{\mathrm{gcd}(\alpha , \beta)}\mid\frac{\beta}{\mathrm{gcd}(\alpha , \beta)}\cdot r\\
            &\Leftrightarrow\quad \frac{\alpha}{\mathrm{gcd}(\alpha , \beta)}\mid r\\
            &\Leftrightarrow\quad r=\frac{\alpha}{\mathrm{gcd}(\alpha , \beta)}\cdot t\text{, for some }t\in R\\
            &\Leftrightarrow\quad r\in\langle\frac{\alpha}{\mathrm{gcd}(\alpha , \beta)}\rangle.
        \end{align*}
        Therefore, $\mathrm{Ann}(\beta v)=\langle\frac{\alpha}{\mathrm{gcd}(\alpha , \beta)}\rangle$.
        \medskip
        \item For any $\beta v\in \langle\langle v\rangle\rangle$, one has $\langle\langle\beta v\rangle\rangle\leq\langle\langle
         v\rangle\rangle$.

         ($\Rightarrow$) If $\langle\langle \beta v\rangle\rangle=\langle\langle v\rangle\rangle$, then (2) implies
         $$\langle\frac{\alpha}{\mathrm{gcd}(\alpha , \beta)}\rangle=\langle\alpha\rangle,$$
         and hence $\mathrm{gcd}(\alpha, \beta)=1.$

         ($\Leftarrow$) We only need to show that $\langle\langle v\rangle\rangle\leq\langle\langle\beta v\rangle\rangle$. To do so, note that by Bezout's Theorem on PID (see Remark \ref{rmk-bezout} above), there exists $x,y\in R$ such that
         $$x\alpha+y\beta= \mathrm{gcd}(\alpha,\beta) = 1.$$
         Then
        $$v= 1v = (x\alpha+y\beta)v=x(\alpha v)+y(\beta v)=y(\beta v)\in\langle\langle\beta v\rangle\rangle.$$
        Therefore $v\in\langle\langle\beta v\rangle\rangle$, and hence $\langle\langle v\rangle\rangle\leq\langle\langle \beta v\rangle\rangle.$
        
    \end{enumerate}
\end{proof}



The following theorem gives a more detailed description of all cyclic modules:
\begin{theorem} \label{thm-crt}
    let $M=\langle\langle v\rangle\rangle$ be cyclic with
    $$\mathrm{Ann}(v)=\langle \alpha\rangle,\quad\alpha = p_1^{e_1}\cdots p_k^{e_k}$$
    ($p_i$ prime elements in $R$, $e_i\in\mathbb{N}$). Then
    \begin{enumerate}
        \item $v = u_1+\cdots+u_k$, $\mathrm{Ann}(u_i)=\langle p_i^{e_i}\rangle$.
        \item $\langle\langle v\rangle\rangle = \langle\langle u_1\rangle\rangle\oplus\cdots\oplus\langle\langle u_k\rangle\rangle$.
    \end{enumerate}
    Consequently, one has
    $$R/\langle p_1^{e_1}\cdots p_k^{e_k}\rangle\cong M=\langle\langle u_1\rangle\rangle\oplus\cdots\oplus\langle\langle u_k\rangle\rangle\cong R/\langle p_1^{e_1}\rangle\oplus\cdots\oplus R/\langle p_k^{e_k}\rangle.$$
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item Let $\beta_i:=\frac{\alpha}{p_i^{e_i}}(=p_1^{e_1}\cdots\widehat{p_i^{e_i}}\cdots p_k^{e_k})$. Then
        $\mathrm{gcd}(\beta_1,\cdots,\beta_k)=1$.\\
        By Bezout's Theorem, there exists $r_1,\cdots,r_k\in R$ such that $$r_1\beta_1+\cdots+r_k\beta_k=1 \quad \quad (\ast)$$
        So for any $v\in M$, one has
        \begin{align*}
            v&=(r_1\beta_1+\cdots+r_k\beta_k)v\\
            &=r_1\beta_1v+\cdots+r_k\beta_kv.
        \end{align*}
        Let $u_i:=r_i\beta_iv$. Then $v = u_1+\cdots+u_k$, and
        \begin{align*}
            \mathrm{Ann}(u_i)&=\mathrm{Ann}(r_i\beta_iv)\\
            &=\langle\frac{\alpha}{\mathrm{gcd}(\alpha , r_i\beta_i)}\rangle\\
            &=\langle\frac{\alpha}{\mathrm{gcd}(p_1^{e_1}\cdots p_k^{e_k} , r_ip_1^{e_1}\cdots\hat{p_i^{e_i}}\cdots p_k^{e_k})}\rangle\\
            &=\langle\frac{\alpha}{\beta_i}\rangle\\
            &=\langle p_i^{e_i}\rangle.
        \end{align*}
        Note that in the fourth equality above, one has $$\mathrm{gcd}(p_1^{e_1}\cdots p_k^{e_k} , r_ip_1^{e_1}\cdots\hat{p_i^{e_i}}\cdots p_k^{e_k}) = p_1^{e_1}\cdots\hat{p_i^{e_i}}\cdots p_k^{e_k} = \beta_i$$ because $r_i$ cannot contain any $p_i$-factors -- otherwise every summand of the expression $r_1\beta_1 + \dots + r_i\beta_i + \dots +r_k\beta_k$ on the left hand side of $(\ast)$ has a common $p_i$-factor, whose sum cannot be equal to $1$.
        
        \item We have $v = u_1+\cdots+u_k$, and hence
        $$\langle\langle v\rangle\rangle=\langle\langle u_1+\cdots+u_k\rangle\rangle\leq\langle\langle u_1\rangle\rangle+\cdots+\langle\langle u_k\rangle\rangle.$$
        Then we check the other inclusion -- note that 
        $$u_i := r_i\beta_iv \in \langle \langle v \rangle \rangle,$$
        so $\langle \langle u_i \rangle \rangle \leq \langle \langle v \rangle \rangle$ for all $1 \leq i \leq k$,
        and hence $\langle\langle u_1\rangle\rangle+\cdots+\langle\langle u_k\rangle\rangle \leq \langle\langle v\rangle\rangle.$
        \medskip
        
        Finally, we need to check the internal sum is direct: Suppose $$\alpha_1u_1+\cdots+\alpha_ku_k= {\bf 0}$$ 
        for some $\alpha_iu_i\in\langle\langle u_i\rangle\rangle$. Then
        \begin{align*}
            \beta_i(\alpha_1u_1+\cdots+\alpha_ku_k)=\beta_i(0) \quad \Rightarrow \quad \beta_i\alpha_iu_i=0
        \end{align*}
            Therefore, 
            $$\alpha_i \in\mathrm{Ann}(\beta_iu_i) =\langle\frac{p_i^{e_i}}{\mathrm{gcd}(\beta_i , p_i^{e_i})}\rangle =\langle\frac{p_i^{e_i}}{1}\rangle =\langle p_i^{e_i}\rangle =\mathrm{Ann}(u_i)$$
         In other words, $\alpha_iu_i = {\bf 0}$ for all $1 \leq i \leq k$,  
        and hence the result follows.
    \end{enumerate}
\end{proof}

\begin{corollary}[Chinese Remainder Theorem] \label{cor-crt}
    Let $R$ be a PID, and $p_1, \dots, p_k \in R$ be distinct primes. Then 
    $$R/\langle p_1^{e_1} \cdots p_k^{e_k} \rangle  \cong R/\langle p_1^{e_1} \rangle \oplus \cdots \oplus R/\langle p_k^{e_k} \rangle$$
\end{corollary}
\begin{proof}
    This follows immediately from Theorem \ref{thm-crt}(2) and $\langle \langle v \rangle \rangle \cong R/\langle \mathrm{Ann}(v) \rangle$ by Proposition \ref{prop-cyclic}(1).
\end{proof}

\begin{example}
    Let $R=\mathbb{Z}$, $M=\mathbb{Z}_{360}$. Then $M=\langle\langle[1]\rangle\rangle$ is cyclic, and $\mathrm{Ann}(M)=\langle 360\rangle=\langle2^3\cdot 3^2\cdot5\rangle$. Therefore, $\mathbb{Z}_{360}=\langle\langle u_8\rangle\rangle\oplus\langle\langle u_9\rangle\rangle\oplus\langle\langle 
    u_5\rangle\rangle$ with
    \begin{align*}
        &\mathrm{Ann}(u_8)=\langle8\rangle \quad\Rightarrow\quad\langle\langle u_8\rangle\rangle\cong\mathbb{Z}/\langle8\rangle \cong\mathbb{Z}_8\\
        &\mathrm{Ann}(u_9)=\langle9\rangle \quad\Rightarrow\quad\langle\langle u_9\rangle\rangle\cong\mathbb{Z}/\langle9\rangle \cong\mathbb{Z}_9\\
        &\mathrm{Ann}(u_5)=\langle5\rangle \quad\Rightarrow\quad\langle\langle u_5\rangle\rangle\cong\mathbb{Z}/\langle5\rangle \cong\mathbb{Z}_5,
    \end{align*}and hence
    \begin{align*}
        \mathbb{Z}_{360} &\cong\mathbb{Z}_8\oplus\mathbb{Z}_9\oplus\mathbb{Z}_5
    \end{align*}
\end{example}


As for all finitely generated torsion modules $M$ with $\mathrm{Ann}(M)=\langle\alpha\rangle$, where $\alpha=p_1^{e_1}\cdots q_k^{e_k}$
(we do not assume $M$ is cyclic here), one has the following theorem:

\begin{theorem} [Primary Decomposition] \label{thm-primarydecomp}
    Let $R$ be a PID, $M$ is a finitely generated torsion $R$-module with $\mathrm{Ann}(M)=\langle\alpha\rangle$, where $\alpha=p_1^{e_1}\cdots p_k^{e_k}$ for $p_1, \dots, p_k$ distinct primes and $e_i \in \mathbb{N}$. Then
    $$M=M_1\oplus\cdots\oplus M_k,$$
    where $M_i:=\{m\in M\ |\ p_i^{e_i}m=0\}$ with $\mathrm{Ann}(M_i)=\langle p_i^{e_i}\rangle$.
    
    Moreover, this decomposition is unique up to isomorphism, i.e. if $M=N_1\oplus\cdots\oplus N_l$ with $\mathrm{Ann}(N_i)=\langle q_i^{f_i}\rangle$ for $q_1, \dots, q_l$ distinct primes and $f_i \in \mathbb{N}$. Then (after some reordering)
    $$k=l,\quad\quad p_i^{e_i}=q_i^{f_i}$$
    for $1\leq i\leq k = l$, so that $M_i\cong N_i$ for all $i$.
\end{theorem}

\begin{proof}
    Let $\beta_i=p_1^{e_1}\cdots \widehat{p_l^{e_l}}\cdots p_k^{e_k}$ as before.
    Then $\beta_{i}M:=\{\beta_{i}m\ |\ m\in M\}\leq M$. Moreover, it is easy to see that $p_i^{e_i}$ annihilates all elements in $\beta_iM$ since $p_i^{e_i}\beta_i = \alpha$, so one has
    $$\beta_i M\leq M_i$$
    for all $1 \leq i \leq k.$

    \medskip
    \noindent\fbox{{\bf  Claim 1:} $\beta_i M =M_i$.} We only need to show $M_i \leq \beta_iM$. This can be seen by noting that $\mathrm{gcd}(\beta_i,p_i^{e_i})=1$, so one has $x, y \in R$ such that $x\beta_i+yp_i^{e_i}=1$.
        
    Now for any $m_i\in M$, 
    $$m_i= 1\cdot m_i = (x\beta_i+yp_i^{e_i})m_i=\beta_i(m_i)\in \beta_i M.$$
    \fbox{{\bf Claim 2:} $M=M_1+\cdots +M_k$.} As before, $\mathrm{gcd}(\beta_1,\cdots ,\beta_k)=1$, so one has $\alpha_1, \dots, \alpha_k$ such that $\alpha_1\beta_1+\cdots+\alpha_k\beta_k=1$. Then for any $m\in M$,
    $$m=(\alpha_1\beta_1+\cdots+\alpha_k\beta_k)m= \alpha_1(\beta_1m)+\cdots+\alpha_k(\beta_km)$$\\
    \fbox{{\bf  Claim 3:} $M=M_1\oplus M_2\oplus \cdots \oplus M_k$ is a direct sum.} Suppose there exists  $m_i \in M_i$ ($i \leq i \leq k$) such that
    $$m_1 + \dots + m_k = {\bf 0}.$$
    Then 
    \begin{align*}
          \beta_im_1 + \dots + \beta_im_i + \dots + \beta_im_k &= {\bf 0}\\
            {\bf 0}+ \dots + {\bf 0} + \beta_i m_i + {\bf 0} + \dots + {\bf 0} &= {\bf 0}
        \end{align*}
        Since $R$ is a PID and $\mathrm{Ann}(m_i)$ is an ideal, there exists $\mu \in R$ such that $\beta_i \in \mathrm{Ann}(m_i) = \langle \mu \rangle$, and hence 
        $$\mu | \beta_i.$$ 
        On the other hand, 
        $$\langle p_i^{e_i} \rangle = \mathrm{Ann}(M) \leq \mathrm{Ann}(m_i) = \langle \mu \rangle  \quad \Rightarrow \quad \mu | p_i^{e_i}.$$
        Consequently, $$\mu\ |\ \mathrm{gcd}(\beta,p_i^{e_i}) = 1,$$
        that is, $\mathrm{Ann}(m_i) = \langle 1 \rangle = R$. In other words, $1\cdot m_i = {\bf 0}$ and hence $m_i = {\bf 0}$. The arguments works for all $i$ and hence 
        $$m_1 = \dots = m_k = {\bf 0}$$
        as desired.
\end{proof}

\section{Primary Modules}
\begin{definition}
    An $R$-module is called {\bf primary} if $\mathrm{Ann}(M)=\langle p^e\rangle$ for some prime $p\in R$.
\end{definition}

\begin{theorem}\label{thm-decom}
    Let $M$ be primary module with $\mathrm{Ann}(M)=\langle p^e\rangle$. Then $M$ is equal to a direct sum of cyclic (1-generated) modules
    $$M=\langle\langle v_1\rangle\rangle\oplus\cdots\oplus\langle\langle v_n\rangle\rangle$$
    where $\mathrm{Ann}(\langle\langle v_i\rangle\rangle)=\mathrm{Ann}(v_i)= \langle p^{e_i} \rangle$ with $e=e_1\geq e_2\geq\cdots\geq e_n$.
\end{theorem}

\begin{proof}
    Since $\mathrm{Ann}(M)=\langle p^e\rangle$, there must be $v\in M$ such that $\mathrm{Ann}(v)=\langle p^e\rangle$.\\
    Consider $\langle\langle v\rangle\rangle\leq M$.  If $M=\langle\langle v\rangle\rangle$, we are done. Otherwise, we construct submodules $$\{{\bf 0}\}=: S_0< S_1< \cdots < S_k < \cdots$$ 
    inductively for all $k$ such that
    $$M_k :=\langle\langle v\rangle\rangle\oplus S_k$$
    is a direct sum, and $M_0<M_1< \cdots < M_k < \cdots$ for all $k$.
    \medskip
    
    The case when $k = 0$ is trivial. Now suppose we have constructed $S_0 < S_1 < \dots < S_k$ and
    $M_k = \langle \langle v \rangle \rangle \oplus S_k$. The process will stop if $M_k = M$. If not, i.e. $M_k<M$, take any $u\in M\setminus M_k$. We make the following 

    \bigskip
    \noindent \fbox{\begin{minipage}{\textwidth}
    {\bf Claim:} There exists $\alpha\in R$ such that
    by letting $S_{k+1}:=\langle\langle S_k,u-\alpha v\rangle\rangle > S_k$ 
    $$M_{k+1}=\langle\langle v\rangle\rangle\oplus S_{k+1}$$
    is a direct sum (note that for all $\alpha\in R$, $u-\alpha v\in M\setminus M_k$ and hence $M_{k+1} > M_k$).\end{minipage}}
\bigskip

    To construct the $\alpha$ in the claim, consider 
    $$I:=\{i\in R\ |\ i\cdot u\in M_k\}.$$
    Then it is easy to see that
    \begin{enumerate}
        \item $I\lhd R$ 
        \item $p^e\in I$ (since $p^e \cdot m = {\bf 0}$ for all $m \in M$)
        \item $1 \not\in I$ (since $u\notin M_k$)
    \end{enumerate}
    Therefore, $I = \langle \nu \rangle$ (since $R$ is a PID) and (2) implies 
    $$p^e \in \langle \nu \rangle \Rightarrow \nu|p^e \Rightarrow \nu = p^f$$
    for some $1 \leq f \leq e$ (note that $f \neq 0$ by (3)). As a result, 
    $$p^{f}u \in M_k =\langle\langle v \rangle\rangle \oplus S_k,$$ 
    i.e. there exists $dv \in \langle \langle v \rangle \rangle$, $s_k' \in S_k$ such that
    $$p^{f}u=dv+s_k' \quad \quad \quad (\star)$$
    Consequently,
    \begin{align*}
    p^{e-f}dv+p^{e-f}s_k' &= p^{e-f}\cdot p^{f}u \\
    p^{e-f}dv+p^{e-f}s_k' &= p^{e}u \\
    \underbrace{p^{e-f}dv}_{\in \langle \langle v \rangle \rangle} + \underbrace{p^{e-f}s_k'}_{\in S_k} &= {\bf 0}
    \end{align*}
    But $M_k = \langle\langle v \rangle\rangle \oplus S_k$ is a direct sum, this implies
    $${p^{e-f}d}v=0 \quad \text{and} \quad p^{e-f}s_k'=0.$$
    The first equality (along with the fact that $\mathrm{Ann}(v) = \langle p^e \rangle$) implies that
        $$p^e|p^{e-f}d \quad \Rightarrow \quad d=p^f\alpha$$
    for some $\alpha \in R$, so that $(\star)$ can be rewritten as:
    $$p^f u = p^f\alpha v + s_k' \quad \Rightarrow \quad p^f(u-\alpha v) = s_k' \in S_k \quad \quad (\ast)$$
    {\bf This is the $\alpha$ we are looking for!} 

    \medskip
    We can now prove the claim -- consider 
    $x\in \langle\langle v\rangle\rangle \cap S_{k+1}$, i.e. 
    $$x= \underbrace{rv}_{\in \langle \langle v \rangle \rangle} = \underbrace{s_k+b(u-\alpha v)}_{\in \langle \langle S_k, u-\alpha v\rangle \rangle =: S_{k+1}}$$
    By rearranging the terms, one has
    $$bu = (r + b\alpha) v - s_k \in \langle \langle v \rangle \rangle \oplus S_k = M_k.$$
    So $b \in I = \langle p^f \rangle$, i.e. $b = \beta p^f$ for some $\beta \in R$. So $x$ can be rewritten as
    $$x= \underbrace{rv}_{\in \langle \langle v \rangle \rangle} = s_k+\beta p^f(u-\alpha v) = \underbrace{s_k + \beta s_k'}_{\in S_k}$$
    where the last equality comes from $(\ast)$. Consequently, $x \in \langle \langle v \rangle \rangle \cap S_k = \{\bf 0\}$, and hence
    $\langle \langle v \rangle \rangle \cap S_{k+1} = \{\bf 0\}$.
    
    \bigskip
    To conclude, one can construct an increasing sequence of submodules of $M$:
    $$M_0 = \langle \langle v \rangle \rangle \oplus S_0\ <\ M_1 = \langle \langle v \rangle \rangle \oplus S_1\ <\ \dots\ <\ M_k = \langle \langle v \rangle \rangle \oplus S_k\ <\ \dots.$$
    as long as $M_k < M$. But $M$ is finite generated, hence noetherian property implies that the sequence must stop in finitely many steps, i.e. there exists $l \in \mathbb{N}$ such that
    $$M = M_l = \langle \langle v \rangle \rangle \oplus S_l.$$
    Now we can apply the same argument on $S_l$ to conclude that
    $$S_l = \langle \langle v_2 \rangle \rangle \oplus T, \quad T = \langle \langle v_3 \rangle \rangle \oplus U, \quad U = \langle \langle v_4 \rangle \rangle \oplus V, \quad \cdots$$
    By noetherian property again, this sequence must terminate at finitely many steps, and hence
    $$M = \langle \langle v \rangle \rangle \oplus \langle \langle v_2 \rangle \rangle \oplus \dots \oplus \langle \langle v_n \rangle \rangle.$$
    Moreover, it is obvious from the construction we described above, the exponents $e_i$ in the annihilators $\mathrm{Ann}(v_i) = \langle p^{e_i}\rangle$ must be decreasing. So the result follows.
\end{proof}

\begin{theorem}
    Let $M$ and $N$ be primary modules with
    \begin{align*}
        M&=\langle\langle v_1\rangle\rangle\oplus\cdots\oplus\langle\langle v_m\rangle\rangle,\quad\mathrm{Ann}(v_i)= \langle p^{e_i} \rangle \quad \text{with} \quad e_1 \geq \dots \geq e_m\\
        N&=\langle\langle u_1\rangle\rangle\oplus\cdots\oplus\langle\langle u_n\rangle\rangle,\quad\mathrm{Ann}(u_i)=\langle q^{f_i} \rangle \quad \text{with} \quad f_1 \geq \dots \geq f_n
    \end{align*}
    as in Theorem \ref{thm-decom}. Then
    $$M\cong N\quad\Leftrightarrow\quad m=n,\quad p=q,\quad e_i=f_i\quad\text{for all }1\leq i\leq m=n.$$
\end{theorem}

\begin{proof}
    ($\Leftarrow$) In such a case, note that by Proposition \ref{prop-cyclic}(1), 
    $$\langle\langle u_i\rangle\rangle \cong R/\langle p^{e_i} \rangle = R/\langle q^{f_i} \rangle \cong \langle\langle v_i\rangle\rangle$$
    for all $1 \leq i \leq m = n$. So it follows immediately that $M \cong N$ are isomorphic.
    
    ($\Rightarrow$) Suppose $M \cong N$ are isomorphic. Then obviously $\mathrm{Ann}(M) = \mathrm{Ann}(N)$
    and hence $p^{e_1} = q^{f_1}$, i.e. $p = q$ and $e_1 = f_1$.

    Now consider $M^{(p)} := \{m \in M\ |\ p \dot m = {\bf 0}\}$ and similarly $N^{(p)} := \{n \in N\ |\ p \dot n = {\bf 0}\}$. Then one can check that
    $M^{(p)} \cong N^{(p)}$ are both $R/\langle p \rangle$-module defined by
    $$\underbrace{(r + \langle p \rangle)}_{\in R/\langle p \rangle} \cdot x := r\cdot x.$$
    for $x \in M^{(p)}$ or $N^{(p)}$. As in the proof of Theorem \ref{thm-freebasis} (bases of free modules have the same cardinality), $\mathbb{F} = R/\langle p \rangle$ is a field, and hence
    $$M^{(p)} = \langle\langle v_1\rangle\rangle^{(p)} \oplus\cdots\oplus\langle\langle v_m\rangle\rangle^{(p)} \cong \langle\langle u_1\rangle\rangle^{(p)} \oplus\cdots\oplus\langle\langle u_n\rangle\rangle^{(p)} = N^{(p)} \quad (\star)$$
are isomorphic as vector spaces over $\mathbb{F}$. But for any 
\begin{align*}
\phi: \langle \langle w_k \rangle \rangle \xrightarrow{\cong} R/\langle p^{g} \rangle, 
\end{align*}
suppose $\kappa \in \langle \langle w_k \rangle \rangle$
is such that 
$\phi(\kappa) = p^{g-1} + \langle p^{g} \rangle$, then 
$$\phi(p\kappa) = p \phi(\kappa) = p(p^{g-1} + \langle p^{g} \rangle) = p^g + \langle p^{g} \rangle = {\bf 0}_{R/\langle p^{g} \rangle}$$
and hence $p\kappa = {\bf 0}$, i.e. $\kappa \in \langle \langle w_k \rangle \rangle^{(p)}$. Consequently, one can check easily that
$$\langle\langle w_k \rangle\rangle^{(p)} = \langle\langle \kappa \rangle\rangle$$
and, as a $\mathbb{F} = R/\langle p \rangle$-vector space, 
$$\langle\langle w_k \rangle\rangle^{(p)} = \mathrm{Span}_{\mathbb{F}}(\kappa)$$
is one-dimensional. Therefore, equation $(\star)$ above implies that 
$$m = \dim_{\mathbb{F}}(M^{(p)}) = \dim_{\mathbb{F}}(N^{(p)}) = n.$$ 
as desired.

Finally, we are left with showing $e_i = f_i$ for all $1 \leq i \leq m = n$. By above, we have already shown that $e_1 = f_1$. As for the other exponents, we proceed by induction on $e_1 = f_1$.

In the case when $e_1 = f_1 = 1$, then since
$$e_1 \geq e_2 \geq \dots \geq e_m$$
$$f_1 \geq f_2 \geq \dots \geq f_m$$
one must have $e_2 = \dots = e_m = f_2 = \dots = f_m = 1$. Now suppose the result holds for all primary modules satisfying $e_1 = f_1 < k$. Then in the case when $e_1 = f_1 = k$, i.e.
    \begin{align*}
        M \cong R/\langle p^k \rangle \oplus R/\langle p^{e_2} \rangle \oplus \cdots\oplus R/\langle p^{e_m} \rangle,
        \quad N \cong R/\langle p^k \rangle \oplus R/\langle p^{f_2} \rangle \oplus \cdots\oplus R/\langle p^{f_m} \rangle
    \end{align*}
Consider the submodules $pM := \{pm\ |\ m \in M\}$ and $pN := \{pn\ |\ n \in N\}$, so that $pM \cong pN$ and
\begin{align*}
        pM &\cong R/\langle p^{k-1} \rangle \oplus R/\langle p^{e_2-1} \rangle \oplus \cdots\oplus R/\langle p^{e_m-1} \rangle,\\
        pN &\cong R/\langle p^{k-1} \rangle \oplus R/\langle p^{f_2-1} \rangle \oplus \cdots\oplus R/\langle p^{f_m-1} \rangle.
    \end{align*}
Then by induction hypothesis, $e_i -1 = f_i -1$ for all $i$, and hence $e_i = f_i$ as we wish to prove.
\end{proof}

To conclude, one has the following classification of finitely generated modules over a PID: 
\begin{corollary} \label{cor-fgpid}
    Let $R$ be a PID, and $M$ be a finitely generated $R$-module. Then
    $$M = M_{free} \oplus M_{tor},$$ 
    where $M_{free} \cong R^k$ is a free $R$-module, and $M_{tor}$ is the torsion submodule of $M$.

\bigskip
    As for $M_{tor}$, suppose $\mathrm{Ann}(M_{tor}) = \langle p_1^{e_1} \dots p_k^{e_k} \rangle$ for primes $p_1, \dots, p_k \in R$, then we have the primary decomposition:
    $$M_{tor} = M_1 \oplus \dots \oplus M_k,$$
        where
        $$M_i := \beta_iM_{tor} = \{m \in M_{tor}\ |\ p_i^{e_i}m = {\bf 0}\} \quad \text{with} \quad \beta_i := p_1^{e_1} \dots \widehat{p_i^{e_i}} \dots p_k^{e_k}$$
        with $\mathrm{Ann}(M_i) = \langle p_i^{e_i} \rangle$.
        
\bigskip  
    Finally, for each primary module $M_i$, there exists $m_{i,1}, \dots, m_{i,a_i} \in M_i$ such that:
    $$M_i = \langle \langle m_{i,1} \rangle \rangle \oplus \dots \oplus \langle \langle m_{i,a_i} \rangle \rangle \quad \text{with} \quad \langle \langle m_{i,j} \rangle \rangle \cong R/\langle p_i^{e_i,j} \rangle$$
    i.e. $\mathrm{Ann}( \langle \langle m_{i,j} \rangle \rangle)) = \langle p_i^{e_{i,j}} \rangle$ for a decreasing sequence of positive integers $e_i = e_{i,1} \geq e_{i,2} \geq \dots \geq e_{i,a_i}$.

\bigskip
Consequently, $M$ is isomorphic to
$$M \cong R^k \oplus \begin{pmatrix} R/\langle p_1^{e_{1,1}} \rangle \oplus \cdots \oplus R/\langle p_1^{e_{1,a_1}} \rangle\\ 
\oplus \\
R/\langle p_2^{e_{2,1}} \rangle \oplus \cdots \oplus R/\langle p_2^{e_{2,a_2}} \rangle\\
\oplus \\
\vdots \\
\oplus \\
R/\langle p_k^{e_{k,1}} \rangle \oplus \cdots \oplus R/\langle p_k^{e_{k,a_k}} \rangle \end{pmatrix}$$
\end{corollary}

\section{Finitely Generated Abelian Groups}
As a direct application of the classification theorem, we have the classification of all abliean groups, i.e. a group $G$ (Definition \ref{def-group}) satisfying 
$$a \ast b = b \ast a$$
for all $a, b \in G$. In such a case, we usually make the following convention:
\begin{itemize}
    \item The `multiplication' operation $\ast$ is denoted by $+$, i.e. one has $a + b = b + a$ in $(G,+)$; 
    \item Denote ${\bf 0} := e$ is the identity element of $G$, i.e. $a + {\bf 0} = {\bf 0} + a = a$ for all $a \in G$;
    \item The inverse of $a \in G$ is denoted as $-a \in G$, i.e. $a + (-a) = (-a) + a = {\bf 0}$.
\end{itemize}

There is a natural $R=\mathbb{Z}$-module structure for any abelian group $(G,+)$:
$$m\cdot g = \begin{cases}
    \overbrace{g+\cdots+g}^{m\text{ times}}\quad\text{if }m\geq 0\\
    \underbrace{(-g)+\cdots+(-g)}_{(-m)\text{ times}}\quad\text{if }m<0
\end{cases}.$$
Therefore, Corollary \ref{cor-fgpid} (and the fact that $\mathbb{Z}/\langle a \rangle \cong \mathbb{Z}_{a}$) implies the following:
\begin{theorem}
    All finitely generated abelian group $(G,+)$ is isomorphic to:
$$G \cong \mathbb{Z}^k \oplus \begin{pmatrix} \mathbb{Z}_{p_1^{e_{1,1}}} \oplus \cdots \oplus \mathbb{Z}_{p_1^{e_{1,a_1}}}\\ 
\oplus \\
\mathbb{Z}_{p_2^{e_{2,1}}} \oplus \cdots \oplus \mathbb{Z}_{p_2^{e_{2,a_2}}}\\
\oplus \\
\vdots \\
\oplus \\
\mathbb{Z}_{p_k^{e_{k,1}}} \oplus \cdots \oplus \mathbb{Z}_{p_k^{e_{k,a_k}}} \end{pmatrix}$$
for distinct prime numbers $p_1, \dots, p_k \in \mathbb{N}$, and positive integers $e_{i,1} \geq \dots \geq e_{i,a_i}$.
\end{theorem}

\begin{example}
    We now use the above classification to get all abelian groups $G$ of order $|G| = 360 = 2^3 \times 3^2 \times 5^1$ up to isomorphism. 

    Firstly, there must be no free part $\mathbb{Z}^k$ since the group is finite. Also, one must have $p_1 = 2$, $p_2 = 3$ and $p_3 = 5$ or else $|G|$ will have a prime factor other than $2, 3$ and $5$.

    Consequently, one has the following choices:
    \begin{align*}
    &p_1 = 2:\ \mathbb{Z}_8 \quad \quad \quad \mathbb{Z}_4 \oplus \mathbb{Z}_2 \quad \quad \quad \mathbb{Z}_2 \oplus \mathbb{Z}_2 \oplus \mathbb{Z}_2 \\
    &p_2 = 3:\ \mathbb{Z}_9 \quad \quad \quad \mathbb{Z}_3 \oplus \mathbb{Z}_3 \\
    &p_3 = 5:\ \mathbb{Z}_5 
    \end{align*}
    and hence there are $3 \times 2 \times 1 = 6$ choices of $|G| = 360$ up to isomorphism (we omit the $\oplus$ in the following expressions):
    $$\begin{matrix}
        \mathbb{Z}_8 \\
        \mathbb{Z}_9 \\
        \mathbb{Z}_5
    \end{matrix}, \quad \quad 
    \begin{matrix}
        \mathbb{Z}_8 &\\
        \mathbb{Z}_3 & \mathbb{Z}_3 \\
        \mathbb{Z}_5 &
    \end{matrix}, \quad \quad 
    \begin{matrix}
        \mathbb{Z}_4 & \mathbb{Z}_2\\
        \mathbb{Z}_9 &  \\
        \mathbb{Z}_5 &
    \end{matrix}$$
$$\begin{matrix}
        \mathbb{Z}_4 & \mathbb{Z}_2 \\
        \mathbb{Z}_3 & \mathbb{Z}_3 \\
        \mathbb{Z}_5
    \end{matrix}, \quad \quad 
    \begin{matrix}
        \mathbb{Z}_2 & \mathbb{Z}_2 & \mathbb{Z}_2 \\
        \mathbb{Z}_9 & & \\
        \mathbb{Z}_5 & &
    \end{matrix}, \quad \quad 
    \begin{matrix}
        \mathbb{Z}_2 & \mathbb{Z}_2 & \mathbb{Z}_2\\
        \mathbb{Z}_3 & \mathbb{Z}_3 \\
        \mathbb{Z}_5 &
    \end{matrix}$$
On the other hand, one can read the sum along the columns. For example, one has 
$$\begin{matrix}
        \mathbb{Z}_4 & \mathbb{Z}_2 \\
        \mathbb{Z}_3 & \mathbb{Z}_3 \\
        \mathbb{Z}_5
    \end{matrix} \quad \cong \quad \mathbb{Z}_{4 \times 3 \times 5} \oplus \mathbb{Z}_{2 \times 3} \quad = \quad \mathbb{Z}_{60} \oplus \mathbb{Z}_6.$$
by Corollary \ref{cor-crt}. Then the $6$ non-isomorphic $|G| = 360$ can be re-written as:
$$\mathbb{Z}_{360} \quad \quad \quad \mathbb{Z}_{120} \oplus \mathbb{Z}_3 \quad \quad \quad  \mathbb{Z}_{180} \oplus \mathbb{Z}_2$$
$$\mathbb{Z}_{60} \oplus \mathbb{Z}_6 \quad \quad \mathbb{Z}_{90} \oplus \mathbb{Z}_2 \oplus \mathbb{Z}_2 \quad \quad \mathbb{Z}_{30} \oplus \mathbb{Z}_6 \oplus \mathbb{Z}_2.$$
\end{example}

Note that in the expressions of $\mathbb{Z}_{d_1} \oplus \mathbb{Z}_{d_2} \oplus \dots \oplus \mathbb{Z}_{d_m}$ above, one always has
$$d_2| d_1, \quad d_3 | d_2, \quad \cdots, \quad d_m|d_{m-1},$$
(e.g. $6|30, 2|6$ in $\mathbb{Z}_{30} \oplus \mathbb{Z}_6 \oplus \mathbb{Z}_2$) which is due to our ordering of $e_{i,1} \geq e_{i,2} \geq \dots \geq e_{i,a_i}$ in Corollary \ref{cor-fgpid}. In general, one has the following:
\begin{corollary}[Smith Normal Form]
Let $R$ be a PID, and $M$ is a finitely generated $R$-module. Then there exists unique $d_1, d_2, \dots, d_m \in R$ such that
$$d_2| d_1, \quad d_3 | d_2, \quad \cdots, \quad d_m|d_{m-1},$$
and 
$$M \cong R^k \oplus R/\langle d_1 \rangle \oplus
R/\langle d_2 \rangle \oplus \cdots \oplus R/\langle d_m \rangle$$
\end{corollary}



\chapter{Linear Operators on Vector Spaces}
We will apply knowledge of finitely generated modules over $R 
= \mathbb{F}[x]$ to understand linear operators $T \in \mathcal{L}(V,V)$ on finite-dimensional vector space $V$ over $\mathbb{F}$. To begin with, note that $(\mathcal{L}(V,V), +, \circ)$ is a unital, {\bf non-commutative} ring under the usual addition:
$$(S + T)(v) := S(v) + T(v)$$
and multiplication given by composition of functions:
$$(S \circ T)(v) := S(T(v))$$
(here the multiplicative identity element is $I \in \mathcal{L}(V,V)$ defined by $I(v) := v$ for all $v \in V$). 

\section{Vector Spaces as $\mathbb{F}[x]$-modules}
Instead of studying $\mathcal{L}(V,V)$, we will consider a subring of $\mathcal{L}(V,V)$:
\begin{definition}
    Let $T \in \mathcal{L}(V,V)$. Define $\mathcal{R}(T) \subseteq \mathcal{L}(V,V)$ by:
$$\mathcal{R}(T) := \{ p(T) := a_n T^n + \dots + a_1 T + a_0 I \in \mathcal{L}(V,V)\ |\ p(x) = a_n x^n + \dots + a_1 x + a_0 \in \mathbb{F}[x]\},$$
where $T^n := \overbrace{T \circ \dots \circ T}^{n\ times}$.
\end{definition}

\begin{proposition} \label{prop-subring}
    For any $T \in \mathcal{L}(V,V)$, $\mathcal{R}(T)$ is a unital commutative subring of $\mathcal{L}(V,V)$.
\end{proposition}
\begin{proof}
    Let $p(T), q(T) \in \mathcal{R}(T)$. In order to show that $\mathcal{R}(T) \leq \mathcal{L}(V,V)$ is a subring, one needs to show the following:
    $$\alpha p(T) + \beta q(T) \in \mathcal{R}(T) \quad \text{and} \quad p(T) \circ q(T) \in \mathcal{R}(T).$$
    But this is obvious, since 
    $$\alpha p(T) + \beta q(T) = (\alpha p + \beta q)(T)$$
    for $(\alpha p + \beta q)(x) := \alpha p(x) + \beta q(x) \in \mathbb{F}[x]$, and
    $$p(T) \circ q(T) = (p \cdot q)(T)$$
    for $(p \cdot q)(x) := p(x)q(x) \in \mathbb{F}[x]$. For instance, if $p(x) = 2x + 1$, $q(x) = 4x^2 + 6x + 7$, then for any $v \in V$, one has
    \begin{align*}
    (p(T) \circ q(T))(v) &= [(2T + I) \circ (4 T\circ T + 6 T + 7I)](v) \\
    &= (2T+I)(4T(T(v)) + 6T(v) + 7v)\\
    &= 2T((4T(T(v)) + 6T(v) + 7v) + (4T(T(v)) + 6T(v) + 7v)\\
    &= 8T^3(v) + 12T^2(v) + 14T(v) + 4T^2(v) + 6T(v) + 7(v)\\
    &= (8T^3 + 16T^2 + 20T + 7I)(v)\\
    &= h(T)(v)
    \end{align*}
    for $h(x) = 8x^3 + 16x^2 + 20x + 7 = (2x+1)(4x^2 + 6x+7) = p(x)q(x)$.

    Consequently, $(\mathcal{R}(T), +, \circ)$ is a subring of $(\mathcal{L}(V,V), +, \circ)$. Moreover, $\mathcal{R}(T)$ is commutative:
    $$p(T) \circ q(T) = (p\cdot q)(T) = (q \cdot p)(T) = q(T) \circ p(T)$$
    since $(p \cdot q)(x) = (q \cdot p)(x)$ in $\mathbb{F}[x]$.
\end{proof}

As a consequence of the above proposition, there is a unital ring homomorphism:
$$\phi: (\mathbb{F}[x], +, \cdot) \longrightarrow (\mathcal{R}(T), +, \circ)$$
given by $$\phi(p(x)) := p(T)$$ for all $p(x) \in \mathbb{F}[x]$. This leads us the following:

\begin{definition} \label{def-fx}
    Let $T:V\to V$ be a linear operator on a finite dimensional vector space over $\mathbb{F}$. Then $V$ is a $\mathbb{F}[x]$-module defined with the scalar multiplication 
    $$f(x)\cdot v:= \phi(f(x))(v) = f(T)(v)$$
    for all $f(x) \in \mathbb{F}[x]$.
\end{definition}
\begin{remark}
    One can check that the above definition satisfies the axioms of modules in Definition \ref{def-module}. For instance, condition (7) holds since:
    \begin{align*}
    [p(x) \cdot q(x)]\cdot v &:= [\phi((p(x)\cdot q(x))](v) \\
    &= [\phi(p(x)) \circ \phi(q(x))](v) \\
    &= (p(T) \circ q(T))(v) \\
    &= p(T)(q(T)(v)) \\
    &=: p(T)\left(q(x)\cdot v\right) \\
    &=: p(x) \cdot [q(x) \cdot v].    
    \end{align*}  
\end{remark}

\section{Minimal Polynomials}
\begin{proposition} \label{prop-minpoly}
    Let $T \in \mathcal{L}(V,V)$ be a linear operator on a finite dimensional vector space $V$ over $\mathbb{F}$. 
    \begin{enumerate}
    \item There exists a polynomial $p(x) \in \mathbb{F}[x]$ such that $p(T) = {\bf 0}_{\mathcal{L}(V,V)}$, in other words,
    $$p(T)(v) = {\bf 0}_V \quad \quad \text{for all}\ v \in V.$$
    \item Let $I := \{ f(x) \in \mathbb{F}[x]\ |\ f(T) = {\bf 0}_{\mathcal{L}(V,V)}\}$. Then $I \lhd \mathbb{F}[x]$ is a nonzero ideal in $\mathbb{F}[x]$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}
        \item Note that $\dim_{\mathbb{F}}(\mathcal{L}(V,V)) = n^2$, so the subset $\{I, T, \dots, T^{n^2}\}$ are linearly dependent in $\mathcal{L}(V,V)$. In other words, there exists $a_0, a_1, \dots, a_{n^2} \in \mathbb{F}$ not all zero such that
        $$a_{n^2} T^{n^2} + \dots + a_1 T + a_0 = {\bf 0}_{\mathcal{L}(V,V)}.$$
        Therefore, one can take $p(x) = a_{n^2} x^{n^2} + \dots + a_1 x + a_0$.
        \item Suppose $f(x), g(x) \in I$, i.e. $f(T)(v) = g(T)(v) = {\bf 0}_V$ for all $v \in V$. Then
        $$[(f+g)(T)](v) = f(T)(v) + g(T)v = {\bf 0} + {\bf 0} = {\bf 0}  \quad \Rightarrow \quad f(x) + g(x) \in I$$
        and for all $r(x) \in \mathbb{F}[x]$,
        $$(r \cdot f)(T)(v) = (r(T) \circ f(T))(v) = r(T)({\bf 0}) = {\bf 0} \quad \Rightarrow \quad (r\cdot f)(x) = r(x)f(x) \in I.$$
        Therefore, $I \lhd \mathbb{F}[x]$ is an ideal.
    \end{enumerate}
\end{proof}

By the above proposition and the fact that $\mathbb{F}[x]$ is a PID, the nonzero ideal $I \lhd \mathbb{F}[x]$ must be principal. So there exists a nonzero polynomial (which can be chosen to be monic, i.e. the leading term coefficient is $1$) $m_T(x)$ such that
$$I = \langle m_T(x) \rangle.$$
This leads us to the following definition:
\begin{definition}[Minimal polynomial]
    Let $T \in \mathcal{L}(V,V)$ be a linear operator on a finite dimensional vector space $V$ over $\mathbb{F}$. The {\bf minimal polynomial} corresponding to $T$ is defined by $m_T(x)$ as discussed above, so that
    $$I = \{f(x)\ |\ f(T)(v) = {\bf 0}_{V}\ \text{for all } v \in V\} = \langle m_T(x) \rangle$$
\end{definition}

As an immediate consequence of Proposition \ref{prop-minpoly}, one has:
\begin{corollary}
    The minimal polynomial $m_T(x)$ is the unique monic polynomial of smallest possible degree such that $m_T(T)(v) = {\bf 0}_V$ for all $v \in V$. Moreover, for any $f(x) \in \mathbb{F}[x]$,
    \begin{center}
    $f(T)(v) = {\bf 0}_V$ for all $v \in V$ \quad $\Leftrightarrow$ \quad
    $m_T(x) | f(x).$    
    \end{center}
    
\end{corollary}

\begin{example} \label{eg-2x2}
Let $V = \mathbb{R}^2$.
\begin{itemize}
    \item[(a)] Suppose $T_1: V \to V$ is given by 
    $$T_1(v) = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}v.$$ 
    Then
$(T_1 - I) = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}$, and hence $(T-I)(v) = {\bf 0}$
for all $v \in V$. So 
$$m_{T_1}(x) = x-1.$$
\item[(b)]   Suppose $T_2: V \to V$ is given by 
$$T_2(v) = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}v.$$ Then
$(T_2 - I)^2 = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}$. By the above corollary, one has
$$m_{T_2}(x) |  (x-1)^2$$
But obviously $m_{T_2}(x) = x-1$ since
$$T-I = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \neq \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}.$$
Therefore, $m_{T_2}(x) = (x-1)^2$.
\end{itemize}
\end{example}

Now we rephrase the above discussions in terms of $\mathbb{F}[x]$-modules:
\begin{corollary}
     Let $T \in \mathcal{L}(V,V)$ be a linear operator on a finite dimensional vector space $V$ over $\mathbb{F}$. Under Definition \ref{def-fx}, 
     $V$ is a $\mathbb{F}[x]$-torsion module with
     $$\mathrm{Ann}(V) = \langle m_T(x) \rangle.$$
\end{corollary}


\section{Coordinate Vectors and Matrix Representation}   
   Let $V$ be a finite dimensional vector space over $\mathbb{F}$, and $\mathcal{B} = \{v_1, \dots, v_n\}$ be an ordered basis of $V$. Recall the coordinate vector map in MAT2042, which is an isomorphism {\it on vector spaces}:
    \begin{align*}\Phi: V &\xrightarrow{\cong} \mathbb{F}^n \\
    v = \alpha_1 v_1 + \dots + \alpha_n v_n \quad &\mapsto \quad \Phi(v) := [v]_{\mathcal{B}} = \begin{pmatrix}
        \alpha_1 \\ \vdots \\ \alpha_n
    \end{pmatrix} \end{align*}
    Now we put $T \in \mathcal{L}(V,V)$ into account -- on the left hand side of the isomorphism, $V$ is an $\mathbb{F}[x]$-module as discussed in the previous section. As for the right hand side, recall the matrix representation $T_{\mathcal{B}\mathcal{B}} \in M_{n \times n}(\mathbb{F})$ of $T \in \mathcal{L}(V,V)$. Then $\mathbb{F}^n$ can be seen as an $\mathbb{F}[x]$-module by:
        $$f(x) \cdot \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix} := f(T_{\mathcal{B}\mathcal{B}})\begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix}$$
    where the right hand side of the equation is the usual matrix-vector multiplication (we will treat every $A \in M_{n \times n}(\mathbb{F}) = \mathcal{L}(\mathbb{F},\mathbb{F})$ as a linear operator on $\mathbb{F}^n$ by matrix vector multiplication). Then one has:
\begin{proposition}
    Under the above setting, the coordinate vector map $\Phi: V \longrightarrow \mathbb{F}^n$ is an isomorphism of $\mathbb{F}[x]$-modules.
\end{proposition}

\begin{proof}
    Since $\Phi$ is already an isomorphism of vector spaces ($\mathbb{F}$-modules), it is bijective and it satisfies
    $$\Phi(v + w) = \Phi(v) + \Phi(w) \quad \quad \Phi(\alpha \cdot v) = \alpha \cdot \Phi(v)$$
    for all $\alpha \in \mathbb{F}$. Now for $x^i \in \mathbb{F}[x]$,
    \begin{align*}
    \Phi(x^i \cdot v) &= \Phi (T^i(v)) \\
    &= [T^{i}(v)]_{\mathcal{B}} \\
    &= [T(T^{i-1}(v))]_{\mathcal{B}} \\ 
    &= T_{\mathcal{B}\mathcal{B}}[T^{i-1}(v)]_{\mathcal{B}} \\
    &= T_{\mathcal{B}\mathcal{B}}T_{\mathcal{B}\mathcal{B}}[T^{i-2}(v)]_{\mathcal{B}} \\
    & \quad \vdots \\
    &= (T_{\mathcal{B}\mathcal{B}})^i[v]_{\mathcal{B}} \\
    &= (T_{\mathcal{B}\mathcal{B}})^i(\Phi(v)) \\
    &= x^i \cdot \Phi(v)
    \end{align*}
where we used the fact (from MAT2042) that $[T(w)]_{\mathcal{B}} = T_{\mathcal{B}\mathcal{B}}[w]_{\mathcal{B}}$ in the fourth equality. Consequently, for all $f(x) = a_0 + a_1x + \dots a_nx^n \in \mathbb{F}[x]$,
    \begin{align*}
    \Phi(f(x) \cdot v) &= \Phi (a_0v + a_1x \cdot v + \dots + a_nx^n \cdot v) \\
    &= a_0\Phi (v) + a_1\Phi(x \cdot v) + \dots + a_n\Phi(x^n \cdot v) \\
    &= a_0\Phi (v) + a_1x \cdot \Phi(v) + \dots + a_nx^n \cdot \Phi(v) \\
    &= (a_0+ a_1x + \dots + a_nx^n) \cdot \Phi(v) \\
    &= f(x) \cdot \Phi(v)
    \end{align*}
and the result follows.  
\end{proof}

\begin{corollary}
    Let $V$ be a vector space, and $\mathcal{B}$ be {\bf any} ordered basis of $V$. Under the above isomorphism $V \cong \mathbb{F}^n$, the minimal polynomials of $T \in \mathcal{L}(V,V)$ and $T_{\mathcal{B}\mathcal{B}} \in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^n) = M_{n \times n}(\mathbb{F})$ are equal, i.e.
    $$m_T(x) = m_{T_{\mathcal{B}\mathcal{B}}}(x).$$
\end{corollary}

\begin{proof}
    Since $V \cong \mathbb{F}^n$ as $\mathbb{F}[x]$-modules, one has
    $$\langle m_T(x) \rangle = \mathrm{Ann}(V) = \mathrm{Ann}(\mathbb{F}^n) = \langle m_{T_{\mathcal{B}\mathcal{B}}}(x) \rangle,$$
    and the result follows since both $m_T(x)$ and $m_{T_{\mathcal{B}\mathcal{B}}}(x)$ have leading coefficient equal to $1$.
\end{proof}


\begin{corollary} \label{cor-minpolyconj}
    If $A, B \in M_{n \times n}(\mathbb{F})$ are similar, i.e. $B = MAM^{-1}$ for some invertible matrix $M \in M_{n \times n}(\mathbb{F})$, then 
    $$m_{A}(x) = m_{B}(x).$$
\end{corollary}

\begin{proof}
Let $V = \mathbb{F}^n$ and $T \in \mathcal{L}(V,V)$ be defined by the usual matrix multiplication by $A$:
$$T(v) := Av$$
By taking the usual basis $\mathcal{E} = \{e_1, \dots, e_n\}$ of $V$, then the matrix representation of $T$ using $\mathcal{E}$ gives us back the matrix $A$:
$$T_{\mathcal{E}\mathcal{E}} = A.$$
On the other hand, let $\mathbb{M} = \{m_1, \dots, m_n\}$ be the columns of the matrix $M$ appearing in the corollary. Then $\mathcal{M}$ is a basis of $V$ since $M$ is invertible, and the matrix representation of $T$ using $\mathcal{M}$ gives:
$$T_{\mathcal{M}\mathcal{M}} = MAM^{-1} = B.$$
Therefore, by the above corollary
$$m_{A}(x) m_{T_{\mathcal{E}\mathcal{E}}}(x) = m_{T}(x) = m_{T_{\mathcal{M}\mathcal{M}}}(x) = m_{B}(x)$$
and the result follows.
\end{proof}

\section{Cayley-Hamilton Theorem}
Now we go back to studying the torsion $\mathbb{F}[x]$-module $V$ with $\mathrm{Ann}(V) = \langle m_T(x) \rangle$. Suppose
$$m_T(x) = p_1(x)^{e_1} \cdot \dots \cdot p_k(x)^{e_k},$$
where $p_i(x)$ are distinct irreducible polynomials in $\mathbb{F}[x]$. Then 
Theorem \ref{thm-primarydecomp} implies the following:
\begin{theorem}[Primary Decomposition for Vector Spaces] \label{thm-primaryvs}
Let $V$ be a finite dimensional vector space over $\mathbb{F}$, and $T \in \mathcal{L}(V,V)$ be such that
$$m_T(x) = p_1(x)^{e_1} \cdots p_k(x)^{e_k}.$$ 
Then there exists vector subspaces $V_1, \dots, V_k \leq V$ such that
\begin{itemize}
    \item $V = V_1 \oplus \dots \oplus V_k$.
    \item $V_i = \{v \in V\ |\ (p_i(T))^{e_i}(v) = {\bf 0}_V\}$ is a {\bf $T$-invariant subspace}, i.e. $T|_{V_i}$ maps $V_i$ to $V_i$.
    \item Each $V_i$ is a primary module with $\mathrm{Ann}(V_i) = \langle p_i(x)^{e_i} \rangle$. In other words, the restricted linear operator $T|_{V_i}: V_i \to V_i$ has minimal polynomial 
    $$m_{T|_{V_i}}(x) = p_i(x)^{e_i}.$$
\end{itemize}
\end{theorem}

\begin{proof}
   Everything follows immediately from the Primary Decomposition Theorem, except for the $T$-invariance of $V_i$, i.e. $T(v_i) \in V_i$ for all $v_i \in V_i$. 
    But this is true in general for all $\mathbb{F}[x]$-submodules $W \leq V$ (in particular $W = V_i$). Indeed, for any $w \in W$, one has
   $$x \cdot w \in W \quad \Rightarrow \quad  T(w) \in W$$
   by taking $x \in \mathbb{F}[x]$.
\end{proof}


Now we study each primary module $V_i$ in Theorem \ref{thm-primaryvs} more closely. Namely, by Theorem \ref{thm-decom}, $V_i$ is further decomposed into:
$$V_i = V_{i_1} \oplus \dots \oplus V_{i,a_i}$$
where $V_{i,l} \cong \mathbb{F}[x]/\langle p_i^{e_{i,l}} \rangle$ for 
a unique decreasing sequence $e_i = e_{i,1} \geq \dots \geq e_{i,a_i}$ of integers. This leads us to:
\begin{definition}
    Let $V$ be a finite dimensional vector space over $\mathbb{F}$, and $T \in \mathcal{L}(V,V)$ with minimal polynomial $m_T(x) = p_1(x)^{e_1} \cdot \dots \cdot p_k(x)^{e_k}$. Suppose
    $$V = V_1 \oplus \dots \oplus V_k$$
    is the primary decomposition of $V$ as in Theorem \ref{thm-primaryvs}, and each $V_i$ is further decomposed into 
    $$V_i = V_{i_1} \oplus \dots \oplus V_{i,a_i}$$ 
    with $V_{i,l} \cong \mathbb{F}[x]/\langle p_i^{e_{i,l}} \rangle$ for $e_i = e_{i,1} \geq \dots \geq e_{i,a_i}$. Then the {\bf characteristic polynomial} corresponding to $T$ is defined as:
    $$\chi_T(x) := \prod_{i = 1}^k \prod_{l = 1}^{a_i} p_i(x)^{e_{i,l}},$$
    where $e_{i,l} \in \mathbb{N}$ are as given in the equation above.
\end{definition}

\begin{remark}
    \begin{enumerate}
        \item Obviously, one has $m_T(x) | \chi_T(x)$. {\bf But it is NOT clear why $\chi_T(x)$ is related to determinant as in MAT2042!}
        \item Since $V \cong \mathbb{F}^n$ as $\mathbb{F}[x]$-modules, one has $$\chi_T(x) = \chi_{T_{\mathcal{B}\mathcal{B}}}(x)$$
        for any ordered basis $\mathcal{B}$ of $V$.
        \item As in Corollary \ref{cor-minpolyconj}, one also has
        $$ \chi_{A}(x) = \chi_{B}(x)$$
        if $A$ and $B$ are similar matrices.
        \item Note that $\mathrm{deg}(\chi_{T}(x)) = \dim_{\mathbb{F}}(V)$. Indeed, for each $V_{i,l} \cong \mathbb{F}[x]/\langle p_i(x)^{e_{i,l}} \rangle$, 
        $$\dim_{\mathbb{F}}(V_{i,l}) = \deg(p_i(x)^{e_{i,l}}) = d$$
        since $\{1 + \langle p_i(x)^{e_{i,l}} \rangle, x + \langle p_i(x)^{e_{i,l}} \rangle, \cdots, x^{d-1} + \langle p_i(x)^{e_{i,l}} \rangle\}$ is a basis of $\mathbb{F}[x]/\langle p_i(x)^{e_{i,l}} \rangle$ as a vector space over $\mathbb{F}$. Therefore,
        $$\quad \quad \deg(\chi_T(x)) = \sum_{i=1}^k \sum_{l=1}^{a_i} \deg(p_i(x)^{e_{i,l}}) = \sum_{i=1}^k \sum_{l=1}^{a_i} \dim(V_{i,l}) = \dim(V).$$
    \end{enumerate}
\end{remark}

\begin{theorem}[Cayley Hamilton]
    Let $T \in \mathcal{L}(V,V)$ be a linear operator on a finite dimensional vector space $V$ over $\mathbb{F}$, then
    $$\chi_T(x) = \det(xI -T_{\mathcal{B}\mathcal{B}}),$$
    where $\mathcal{B}$ is any ordered basis of $V$, and $\det$ is the determinant of an $n \times n$ matrix as defined in Section \ref{sec-det}. 
\end{theorem}

\begin{proof}
 Decompose
$$V= (V_{1,1} \oplus \cdots \oplus V_{1,a_1}) \oplus 
(V_{2,1} \oplus \cdots \oplus V_{2,a_2}) \oplus \cdots \oplus 
(V_{k,1} \oplus \cdots \oplus V_{k,a_k})$$
where $V_{i,l} \cong \mathbb{F}[x]/\langle p_i(x)^{e_{i,l}} \rangle$ as in the definition of $\chi_{T}(x)$. Then as in the proof of Theorem \ref{thm-primaryvs}, each $V_{i,l} \leq V$ is $T$-invariant. As a consequence, let $\mathcal{B}_{i,l}$ be an ordered basis of $V_{i,l}$ as constructed above, and
$$\mathcal{B} = \bigsqcup_{i=1}^k \bigsqcup_{l = 1}^{a_i} \mathcal{B}_{i,l}$$
be an ordered basis of $V$. Then the matrix representation of $T$ is a {\bf block-diagonal matrix}:
$$T_{\mathcal{B}\mathcal{B}} = \tiny{\begin{pmatrix} (T|_{V_{1,1}})_{\mathcal{B}_{1,1}\mathcal{B}_{1,1}} & & & & & \\
& \ddots & & & & & \\ 
& & (T|_{V_{1,a_1}})_{\mathcal{B}_{1,a_1}\mathcal{B}_{1,a_1}} & & & & \\
& & & \ddots & & &\\
& & & & (T|_{V_{k,1}})_{\mathcal{B}_{k,1}\mathcal{B}_{k,1}} & & \\
& & & & & \ddots & \\
& & & & & & (T|_{V_{k,a_k}})_{\mathcal{B}_{k,a_k}\mathcal{B}_{k,a_k}}\end{pmatrix}}
$$
By usual way of computing determinants, one has
$$\det(xI-T_{\mathcal{B}\mathcal{B}}) = \prod_{i = 1}^k \prod_{l = 1}^{a_i} \det(xI - (T|_{V_{i,l}})_{\mathcal{B}_{i,l}\mathcal{B}_{i,l}}),$$
and the theorem is reduced to proving
$$\det(xI - (T|_{V_{i,l}})_{\mathcal{B}_{i,l}\mathcal{B}_{i,l}}) = p_i(x)^{e_{i,l}}.$$

\bigskip
To see why it is the case, suppose 
$$p_i(x)^{e_{i,l}} = x^d + \beta_{d-1}x^{d-1} + \dots + \beta_1 x + \beta_0,$$ 
and the isomorphism $\phi : V_{i,l} \xrightarrow{\cong} \mathbb{F}[x]/\langle p_i(x)^{e_{i,l}} \rangle$ is given by:
\begin{align*}
\phi(w^{j}) = x^j + \langle p_i(x)^{e_{i,l}} \rangle 
\end{align*}
for $0 \leq i \leq d-1$, then $\mathcal{B}_{i,l} = \{w^0, \dots, w^{d-1}\}$ forms a basis of $V_{i,l}$, and 
\begin{align*}
\phi(T(w^i)) &= \phi(x \cdot w^i)\\ 
&= x \cdot \phi(w^i)\\ &= x \cdot \left(x^{i} + \langle p_i(x)^{e_{i,l}} \rangle\right)\\ 
&= x^{i+1} + \langle p_i(x)^{e_{i,l}} \rangle \\
&= \begin{cases} x^{i+1} + \langle p_i(x)^{e_{i,l}} \rangle & \text{if}\ i < d-1 \\
x^d +  \langle p_i(x)^{e_{i,l}} \rangle & \text{if}\ i = d-1 \end{cases}\\
&= \begin{cases} \phi(w^{i+1}) & \text{if}\ i < d-1 \\
-\beta_{d-1}x^{d-1} - \dots - \beta_1 x - \beta_0 +  \langle p_i(x)^{e_{i,l}} \rangle & \text{if}\ i = d-1 \end{cases} \\
&= \begin{cases} \phi(w^{i+1}) & \text{if}\ i < d-1 \\
\phi(-\beta_{d-1}w^{d-1} - \dots - \beta_1 w^{1} - \beta_0 w^0) & \text{if}\ i = d-1 \end{cases} 
\end{align*}
Since $\phi$ is a bijection, one has
$$T|_{V_{i,l}}(w^i) = T(w^i)= \begin{cases} w^{i+1} & \text{if}\ i < d-1 \\
-\beta_{d-1}w^{d-1} - \dots - \beta_1 w^{1} - \beta_0 w^0 & \text{if}\ i = d-1 \end{cases} $$
In other words, the matrix representation of $T|_{V_{i,l}}: V_{i,l} \to V_{i,l}$ is given by:
$$(T|_{V_{i,l}})_{\mathcal{B}_{i,l}\mathcal{B}_{i,l}} = \begin{pmatrix}
    0 &  &  &  -\beta_0 \\
    1 & \ddots     & &   -\beta_1 \\
    & \ddots & 0 &  \vdots \\ 
     &  & 1 & -\beta_{d-1}
\end{pmatrix}$$
By direct computation on determinant again, one can check that $\det(xI - (T|_{V_{i,l}})_{\mathcal{B}_{i,l}\mathcal{B}_{i,l}}) = \beta_0 + \beta_1 x + \dots + \beta_{d-1}x^{d-1} = p_i(x)^{e_{i,l}}$, and the result follows.
\end{proof}

Since $\chi_T(x) = \chi_{T_{\mathcal{B}\mathcal{B}}}(x)$ by the previous remark, one immediately has
\begin{corollary}[Cayley Hamilton Theorem for Matrices]
For any $A \in M_{n \times n}(\mathbb{F})$,
     $$\chi_{A}(x) = \det(xI -A).$$
Consequently, $m_A(x) | \chi_A(x) = \det(xI-A)$, and hence $\chi_A(A) = {\bf 0}_{n \times n}$.
\end{corollary}
In other words, we get back our usual definition of $\chi_A(x)$ for any $n \times n$ matrix $A$, and one can guess the minimal polynomial $m_A(x)$ by looking at all possible factors of $\chi_A(x)$.

\begin{example}
    Consider $A_1 := \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$ and $A_2 := \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$ as in Example \ref{eg-2x2}. Then the corollary implies that one can calculate characteristic polynomials using determinants, which give us:
    $$\chi_{A_1}(x) = \chi_{A_2}(x) = (x-1)^2$$
    and hence 
    $$m_{A_1}(x), \ m_{A_2}(x) | (x-1)^2.$$
    So the possibilities of $m_{A_1}(x)$ and $m_{A_2}(x)$ are $x-1$
    and $(x-1)^2$. 
    In the first case, since $A_1 - I = {\bf 0}_{2 \times 2}$, one has
    $$m_{A_1}(x) = x-1.$$
    While in the second case, $A_2 - I \neq {\bf 0}_{2 \times 2}$ so that $m_{A_2}(x) \neq x-1$. Therefore, one has
    $$m_{A_2}(x) = (x-1)^2.$$
\end{example}

As a side product of the above proof, we have
\begin{corollary}[Rational Canonical Form] \label{cor-rcf}
Let $V$ be a finite dimensional vector space over $\mathbb{F}$, and $T \in \mathcal{L}(V,V)$ has minimal polynomial $m_T(x) = p_1(x)^{e_1} \dots p_k(x)^{e_k}$ and $V$ is decomposed into
$$V = (V_{1,1} \oplus \cdots \oplus V_{1,a_1}) \oplus \cdots \oplus (V_{k,1} \oplus \cdots \oplus V_{k,a_k})$$
with $V_{i,l} \cong \mathbb{F}[x]/\langle p_i(x)^{e_{i,l}} \rangle$. Then there exists an ordered basis $\mathcal{B}$ of $V$ such that 
    $$T_{\mathcal{B}\mathcal{B}} = \begin{pmatrix} R_{1,1} & & & & & \\
& \ddots & & & & & \\ 
& & R_{1,a_1} & & & & \\
& & & \ddots & & &\\
& & & & R_{k,1} & & \\
& & & & & \ddots & \\
& & & & & & R_{k,a_k}\end{pmatrix},\ \ R_{i,l} := \begin{pmatrix}
    0 &  &  &  -\beta_0 \\
    1 & \ddots     & &   -\beta_1 \\
    & \ddots & 0 &  \vdots \\ 
     &  & 1 & -\beta_{d-1}
\end{pmatrix}
$$
for $p_i(x)^{e_{i,l}} = x^d + \beta_{d-1}x^{d-1}+ \dots + \beta_1 x + \beta_0$.
\end{corollary}

And for matrices, one can apply the trick in Corollary \ref{cor-minpolyconj} to conclude that:
\begin{corollary}[Rational Canonical Form for Matrices]
Let $A \in M_{n\times n}(\mathbb{F})$ with minimal polynomial
$m_A(x) = p_1(x)^{e_1} \dots p_k(x)^{e_k}$, and $V = \mathbb{F}^n$ is decomposed into $\mathbb{F}[x]$-submodules
$$\mathbb{F}^n \cong (F_{1,1} \oplus \cdots \oplus F_{1,a_1}) \oplus \cdots \oplus (F_{k,1} \oplus \cdots \oplus F_{k,a_k})$$
with $F_{i,l} \cong \mathbb{F}[x]/\langle p_i(x)^{e_{i,l}} \rangle$. Then there exists an invertible matrix $M$ such that 
    $$MAM^{-1} = \begin{pmatrix} R_{1,1} & & & & & \\
& \ddots & & & & & \\ 
& & R_{1,a_1} & & & & \\
& & & \ddots & & &\\
& & & & R_{k,1} & & \\
& & & & & \ddots & \\
& & & & & & R_{k,a_k}\end{pmatrix},\ \ R_{i,l} := \begin{pmatrix}
    0 &  &  &  -\beta_0 \\
    1 & \ddots     & &   -\beta_1 \\
    & \ddots & 0 &  \vdots \\ 
     &  & 1 & -\beta_{d-1}
\end{pmatrix}
$$
for $p_i(x)^{e_{i,l}} = x^d + \beta_{d-1}x^{d-1}+ \dots + \beta_1 x + \beta_0$.    
\end{corollary}
\begin{proof}
    Take $V = \mathbb{F}^n$ in Corollary \ref{cor-rcf}, and $T(v) := A(v)$. Then one has a basis $\mathcal{B}$ such that $T_{\mathcal{B}\mathcal{B}}$ satisfies the hypothesis of the corollary. Let $M = (b_1|b_2|\cdots|b_n)$, where $b_i \in \mathcal{B}$ are the basis elements, then one can verify (using MAT2042) that
    $$T_{\mathcal{B}\mathcal{B}} = MAM^{-1},$$
    and the result follows.
\end{proof}


\section{Jordan Normal Form}
In this section, we will focus on the case when the irreducible factors in the minimal polynomial $m_T(x) \in \mathbb{F}[x]$ of 
$T \in \mathcal{L}(V,V)$ are linear, i.e.
$$m_T(x) = (x-\alpha_1)^{e_1} \dots (x - \alpha_k)^{e_k}$$
for distinct $\alpha_1, \dots, \alpha_k \in \mathbb{F}$.
This is always the case when $\mathbb{F}$ is {\bf algebraically closed}, for example $\mathbb{F} = \mathbb{C}$. In such a case:
\begin{theorem}[Jordan Normal Form]
Let $V$ be a finite dimensional vector space over $\mathbb{F}$, and $T \in \mathcal{L}(V,V)$ be such that the minimal polynomial
$$m_T(x) = (x-\alpha_1)^{e_1} \dots (x - \alpha_k)^{e_k}$$
consists only of linear factors. Then there exists an ordered basis $\mathcal{B}$ of $V$ such that 
    $$T_{\mathcal{B}\mathcal{B}} = \begin{pmatrix} J_{1,1} & & & & & \\
& \ddots & & & & & \\ 
& & J_{1,a_1} & & & & \\
& & & \ddots & & &\\
& & & & J_{k,1} & & \\
& & & & & \ddots & \\
& & & & & & J_{k,a_k}\end{pmatrix}, \quad \text{where}\ J_{i,l} := \begin{pmatrix} \alpha_i & 1 & & \\
 & \alpha_i & \ddots & \\
 &  & \ddots & 1 \\
 &  &  & \alpha_i \end{pmatrix}.
$$
\end{theorem}

\begin{proof}
As before, one has 
$$V= (V_{1,1} \oplus \cdots \oplus V_{1,a_1}) \oplus 
(V_{2,1} \oplus \cdots \oplus V_{2,a_2}) \oplus \cdots \oplus 
(V_{k,1} \oplus \cdots \oplus V_{k,a_k})$$
where $V_{i,l} \cong \mathbb{F}[x]/\langle (x-\alpha_i)^{e_{i,l}} \rangle$ are all $T$-invariant subspaces of $V$, and
$$T_{\mathcal{B}\mathcal{B}} = \tiny{\begin{pmatrix} (T|_{V_{1,1}})_{\mathcal{B}_{1,1}\mathcal{B}_{1,1}} & & & & & \\
& \ddots & & & & & \\ 
& & (T|_{V_{1,a_1}})_{\mathcal{B}_{1,a_1}\mathcal{B}_{1,a_1}} & & & & \\
& & & \ddots & & &\\
& & & & (T|_{V_{k,1}})_{\mathcal{B}_{k,1}\mathcal{B}_{k,1}} & & \\
& & & & & \ddots & \\
& & & & & & (T|_{V_{k,a_k}})_{\mathcal{B}_{k,a_k}\mathcal{B}_{k,a_k}}\end{pmatrix}}
$$
for an ordered basis $\displaystyle \mathcal{B} = \bigsqcup_{i=1}^k \bigsqcup_{l = 1}^{a_i} \mathcal{B}_{i,l}$ of $V$. So one needs to find a basis $\mathcal{B}_{i,l}$ of $V_{i,l}$ such that
$$(T|_{V_{i,l}})_{\mathcal{B}_{i,l}\mathcal{B}_{i,l}} = J_{i,l}.$$
To do so, consider the isomorphism:
$$\phi: V_{i,l} \xrightarrow{\cong} \mathbb{F}[x]/\langle (x-\alpha_i)^{e_{i,l}}\rangle$$
Then there exists $\mathcal{B}_{i,l} := \{u^{e_{i,l}-1}, \dots, u^{1}, u^0\}$ in $V_{i,l}$ such that
$$\phi(u^{j}) = (x-\alpha_i)^j + \langle (x - \alpha_i)^{e_{i,l}} \rangle$$
and hence $\mathcal{B}_{i,l}$ is a basis of $V_{i,l}$. Now
\begin{align*}\phi(T(u^j)) &= \phi(x \cdot u^j)\\
&= x \cdot \phi(u^j) \\
&= x \cdot [(x-\alpha_i)^j + \langle (x - \alpha_i)^{e_{i,l}} \rangle]\\ 
&= x(x-\alpha_i)^j + \langle (x - \alpha_i)^{e_{i,l}} \rangle \\
&= [(x-\alpha_i)^{j+1} + \langle (x - \alpha_i)^{e_{i,l}} \rangle] + [\alpha_i(x-\alpha_i)^{j} + \langle (x - \alpha_i)^{e_{i,l}} \rangle] \\
&= \begin{cases} \phi(u^{j+1}) + \phi( \alpha_i u^j) & \text{if}\ j+1 \neq e_{i,l} \\
0 + \phi( \alpha_i u^j) & \text{if}\ j+1 = e_{i,l} \end{cases} \\
&= \begin{cases} \phi(u^{j+1} + \alpha_i u^j) & \text{if}\ j+1 \neq e_{i,l} \\
\phi( \alpha_i u^j) & \text{if}\ j+1 = e_{i,l} \end{cases} 
\end{align*}
Since $\phi$ is an isomorphism, one has
$$ T|_{V_{i,l}}(u^j) = T(u^j) = \begin{cases} u^{j+1} + \alpha_i u^j & \text{if}\ j+1 \neq e_{i,l} \\
\alpha_i u^j & \text{if}\ j+1 = e_{i,l} \end{cases} $$
Then the result follows by writing down the matrix representation $(T|_{V_{i,l}})_{\mathcal{B}_{i,l}\mathcal{B}_{i,l}}$ using the above formula.
\end{proof}

%Since $m_T(x) | \chi_T(x)$, one immediately has:
%\begin{corollary}
%    Let $V$ be a finite dimensional vector space over $\mathbb{F}$, and $T \in \mathcal{L}(V,V)$ be such that the {\bf characteristic} polynomial
%$$\chi_T(x) = (x-\alpha_1)^{f_1} \dots (x - \alpha_k)^{f_k}$$
%consists only of linear factors, then the same result holds for $T$.
%\end{corollary}

As for matrices, one can once again treat $A$ as a linear operator on $V = \mathbb{F}^n$ as in Corollary \ref{cor-minpolyconj} and get:
\begin{corollary}[Jordan Normal Form for Matrices]
    Let $A \in M_{n \times n}(\mathbb{F})$ be such that the minimal polynomial
$$\chi_A(x) = (x-\alpha_1)^{e_1} \dots (x - \alpha_k)^{e_k}$$
consists only of linear factors, then there exists an invertible matrix $M$ such that
$$MAM^{-1} = \begin{pmatrix} J_{1,1} & & & & & \\
& \ddots & & & & & \\ 
& & J_{1,a_1} & & & & \\
& & & \ddots & & &\\
& & & & J_{k,1} & & \\
& & & & & \ddots & \\
& & & & & & J_{k,a_k}\end{pmatrix},\ J_{i,l} := \begin{pmatrix} \alpha_i & 1 & & \\
 & \alpha_i & \ddots & \\
 &  & \ddots & 1 \\
 &  &  & \alpha_i \end{pmatrix}.
$$
\end{corollary}
\begin{example}
    Let $V = \mathbb{C}^{10}$ and $A \in M_{n \times n}(\mathbb{C})$ be a complex matrix. Suppose
    $$m_{A}(x) = (x-1)^4(x-2)^3(x-3)$$
    Then one has 
    $$\mathbb{C}^{10} \cong \quad \begin{matrix} \mathbb{F}[x]/\langle (x-1)^{e_{1,1}} \rangle \oplus  \cdots \oplus \mathbb{F}[x]/\langle (x-1)^{e_{1,a_1}} \rangle \\
    \oplus \\ 
    \mathbb{F}[x]/\langle (x-2)^{e_{2,1}} \rangle \oplus \cdots \oplus \mathbb{F}[x]/\langle (x-2)^{e_{2,a_2}} \rangle\\ \oplus \\ 
    \mathbb{F}[x]/\langle (x-3)^{e_{3,1}} \rangle \oplus \cdots \oplus \mathbb{F}[x]/\langle (x-3)^{e_{3,a_3}}\rangle \end{matrix}$$
    with
    \begin{align*}
    4 &= e_{1,1} \geq e_{1,2} \geq \dots \geq e_{1,a_1}\\
    3 &= e_{2,1} \geq e_{2,2} \geq \dots \geq e_{2,a_2}\\
    1 &= e_{3,1} \geq e_{3,2} \geq \dots \geq e_{3,a_3}
    \end{align*}
    Recall that $\dim_{\mathbb{F}}(\mathbb{F}[x]/\langle p(x) \rangle) = \deg(p(x))$, so one must have
    $$10 = (e_{1,1} + \dots + e_{1,a_1})+ (e_{2,1} + \dots + e_{2,a_2}) + (e_{3,1} + \dots + e_{3,a_3})$$
    and hence
    $$2 = (e_{1,2} + \dots + e_{1,a_1})+ (e_{2,2} + \dots + e_{2,a_2}) + (e_{3,2} + \dots + e_{3,a_3}).$$
    So the possibilities of the exponents $e_{i,l}$ are:
    \begin{center}
    \begin{tabular}{|c|c|c|}
        \hline 
        $e_{1,l}$ &  $e_{2,l}$ & $e_{3,l}$\\
        \hline
         $4 \geq 2$ & $3$ & $1$\\
         \hline
         $4 \geq 1 \geq 1$ & $3$ & $1$\\
         \hline
         $4 \geq 1$ & $3 \geq 1$ & $1$\\
         \hline
         $4 \geq 1$ & $3$ & $1 \geq 1$\\
         \hline
         $4$ & $3 \geq 2$ & $1$\\
         \hline
         $4$ & $3 \geq 1 \geq 1$ & $1$\\
         \hline
         $4$ & $3 \geq 1$ & $1 \geq 1$\\
         \hline
         $4$ & $3$ & $1 \geq 1 \geq 1$\\
         \hline
    \end{tabular}    
    \end{center}

    \medskip
    Take the example of $e_{1,1}= 4 \geq e_{1,2} = 1$, $e_{2,1} = 3$ and $e_{3,1} = 1 \geq e_{3,2} = 1$, its Jordan Normal Form matrix looks like:
$$\begin{pmatrix} 
1 & 1 & 0 & 0 & & & & & & \\
0 & 1 & 1 & 0 & & & & & & \\ 
0 & 0 & 1 & 1 & & & & & &\\
0 & 0 & 0 & 1 & & & & & &\\
& & & & 1 & & & & & \\
& & & &  & 2 & 1 & 0 & & \\
& & & &  & 0 & 2 & 1 & & \\
& & & &  & 0 & 0 & 2 & & \\
& & & &  &  &  &  & 3 & \\
& & & &  &  &  &  &  & 3
\end{pmatrix}.
$$

    
\end{example}

\end{document}